---
editor: visual
execute:
  echo: false
  warning: false
  message: false
  cache: true
  cache.lazy: false
  fig-align: center
---
```{r setup}
knitr::opts_chunk$set(fig.align = "center")

library(tidyverse)
library(brms)
library(bayesplot)
library(tidybayes)
library(patchwork)
library(GGally)
library(dagitty)
library(ggdag)
library(ggrepel)

library(rethinking)
data(milk)

at <- c(-3, -2, -1, 0, 1, 2, 3)
```

# TLDR

We don't necessarily care about fitting the data well, we care about making good predictions about the future. Occam's razor should shape our models so that we prefer parsimonious models all else equal. How do we quantify the "parsimonious-ness" of a model? We have a grab bag of tools to hopefully steer us between the opposing twin dangers of over-fitting or under-fitting.

What are our tools? 

**(1) Regularizing the prior**

- Don't get to excited about the data

- Same as frequentists use of a penalized likelihood (e.g. Lasso, Ridge) 

**(2) Scoring Devices**

- Information Criteria: AIC BIC, WAIC

- Cross Validation: PSIS-LOO

## The Problem with Parameters

Last chapter we spent a lot of time building causal models. Causal models are fantastic because they allow you to see the consequences of changing a variable, i.e. the $Do(X)$ operation. If we don't care about understanding the effects of our actions, rather we only want to predict what $Y$ will be, it's tempting to put as many variables into our regression as possible. The more variables we add, inevitably our model will fit the data better.

$R^2$ is a common metric that incentives naive scientists to keep adding variables. $R^2$ is defined as:

$$R^2 = \frac{\text{var(outcome) - var(residuals)}}{\text{var(outcome)}} = 1 - \frac{\text{var(residuals)}}{\text{var(outcome)}}$$

$R^2$ always increases as more variables are added even when you just add random numbers which have no relation to the outcome.

```{r}
#| fig-width: 5
#| fig-height: 4

# Alternative: Function to run multiple simulations for robustness
simulate_r2_inflation_robust <- function(n_sims = 30, n_covariates = 20, n_iterations = 100, seed = 123) {
  set.seed(seed)
  
  # Run multiple iterations
  all_results <- map_dfr(1:n_iterations, function(iter) {
    # Generate new data for each iteration
    y <- rnorm(n_sims, mean = 0, sd = 1)
    covariate_data <- map_dfc(1:n_covariates, ~rnorm(n_sims)) %>%
      set_names(paste0("x", 1:n_covariates))
    simData <- bind_cols(covariate_data, y = y)
    
    # Calculate R-squared for each number of covariates
    map_dfr(1:n_covariates, function(k) {
      covars <- paste0("x", 1:k, collapse = " + ")
      formula_str <- paste("y ~", covars)
      model <- lm(as.formula(formula_str), data = simData)
      
      tibble(
        iteration = iter,
        n_covariates = k,
        r_squared = summary(model)$r.squared,
        adjusted_r_squared = summary(model)$adj.r.squared
      )
    })
  })
  
  # Summarize across iterations
  summary_results <- all_results %>%
    group_by(n_covariates) %>%
    summarise(
      mean_r_squared = mean(r_squared),
      se_r_squared = sd(r_squared) / sqrt(n()),
      mean_adj_r_squared = mean(adjusted_r_squared),
      se_adj_r_squared = sd(adjusted_r_squared) / sqrt(n()),
      .groups = "drop"
    )
  
  return(summary_results)
}

# Example usage of robust version (uncomment to run):
robust_results <- simulate_r2_inflation_robust(n_sims = 30, n_covariates = 30, n_iterations = 30)
# print(robust_results)

robust_results %>%
  ggplot(aes(x = n_covariates, y = mean_r_squared)) +
  geom_ribbon(aes(ymin = mean_r_squared - 1.96 * se_r_squared,
                  ymax = mean_r_squared + 1.96 * se_r_squared),
              alpha = 0.2, fill = "#e74c3c") +
  geom_line(color = "#e74c3c", size = 1.2) +
  geom_point(color = "#e74c3c", size = 2) +
  labs(
    title = "R² Inflation with Random Covariates (Robust)",
    subtitle = paste("Average across 30 simulations with", 
                    max(robust_results$n_covariates), "random covariates"),
    x = "Number of Covariates",
    y = "Mean R²",
    caption = "Shaded area shows 95% confidence interval"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12, color = "gray50"),
    plot.caption = element_text(size = 10, color = "gray60")
  ) +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent_format())
```

While these more complex models predict the data better, they will often predict the new data worse. We have overfitted the data at that point.

### Brain Body Vignette

Plotted below is the relationship between the body size of several primate species and their corresponding brain size. There's not a strong *a priori* reason to think body and brain size are perfectly linearly related. The true relationship between brain and body size could be any number of polynomial or log'd functions. Let's go through a couple to see what's gained and what's lost with trying ever more complex functions to fit the data.

```{r}
#| fig-width: 5
#| fig-height: 4

speciesBrain <- 
  tibble(species = c("afarensis", "africanus", "habilis", "boisei", "rudolfensis", "ergaster", "sapiens"), 
         brain   = c(438, 452, 612, 521, 752, 871, 1350), 
         mass    = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5))

ggplot(data = speciesBrain, aes(x = mass, y = brain))+
  geom_point(shape = 21, color = "blue4", fill = "white")+
  theme_minimal()+
  ggrepel::geom_text_repel(aes(label = species), size = 3)+
  labs(x = "body mass (kg)", y = "brain volume (cc)")
```

Simplest model is a linear one. 

$$\text{Brain Size}_i \sim \text{Normal}(\mu_i, \sigma)$$

$$\mu_i = \alpha + \beta_b\text{Body Mass}_i$$

$$\alpha \sim \text{Normal(0.5,  1)}$$

$$\beta_b \sim \text{Normal(0,  10)}$$

$$\sigma \sim \text{Log-Normal(0,  1)}$$

```{r}
speciesBrain <- speciesBrain %>% 
  mutate(mass_z = (mass - mean(mass))/sd(mass),
         brain_z = (brain - mean(brain))/sd(brain))

b7.0 <- brm(data = speciesBrain,
            family = gaussian,
            brain_z ~ 1 + mass_z,
            prior = c(
              prior(normal(0.5, 1), class = Intercept),
              prior(normal(0, 1), class = b),
              prior(lognormal(0,1), class = sigma)
            ),
            iter = 2000, warmup = 500, seed = 4, cores = 4,
            backend = "cmdstanr", silent = 2, file = "fits/b07.0.5"
            )

b7.0_sim <- as_tibble(b7.0) %>% 
  mutate(simMass = seq(from = -3, to = 3, length.out = n()),
         simBrainEst = Intercept + (b_mass_z * simMass),
         simBrain = rnorm(n(), simBrainEst, sd = sigma))

```

```{r}
#| fig-width: 6
#| fig-height: 4

ggplot() +
  stat_density_2d(data = b7.0_sim, 
                  aes(x = simMass, y = simBrainEst, fill = after_stat(ndensity)),
                  geom = "raster", contour = FALSE) +
  scale_fill_viridis_c(option = "magma") +
  geom_point(data = speciesBrain,
             aes(x = mass_z, y = brain_z), 
             shape = 21, color = "white", fill = "black", lwd = 3, alpha = 1, size =2.5)+
  labs(title = "brain ~ mass", subtitle = "Mu estimate") +
  theme_minimal()+
  ggrepel::geom_text_repel(data = speciesBrain, aes(x = mass_z, y = brain_z, label = species), color = "white", size = 3)+
  # xlim(c(-3,3))+
  # ylim(c(-3,3))+
  scale_x_continuous("body mass (kg)",
                     breaks = at,
                     labels = round(at * sd(speciesBrain$mass) +
                                      mean(speciesBrain$mass)),
                     limits = c(min(speciesBrain$mass_z), max(speciesBrain$mass_z))* 1.5 * sd(speciesBrain$brain_z)) +
  scale_y_continuous("brain volume (cc)",
                     breaks = at,
                     labels = round(at * sd(speciesBrain$brain) +
                                      mean(speciesBrain$brain)),
                     limits = c(min(speciesBrain$brain_z), max(speciesBrain$brain_z)) * 1.5 * sd(speciesBrain$brain_z))+
  guides(fill = "none")


```

```{r}
mass_brain <- speciesBrain %>% 
  rename("mass_obs" = mass_z,
         "brain_obs" = brain_z) %>%
  dplyr::select(c(mass_obs, brain_obs))


as_tibble(b7.0) %>%
  # Add row numbers to track individual draws
  mutate(draw_id = row_number()) %>%
  # Create all combinations of draws and new X values
  crossing(mass_brain) %>% 
  mutate(brain_mu_pred = b_Intercept + (b_mass_z * brain_obs),
         brain_mu_resid= brain_obs - brain_pred_mu,
         brain_mu_
         )
```

