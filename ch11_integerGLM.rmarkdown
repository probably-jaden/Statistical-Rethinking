---
title: "Chapter 11 Integers"
editor: visual
execute:
  echo: false
  warning: false
  message: false
  cache: true
  cache.lazy: false
  fig-align: center
---

```{r setup}
library(tidyverse)
library(brms)
library(rethinking)
library(flextable)
library(tidybayes)
data(UCBadmit)
admit <- UCBadmit
data(chimpanzees)
chimp <- chimpanzees


``` 



We are modeling how seeing other Chimpanzees will effect the decision of a Chimpanzee subject to pull a pro-social option over a null alternative. We switch which hand the pro-social option is on to counter any bias the chimp might have in preferring one hand over the other.

![fig 2](/Users/jaden/Documents/Intellectual Fun/statisticalRethinking/chimpFig.png){width="600px" height="600px"}


Since our outcomes are binary (either pro-social or not) we are going to model the outcome with a binomial (technically bernoulli) distribution. Since we aren't directly using the gaussian this will be our first generalized linear model. 

$$ \text{Lever Pulled}_i \sim \text{Binomial}(1, p_i) $$

Generalized linear models usually need a link to transform their linear predictions into the non-linear nature of non-gaussian outcome distribution parameters. In this case we are modeling $p$ the probability that the left lever is pulled. 

$$ \text{logit}(p_i) = \alpha_\text{ACTOR[i]} + \beta_\text{TREATMENT[i]} $$

$$\alpha_j \sim \text{Normal(0, 1.5)}$$
$$\beta_k \sim \text{Normal(0, 0.5)} $$



```{r}
#| fig-width: 5
#| fig-height: 2

chimp %>% 
  distinct(prosoc_left, condition) %>% 
  mutate(description = str_c("Two food items on ", c("right and no partner",
                                                     "left and no partner",
                                                     "right and partner present",
                                                     "left and partner present"))) %>%
  flextable() %>% 
  width(width = c(1, 1, 4))
  
  
chimp <-
  chimp %>% 
  mutate(treatment = factor(1 + prosoc_left + 2 * condition),
         labels = factor(treatment,
                         levels = 1:4,
                         labels = c("r/n", "l/n", "r/p", "l/p")),
         actor = factor(actor))

```

```{r}
#| fig-width: 8
#| fig-height: 2

chimp_11.1 <- 
  brm(data = chimp, 
      family = binomial,
      bf(pulled_left | trials(1) ~ a + b,
         a ~ 0 + actor, 
         b ~ 0 + treatment,
         nl = TRUE),
      prior = c(prior(normal(0, 1.5), nlpar = a),
                prior(normal(0, 0.5), nlpar = b)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 11, backend = "cmdstanr", file = "fits/b11.01.0")

post_chimp <- as_draws_df(chimp_11.1)

post_chimp %>% 
  pivot_longer(contains("actor")) %>%
  mutate(probability = inv_logit_scaled(value),
         actor       = factor(str_remove(name, "b_a_actor"),
                              levels = 7:1)) %>% 
  
  ggplot(aes(x = probability, y = actor)) +
  geom_vline(xintercept = c(0,1), linetype = 3) +
  stat_pointinterval(.width = .95, size = 1/2) +
  scale_x_continuous(expression(alpha[actor]), limits = 0:1) +
  ylab(NULL) +
  theme_minimal()

```


Each row is a chimpanzee, the numbers corresponding to the values in actor. Four of the individuals—numbers 1, 3, 4, and 5—show a preference for the right lever. Two individuals— numbers 2 and 7—show the opposite preference. Number 2’s preference is very strong in- deed. If you inspect the data, you’ll see that actor 2 never once pulled the right lever in any trial or treatment. 




```{r}
#| fig-width: 8
#| fig-height: 2

post_chimp %>% 
  select(contains("treatment")) %>% 
  set_names("Pro-social on Right / No Partner","Pro-social on Left / No Partner","Pro-social on Right / Partner","Pro-social on Left / Partner") %>% 
  pivot_longer(everything()) %>%
  mutate(probability = inv_logit_scaled(value),
         treatment       = factor(name)) %>% 
  mutate(treatment = fct_rev(treatment)) %>% 
  
  ggplot(aes(x = probability, y = treatment)) +
  geom_vline(xintercept = c(0,1), linetype = 3) +
  stat_pointinterval(.width = .95, size = 1/2) +
  scale_x_continuous(expression(beta[treatment]), limits = 0:1) +
  ylab(NULL) +
  theme_minimal()
```


Looking our different treatment effects we should compare the "Pro-social on Left" against each other with the "Pro-social on Right". Note that probability 1 means that the chimps pulled the left lever 100% of the time, so when we consider the "Prosocial on Right" we should remember the lower the effect value the more pro-social they were.

There doesn't look like to much evidence of a pro-social intention in these data. But let's calculate the differences between no-partner/partner and make sure. 



```{r}
#| fig-width: 8
#| fig-height: 2

post_chimp %>% 
  mutate("Pro-social Right Difference" = b_b_treatment1 - b_b_treatment3,
         "Pro-social Left Difference"  = b_b_treatment2 - b_b_treatment4) %>% 
  dplyr::select(contains("Difference"))%>%
  pivot_longer(everything()) %>%
  ggplot(aes(x = value, y = name)) +
  geom_vline(xintercept = c(0), linetype = 3) +
  stat_pointinterval(.width = .95, size = 1/2) +
  #scale_x_continuous(expression(beta[treatment]), limits = 0:1) +
  ylab(NULL) +
  theme_minimal()
```



These are the constrasts between the no-partner/partner treatments. The scale is log odds of pulling the left lever still. "Pro-social Right Difference" is the difference between no-partner/partner treatments when the prosocial option was on the right. So if there is evidence of more pro-social choice when partner is present, this will show up here as a larger difference, consistent with pulling right more when partner is present. There is indeed weak evidence that individuals pulled left more when the partner was absent, but the compatibility interval is quite wide.  "Pro-social Left Difference" is the same difference, but for when the prosocial option was on the left. Now negative differences would be consistent with more pro-social choice when partner is present. Clearly that is not the case. If anything, individuals chose pro-social more when partner was absent. Overall, there isn’t any compelling evidence of pro-social choice in this experiment.




```{r}

chimp %>% 
  group_by(actor, treatment) %>%
  summarise(proportion = mean(pulled_left)) %>% 
  filter(actor == 1)


p1 <- chimp %>%
  group_by(actor, treatment) %>%
  summarise(proportion = mean(pulled_left)) %>% 
  left_join(chimp %>% distinct(actor, treatment, labels, condition, prosoc_left),
            by = c("actor", "treatment")) %>% 
  mutate(condition = factor(condition)) %>% 
  
  ggplot(aes(x = labels, y = proportion)) +
  geom_hline(yintercept = .5) +
  geom_line(aes(group = prosoc_left),
            linewidth = 1/4) +
  geom_point(aes(color = condition),
             size = 2.5, show.legend = F) + 
  labs(subtitle = "observed proportions")


nd <- 
  chimp %>% 
  distinct(actor, treatment, labels, condition, prosoc_left)

p2 <-
  fitted(chimp_11.1,
         newdata = nd) %>% 
  data.frame() %>% 
  bind_cols(nd) %>% 
  mutate(condition = factor(condition)) %>% 
  
  ggplot(aes(x = labels, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = .5) +
  geom_line(aes(group = prosoc_left), linewidth = 1/4) +
  geom_pointrange(aes(color = condition),
                  fatten = 2.5, show.legend = F) + 
  labs(subtitle = "posterior predictions")

# combine the two ggplots
library(patchwork)

(p1 / p2) &
  scale_color_manual(values = c(2:1)) &
  scale_y_continuous("proportion left lever", 
                     breaks = c(0, .5, 1), limits = c(0, 1)) &
  xlab(NULL) &
  theme(axis.ticks.x = element_blank(),
        panel.background = element_rect(fill = alpha("white", 1/10), linewidth = 0)) &
  facet_wrap(~ actor, nrow = 1, labeller = label_both)


```



The model expects almost no change when adding a partner. Most of the variation in predictions comes from the actor intercepts. Handedness seems to be the big story of this experiment.



We haven’t considered a model that splits into separate index variables the location of the pro-social option and the presence of a partner. Why not? Because the driving hypothesis of the experiment is that the pro-social option will be chosen more when the partner is present. That is an interaction effect—the effect of the pro-social option depends upon a partner being present. But we could build a model without the interaction and the use PSIS or WAIC to compare it to m11.4. You can guess from the posterior distribution of m11.4 what would happen: The simpler model will do just fine, because there doesn’t seem to be any evidence of an interaction between location of the pro-social option and the presence of the partner.



```{r}
chimp <- chimp %>% 
  mutate(side = prosoc_left + 1, # right 1, left 2
         cond = condition + 1) # no partner 1, partner 2


chimp_11.2 <- 
  brm(data = chimp, 
      family = binomial,
      bf(pulled_left | trials(1) ~ a + bs + bc,
         a ~ 0 + actor, 
         bs ~ 0 + side,
         bc ~ 0 + cond,
         nl = TRUE),
      prior = c(prior(normal(0, 1.5), nlpar = a),
                prior(normal(0, 0.5), nlpar = bs),
                prior(normal(0, 0.5), nlpar = bc)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 11, backend = "cmdstanr", file = "fits/b11.02.0")

chimp_11.1 <- add_criterion(chimp_11.1, c("loo", "waic"))
chimp_11.2 <- add_criterion(chimp_11.2, c("loo", "waic"))

loo_compare(chimp_11.1, chimp_11.2, criterion = "loo") %>% print(simplify = F)
```



As we guessed, the model without the interaction is really no worse, in expected predictive accuracy, than the model with it.



In the chimpanzees data context, the models all calculated the likelihood of observing either zero or one pulls of the left-hand lever (a bernoulli outcome). The models did so, because the data were organized such that each row describes the outcome of a single pull. But in principle the same data could be organized differently. As long as we don’t care about the order of the individual pulls, the same information is contained in a count of how many times each individual pulled the left-hand lever, for each combination of predictor variables. This is truly using the binomial distribution as the outcome.



```{r}
chimp_aggregated <- chimp %>%
  group_by(treatment, actor, side, cond) %>%
  summarise(left_pulls = sum(pulled_left)) %>% 
  ungroup()


chimp_11.3 <- 
  brm(data = chimp_aggregated, 
      family = binomial,
      bf(left_pulls | trials(18) ~ a + b,
         a ~ 0 + actor, 
         b ~ 0 + treatment,
         nl = TRUE),
      prior = c(prior(normal(0, 1.5), nlpar = a),
                prior(normal(0, 0.5), nlpar = b)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 11, backend = "cmdstanr", file = "fits/b11.03.0")


bind_rows(as_draws_df(chimp_11.1),
          as_draws_df(chimp_11.3)) %>% 
  mutate(fit = rep(c("chimp_11.1", "chimp_11.3"), each = n() / 2)) %>% 
  pivot_longer(b_a_actor1:b_b_treatment4) %>% 
  
  ggplot(aes(x = value, y = name, color = fit)) +
  stat_pointinterval(.width = .95, size = 2/3,
                     position = position_dodge(width = 0.5)) +
  scale_color_manual(
    values = c("chimp_11.1" = "#E41A1C", "chimp_11.3" = "#377EB8"),
    labels = c("chimp_11.1" = "bernoulli", "chimp_11.3" = "binomial")
  ) +
  labs(
    x = "posterior (log-odds scale)",
    y = NULL,
    color = NULL  # optional: remove legend title
  )



```



As we can see the beta estimated effects for each type of distribution are seamlessly handled to give the same result.




```{r}
chimp_11.1 <- add_criterion(chimp_11.1, "loo")
chimp_11.3 <- add_criterion(chimp_11.3, "loo")


# loo_compare(b11.4, b11.6, criterion = "loo") %>% print(simplify = F) # fails to work because the loo_compare function knows that we have compared models with different size n of observations

loo(chimp_11.1)

loo(chimp_11.3)
```


But when we calculate different model criteria like leave one out (LOO) we get different results. Why? Because in the aggregated binomial model we leave out all 18 observations of one of the chimps not the 1 observation per trial of one of the chimps.


Logistic modeling with different size N





```{r}
x <- rbeta(1e6, .5, .5)
logit_x <- log(x/(1-x)) #exp(x)/(1+exp(x))

my_data <- tibble(og = x, logOdds = logit_x,
                  bell = rnorm(1e6, 0, 1.813))

ggplot(data = my_data)+
  geom_density(aes(x = logOdds), color = "red")+
  geom_density(aes(x = bell), color = "black")
  


my_data2 <- tibble(logOdds1 = rnorm(1e6, 0, 10),
                   prob1 = exp(logOdds1)/(1+exp(logOdds1)),
                   logOdds2 = rnorm(1e6, 0, 1.5),
                   prob2 = exp(logOdds2)/(1+exp(logOdds2)))

ggplot(data = my_data2)+
  geom_density(aes(x = prob1), color = "black")+
  geom_density(aes(x = prob2), color = "red")


my_data3 <- tibble(prob = runif(1e6, min = 0, max = 1),
                   logOdds = log(prob/(1-prob)),
                   bell = rnorm(1e6, 0, sd = 1.81))

ggplot(data = my_data3)+
  geom_density(aes(x = logOdds), color = "black")+
  geom_density(aes(x = bell), color = "red")


a <- rnorm(1e6, 0, 1.5)

my_data4 <- tibble(logOdds1A = a + rnorm(1e6, 0, 10),
                   prob1A = exp(logOdds1A)/(1+exp(logOdds1A)),
                   logOdds1B = a + rnorm(1e6, 0, 10),
                   prob1B = exp(logOdds1B)/(1+exp(logOdds1B)),
                   diff1 = abs(prob1A - prob1B),
                   
                   logOdds2A = a + rnorm(1e6, 0, .5),
                   prob2A = exp(logOdds2A)/(1+exp(logOdds2A)),
                   logOdds2B = a + rnorm(1e6, 0, .5),
                   prob2B = exp(logOdds2B)/(1+exp(logOdds2B)),
                   diff2 = abs(prob2A - prob2B))

ggplot(data = my_data4)+
  geom_density(aes(x = diff1), color = "black")+
  geom_density(aes(x = diff2), color = "red")


ggplot(data = tibble(x = rexp(1e6, rate = 3)), aes(x = x))+
  geom_density()

```

