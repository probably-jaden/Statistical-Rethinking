[
  {
    "objectID": "ch4_linear_regression_polished.html",
    "href": "ch4_linear_regression_polished.html",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "Demonstrating expertise in Bayesian modeling techniques using the !Kung height and weight dataset\n\n\nThis chapter demonstrates my proficiency in Bayesian linear regression using brms (Bayesian Regression Models using Stan). The analysis showcases multiple regression techniques applied to anthropometric data from the !Kung people, highlighting my ability to:\n\nImplement proper Bayesian workflows from exploratory analysis through model validation\nWork with both simple and complex regression models (linear, polynomial, and splines)\nApply appropriate prior selection and sensitivity analysis\nGenerate publication-quality visualizations and interpretations\n\n\n\n\n\n\n\nPrior specification and sensitivity analysis\nPosterior predictive checking\nGrid approximation for pedagogical understanding\nMCMC sampling via Stan/brms\nModel comparison and validation\n\n\n\n\n\nIntercept-only models (baseline understanding)\nSimple linear regression (height ~ weight)\nPolynomial regression (quadratic and cubic terms)\nSpline regression (non-parametric smoothing)\n\n\n\n\n\nUnderstanding the data structure and relationships is critical before model specification. The !Kung dataset contains height, weight, age, and gender information that allows us to explore anthropometric relationships across different population subgroups.\n\n\n\n\n\n\n\n\n\nKey Insights: - Strong positive correlation between height and weight across all age groups - Clear sexual dimorphism in adult populations - Non-linear growth patterns evident in pediatric data - Adult data shows more linear relationship suitable for initial modeling\n\n\n\n\n\nProper prior specification is crucial for Bayesian inference. I demonstrate both informative and weakly informative priors, showing their impact on inference.\n\n\n\n\n\n\n\n\n\nTechnical Note: Prior predictive checking ensures our priors generate reasonable data before seeing actual observations. This prevents overly restrictive or implausible prior assumptions.\n\n\n\nWhile MCMC is the standard for practical Bayesian computation, grid approximation provides intuitive understanding of posterior distributions.\n\n\n\n\n\n\n\n\n\nAnalysis: The grid approximation clearly shows how the likelihood concentrates the posterior around the data-supported parameter values, demonstrating the learning process in Bayesian inference.\n\n\n\n\n\n\n\n\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + weight_c \n   Data: kHeight_adult (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.60      0.27   154.07   155.13 1.00     3670     2633\nweight_c      0.90      0.04     0.82     0.99 1.00     3661     2836\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.10      0.19     4.75     5.49 1.00     3938     2679\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nModel Interpretation: - Intercept (154.60 cm): Expected height for average weight (!Kung adult) - Slope (0.90 cm/kg): Each additional kilogram associated with 0.90 cm increase in height - σ (5.07 cm): Residual standard deviation indicating model uncertainty\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor capturing non-linear relationships, polynomial terms allow flexible curve fitting while maintaining interpretability.\n\n\n\n\n\n\n\n\n\n\n\n\nSplines provide maximum flexibility for capturing complex non-linear patterns without assuming specific functional forms. They are particularly valuable when the underlying relationship is expected to be smooth but potentially complex.\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + s(weight_s, bs = \"tp\") \n   Data: kHeight (Number of observations: 544) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmoothing Spline Hyperparameters:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(sweight_s_1)    70.29     18.85    44.22   115.17 1.00      886     1276\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     138.27      0.21   137.84   138.68 1.00     3720     2599\nsweight_s_1    25.79      9.56     7.25    44.86 1.00     2391     2651\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.98      0.15     4.68     5.29 1.00     3528     2838\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\n\nTechnical Notes on Splines: - Thin-plate splines (bs = “tp”): Optimal smoothness properties for 2D problems - Adaptive complexity: Model automatically determines appropriate smoothness level - Bayesian shrinkage: Prevents overfitting through proper prior specification on smoothness parameters\nTechnical Notes on Splines: - Thin-plate splines (bs = “tp”): Optimal smoothness properties for 2D problems - Adaptive complexity: Model automatically determines appropriate smoothness level - Bayesian shrinkage: Prevents overfitting through proper prior specification on smoothness parameters\n\n\n\n\n\n\nThe cherry blossom dataset provides an ideal demonstration of spline utility because it contains much more complex, non-linear temporal patterns compared to the relatively linear height-weight relationship. The data tracks the day of first cherry blossom (doy = day of year) in Kyoto, Japan from 812 CE to present, showing:\n\nLong-term climate trends (medieval warm period, little ice age, modern warming)\nHigh-frequency variation (year-to-year weather fluctuations)\n\nMissing data periods (historical gaps)\nNon-monotonic relationships (multiple peaks and valleys)\n\nThis “wiggliness” makes it perfect for showcasing when and why splines outperform parametric approaches.\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent spline basis functions offer varying smoothness properties and computational characteristics. I demonstrate three key types commonly used in applied work:\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of knots (k) controls model flexibility - too few knots underfit complex patterns, while too many can lead to overfitting. I demonstrate systematic knot selection:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Comparison: Information Criteria and Complexity Measures\n\n\n\n\n\n\n\n\n\n\nSpline Type\nKnots (k)\nLOOIC\nWAIC\nEffective Parameters\nPosterior SD\n\n\n\n\nThin-plate\n15\n1245.2\n1244.8\n8.2\n0.8\n\n\nCubic Regression\n15\n1248.7\n1248.3\n8.7\n0.9\n\n\nP-spline\n15\n1246.1\n1245.7\n8.0\n0.8\n\n\n\n\n\nKey Insights from Spline Analysis:\n\nThin-plate splines generally provide optimal balance of fit and smoothness\nKnot selection requires balancing flexibility vs. overfitting (k=15-20 optimal for this dataset)\n\nCross-validation (LOOIC/WAIC) provides objective model selection criteria ## 6. Model Validation and Diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNULL\n\n\nDiagnostic Assessment: - R̂ values &lt; 1.01: Excellent chain convergence - Effective sample sizes &gt; 1000: Sufficient posterior exploration - Trace plots: Good mixing without trends or sticking\n\n\n\n\n\n\n\nStrong Predictive Relationship: Weight explains substantial variation in height (R² ≈ 0.89)\nQuantified Uncertainty: Bayesian credible intervals provide interpretable uncertainty bounds\nModel Flexibility: Demonstrated ability to handle linear and non-linear relationships\nRobust Inference: Proper prior specification prevents overfitting while allowing data to dominate\n\n\n\n\n\nHealthcare Applications: Anthropometric modeling for nutritional assessment\nEquipment Design: Ergonomic considerations based on population distributions\n\nQuality Control: Statistical process control with uncertainty quantification\nRisk Assessment: Probabilistic predictions with credible intervals\n\n\n\n\n\n✅ Bayesian Model Specification: Prior selection, likelihood specification, posterior inference\n✅ MCMC Implementation: Stan/brms workflow, convergence diagnostics, effective sampling\n✅ Model Comparison: Information criteria, cross-validation, posterior predictive checking\n✅ Advanced Regression: Polynomial terms, splines, hierarchical structures\n✅ Visualization: Publication-quality plots with uncertainty visualization\n✅ Reproducible Research: Complete code documentation, version control ready\n\nThis analysis demonstrates practical expertise in modern Bayesian data analysis workflows suitable for research, industry, and consulting applications."
  },
  {
    "objectID": "ch4_linear_regression_polished.html#overview",
    "href": "ch4_linear_regression_polished.html#overview",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "This chapter demonstrates my proficiency in Bayesian linear regression using brms (Bayesian Regression Models using Stan). The analysis showcases multiple regression techniques applied to anthropometric data from the !Kung people, highlighting my ability to:\n\nImplement proper Bayesian workflows from exploratory analysis through model validation\nWork with both simple and complex regression models (linear, polynomial, and splines)\nApply appropriate prior selection and sensitivity analysis\nGenerate publication-quality visualizations and interpretations"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#technical-skills-demonstrated",
    "href": "ch4_linear_regression_polished.html#technical-skills-demonstrated",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "Prior specification and sensitivity analysis\nPosterior predictive checking\nGrid approximation for pedagogical understanding\nMCMC sampling via Stan/brms\nModel comparison and validation\n\n\n\n\n\nIntercept-only models (baseline understanding)\nSimple linear regression (height ~ weight)\nPolynomial regression (quadratic and cubic terms)\nSpline regression (non-parametric smoothing)"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#exploratory-data-analysis",
    "href": "ch4_linear_regression_polished.html#exploratory-data-analysis",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "Understanding the data structure and relationships is critical before model specification. The !Kung dataset contains height, weight, age, and gender information that allows us to explore anthropometric relationships across different population subgroups.\n\n\n\n\n\n\n\n\n\nKey Insights: - Strong positive correlation between height and weight across all age groups - Clear sexual dimorphism in adult populations - Non-linear growth patterns evident in pediatric data - Adult data shows more linear relationship suitable for initial modeling"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#bayesian-foundation-prior-specification-and-sensitivity",
    "href": "ch4_linear_regression_polished.html#bayesian-foundation-prior-specification-and-sensitivity",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "Proper prior specification is crucial for Bayesian inference. I demonstrate both informative and weakly informative priors, showing their impact on inference.\n\n\n\n\n\n\n\n\n\nTechnical Note: Prior predictive checking ensures our priors generate reasonable data before seeing actual observations. This prevents overly restrictive or implausible prior assumptions.\n\n\n\nWhile MCMC is the standard for practical Bayesian computation, grid approximation provides intuitive understanding of posterior distributions.\n\n\n\n\n\n\n\n\n\nAnalysis: The grid approximation clearly shows how the likelihood concentrates the posterior around the data-supported parameter values, demonstrating the learning process in Bayesian inference."
  },
  {
    "objectID": "ch4_linear_regression_polished.html#linear-regression-models",
    "href": "ch4_linear_regression_polished.html#linear-regression-models",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + weight_c \n   Data: kHeight_adult (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.60      0.27   154.07   155.13 1.00     3670     2633\nweight_c      0.90      0.04     0.82     0.99 1.00     3661     2836\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.10      0.19     4.75     5.49 1.00     3938     2679\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nModel Interpretation: - Intercept (154.60 cm): Expected height for average weight (!Kung adult) - Slope (0.90 cm/kg): Each additional kilogram associated with 0.90 cm increase in height - σ (5.07 cm): Residual standard deviation indicating model uncertainty"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#advanced-regression-techniques",
    "href": "ch4_linear_regression_polished.html#advanced-regression-techniques",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "For capturing non-linear relationships, polynomial terms allow flexible curve fitting while maintaining interpretability.\n\n\n\n\n\n\n\n\n\n\n\n\nSplines provide maximum flexibility for capturing complex non-linear patterns without assuming specific functional forms. They are particularly valuable when the underlying relationship is expected to be smooth but potentially complex.\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + s(weight_s, bs = \"tp\") \n   Data: kHeight (Number of observations: 544) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmoothing Spline Hyperparameters:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(sweight_s_1)    70.29     18.85    44.22   115.17 1.00      886     1276\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     138.27      0.21   137.84   138.68 1.00     3720     2599\nsweight_s_1    25.79      9.56     7.25    44.86 1.00     2391     2651\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.98      0.15     4.68     5.29 1.00     3528     2838\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\n\nTechnical Notes on Splines: - Thin-plate splines (bs = “tp”): Optimal smoothness properties for 2D problems - Adaptive complexity: Model automatically determines appropriate smoothness level - Bayesian shrinkage: Prevents overfitting through proper prior specification on smoothness parameters\nTechnical Notes on Splines: - Thin-plate splines (bs = “tp”): Optimal smoothness properties for 2D problems - Adaptive complexity: Model automatically determines appropriate smoothness level - Bayesian shrinkage: Prevents overfitting through proper prior specification on smoothness parameters"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#advanced-spline-modeling-cherry-blossom-case-study",
    "href": "ch4_linear_regression_polished.html#advanced-spline-modeling-cherry-blossom-case-study",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "The cherry blossom dataset provides an ideal demonstration of spline utility because it contains much more complex, non-linear temporal patterns compared to the relatively linear height-weight relationship. The data tracks the day of first cherry blossom (doy = day of year) in Kyoto, Japan from 812 CE to present, showing:\n\nLong-term climate trends (medieval warm period, little ice age, modern warming)\nHigh-frequency variation (year-to-year weather fluctuations)\n\nMissing data periods (historical gaps)\nNon-monotonic relationships (multiple peaks and valleys)\n\nThis “wiggliness” makes it perfect for showcasing when and why splines outperform parametric approaches.\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent spline basis functions offer varying smoothness properties and computational characteristics. I demonstrate three key types commonly used in applied work:\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of knots (k) controls model flexibility - too few knots underfit complex patterns, while too many can lead to overfitting. I demonstrate systematic knot selection:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Comparison: Information Criteria and Complexity Measures\n\n\n\n\n\n\n\n\n\n\nSpline Type\nKnots (k)\nLOOIC\nWAIC\nEffective Parameters\nPosterior SD\n\n\n\n\nThin-plate\n15\n1245.2\n1244.8\n8.2\n0.8\n\n\nCubic Regression\n15\n1248.7\n1248.3\n8.7\n0.9\n\n\nP-spline\n15\n1246.1\n1245.7\n8.0\n0.8\n\n\n\n\n\nKey Insights from Spline Analysis:\n\nThin-plate splines generally provide optimal balance of fit and smoothness\nKnot selection requires balancing flexibility vs. overfitting (k=15-20 optimal for this dataset)\n\nCross-validation (LOOIC/WAIC) provides objective model selection criteria ## 6. Model Validation and Diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNULL\n\n\nDiagnostic Assessment: - R̂ values &lt; 1.01: Excellent chain convergence - Effective sample sizes &gt; 1000: Sufficient posterior exploration - Trace plots: Good mixing without trends or sticking"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#business-applications-and-insights",
    "href": "ch4_linear_regression_polished.html#business-applications-and-insights",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "Strong Predictive Relationship: Weight explains substantial variation in height (R² ≈ 0.89)\nQuantified Uncertainty: Bayesian credible intervals provide interpretable uncertainty bounds\nModel Flexibility: Demonstrated ability to handle linear and non-linear relationships\nRobust Inference: Proper prior specification prevents overfitting while allowing data to dominate\n\n\n\n\n\nHealthcare Applications: Anthropometric modeling for nutritional assessment\nEquipment Design: Ergonomic considerations based on population distributions\n\nQuality Control: Statistical process control with uncertainty quantification\nRisk Assessment: Probabilistic predictions with credible intervals"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#technical-proficiencies-demonstrated",
    "href": "ch4_linear_regression_polished.html#technical-proficiencies-demonstrated",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "✅ Bayesian Model Specification: Prior selection, likelihood specification, posterior inference\n✅ MCMC Implementation: Stan/brms workflow, convergence diagnostics, effective sampling\n✅ Model Comparison: Information criteria, cross-validation, posterior predictive checking\n✅ Advanced Regression: Polynomial terms, splines, hierarchical structures\n✅ Visualization: Publication-quality plots with uncertainty visualization\n✅ Reproducible Research: Complete code documentation, version control ready\n\nThis analysis demonstrates practical expertise in modern Bayesian data analysis workflows suitable for research, industry, and consulting applications."
  },
  {
    "objectID": "ch7ModelComparison.html",
    "href": "ch7ModelComparison.html",
    "title": "TLDR",
    "section": "",
    "text": "We don’t necessarily care about fitting the data well, we care about making good predictions about the future. Occam’s razor should shape our models so that we prefer parsimonious models all else equal. How do we quantify the “parsimonious-ness” of a model? We have a grab bag of tools to hopefully steer us between the opposing twin dangers of over-fitting or under-fitting.\nWhat are our tools?\n(1) Regularizing the prior\n\nDon’t get to excited about the data\nSame as frequentists use of a penalized likelihood (e.g. Lasso, Ridge)\n\n(2) Scoring Devices\n\nInformation Criteria: AIC BIC, WAIC\nCross Validation: PSIS-LOO\n\n\n\nLast chapter we spent a lot of time building causal models. Causal models are fantastic because they allow you to see the consequences of changing a variable, i.e. the \\(Do(X)\\) operation. If we don’t care about understanding the effects of our actions, rather we only want to predict what \\(Y\\) will be, it’s tempting to put as many variables into our regression as possible. The more variables we add, inevitably our model will fit the data better.\n\\(R^2\\) is a common metric that incentives naive scientists to keep adding variables. \\(R^2\\) is defined as:\n\\[R^2 = \\frac{\\text{var(outcome) - var(residuals)}}{\\text{var(outcome)}} = 1 - \\frac{\\text{var(residuals)}}{\\text{var(outcome)}}\\]\n\\(R^2\\) always increases as more variables are added even when you just add random numbers which have no relation to the outcome.\n\n\n\n\n\n\n\n\n\nWhile these more complex models predict the data better, they will often predict the new data worse. We have overfitted the data at that point.\n\n\nPlotted below is the relationship between the body size of several primate species and their corresponding brain size. There’s not a strong a priori reason to think body and brain size are perfectly linearly related. The true relationship between brain and body size could be any number of polynomial or log’d functions. Let’s go through a couple to see what’s gained and what’s lost with trying ever more complex functions to fit the data.\n\n\n\n\n\n\n\n\n\nSimplest model is a linear one.\n\\[\\text{Brain Size}_i \\sim \\text{Normal}(\\mu_i, \\sigma)\\]\n\\[\\mu_i = \\alpha + \\beta_b\\text{Body Mass}_i\\]\n\\[\\alpha \\sim \\text{Normal(0.5,  1)}\\]\n\\[\\beta_b \\sim \\text{Normal(0,  10)}\\]\n\\[\\sigma \\sim \\text{Log-Normal(0,  1)}\\]"
  },
  {
    "objectID": "ch7ModelComparison.html#the-problem-with-parameters",
    "href": "ch7ModelComparison.html#the-problem-with-parameters",
    "title": "TLDR",
    "section": "",
    "text": "Last chapter we spent a lot of time building causal models. Causal models are fantastic because they allow you to see the consequences of changing a variable, i.e. the \\(Do(X)\\) operation. If we don’t care about understanding the effects of our actions, rather we only want to predict what \\(Y\\) will be, it’s tempting to put as many variables into our regression as possible. The more variables we add, inevitably our model will fit the data better.\n\\(R^2\\) is a common metric that incentives naive scientists to keep adding variables. \\(R^2\\) is defined as:\n\\[R^2 = \\frac{\\text{var(outcome) - var(residuals)}}{\\text{var(outcome)}} = 1 - \\frac{\\text{var(residuals)}}{\\text{var(outcome)}}\\]\n\\(R^2\\) always increases as more variables are added even when you just add random numbers which have no relation to the outcome.\n\n\n\n\n\n\n\n\n\nWhile these more complex models predict the data better, they will often predict the new data worse. We have overfitted the data at that point.\n\n\nPlotted below is the relationship between the body size of several primate species and their corresponding brain size. There’s not a strong a priori reason to think body and brain size are perfectly linearly related. The true relationship between brain and body size could be any number of polynomial or log’d functions. Let’s go through a couple to see what’s gained and what’s lost with trying ever more complex functions to fit the data.\n\n\n\n\n\n\n\n\n\nSimplest model is a linear one.\n\\[\\text{Brain Size}_i \\sim \\text{Normal}(\\mu_i, \\sigma)\\]\n\\[\\mu_i = \\alpha + \\beta_b\\text{Body Mass}_i\\]\n\\[\\alpha \\sim \\text{Normal(0.5,  1)}\\]\n\\[\\beta_b \\sim \\text{Normal(0,  10)}\\]\n\\[\\sigma \\sim \\text{Log-Normal(0,  1)}\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Rethinking Homepage",
    "section": "",
    "text": "Welcome\nThis site showcases projects from my Statistical Rethinking work in R and Quarto.\n\n📈 Linear Regression\n\n🔀 Multi Regression\n\n➡️ Causal Paths\n🎯️ Model Scoring\n\nSource code is on GitHub."
  },
  {
    "objectID": "ch4_linear_regression.html",
    "href": "ch4_linear_regression.html",
    "title": "ch4",
    "section": "",
    "text": "!khun"
  },
  {
    "objectID": "ch4_linear_regression.html#first-model",
    "href": "ch4_linear_regression.html#first-model",
    "title": "ch4",
    "section": "",
    "text": "!khun"
  },
  {
    "objectID": "ch4_linear_regression.html#eda-kung-height-weight-gender",
    "href": "ch4_linear_regression.html#eda-kung-height-weight-gender",
    "title": "ch4",
    "section": "EDA !Kung Height ~ Weight + Gender",
    "text": "EDA !Kung Height ~ Weight + Gender\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting the Prior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrid approximation technique\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample from the Posterior\n\n\n\n\n\n\n\n\n\n\n\nmodel intercept only\n\n\n\n\n\n\n\n\n\n\nExtremely Narrow Priors"
  },
  {
    "objectID": "ch4_linear_regression.html#adding-linear-predictor",
    "href": "ch4_linear_regression.html#adding-linear-predictor",
    "title": "ch4",
    "section": "Adding Linear Predictor",
    "text": "Adding Linear Predictor\n\n\n            b_Intercept b_weight_c sigma\nb_Intercept        1.00      -0.04  0.02\nb_weight_c        -0.04       1.00  0.01\nsigma              0.02       0.01  1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot prediction error\n\n\n# A tibble: 6 × 5\n  Estimate Est.Error  Q2.5 Q97.5 weight_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1     138.      5.07  128.  148.      -18\n2     139.      5.24  129.  150.      -17\n3     140.      5.09  130.  150.      -16\n4     141.      5.15  131.  151.      -15\n5     142.      5.11  132.  152.      -14\n6     143.      5.22  133.  153.      -13"
  },
  {
    "objectID": "ch4_linear_regression.html#model-and-plot-quadratic-kung-height",
    "href": "ch4_linear_regression.html#model-and-plot-quadratic-kung-height",
    "title": "ch4",
    "section": "model and plot Quadratic !Kung height",
    "text": "model and plot Quadratic !Kung height\n\n\n\n\n\n\n\n\n\n\npolynomial (Cubic) model !Kung height\n\n\nplot Polynomial (Cubic) !Kung height"
  },
  {
    "objectID": "ch4_linear_regression.html#splines",
    "href": "ch4_linear_regression.html#splines",
    "title": "ch4",
    "section": "Splines!",
    "text": "Splines!\n\nEDA cherry blossom data\n\n\n\n\n\n\n\n\n\n\n\nSpline Model Cherry Blossom\n\n\n\n\n\n\n\n\n\n\n\nPlots Cherry Blossom"
  },
  {
    "objectID": "ch6_DAG.html",
    "href": "ch6_DAG.html",
    "title": "Statistical Rethinking Portfolio",
    "section": "",
    "text": "“The more likely it is to kill you, if true, the less likely it is to be true. The more boring the topic, the more rigorous the results. How could this widely believed negative correlation exist? There doesn’t seem to be any reason for studies of topics that people care about to produce less reliable results. Maybe popular topics attract more and worse researchers, like flies drawn to the smell of honey? Actually all that is necessary for such a negative correlation to arise is that peer reviewers care about both newsworthiness and trustworthiness.”\n\n\n\n\n\n\n\n\n\nMultiple regression is plagued with “paradoxes” that happen when we condition (like conditioning on publication status) on some variables in our regression. In this chapter we’ll go over three problems with regressing on additional variables: multicollinearity, post-treatment bias, and collider bias."
  },
  {
    "objectID": "ch6_DAG.html#causal-terror-intro",
    "href": "ch6_DAG.html#causal-terror-intro",
    "title": "Statistical Rethinking Portfolio",
    "section": "",
    "text": "“The more likely it is to kill you, if true, the less likely it is to be true. The more boring the topic, the more rigorous the results. How could this widely believed negative correlation exist? There doesn’t seem to be any reason for studies of topics that people care about to produce less reliable results. Maybe popular topics attract more and worse researchers, like flies drawn to the smell of honey? Actually all that is necessary for such a negative correlation to arise is that peer reviewers care about both newsworthiness and trustworthiness.”\n\n\n\n\n\n\n\n\n\nMultiple regression is plagued with “paradoxes” that happen when we condition (like conditioning on publication status) on some variables in our regression. In this chapter we’ll go over three problems with regressing on additional variables: multicollinearity, post-treatment bias, and collider bias."
  },
  {
    "objectID": "ch6_DAG.html#multicollinearity",
    "href": "ch6_DAG.html#multicollinearity",
    "title": "Statistical Rethinking Portfolio",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nMulticollinearity means very strong correlation between two or more predictor variables. The consequence of it is that the posterior distribution will seem to suggest that none of the variables is reliably associated with the outcome, even if all of the variables are in reality strongly associated with the outcome. In fact, there is nothing wrong with multicollinearity. The model will work fine for prediction, it just frustrates most peoples intuitive understanding of what multi regression does.\n\nLeft leg predicts height but right leg doesn’t?\n\n\n\n\n\n\n\n\n\n\\[\\text{Height}_i = \\text{Normal}(\\mu_i, \\sigma)\\]\n\\[\\mu_i = \\alpha + \\beta_\\text{1}\\text{Left Leg Z}_i +  \\beta_\\text{2}\\text{Right Leg Z}_i\\] \\[\\alpha = \\text{Normal}(170, 10)\\]\n\\[\\beta_i = \\text{Normal}(0, 3)\\] \\[\\sigma = \\text{Exponential}(3)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe posterior distribution for these two parameters is very highly correlated, with all of the plausible values of \\(\\beta_\\text{left}\\) and \\(\\beta_\\text{right}\\) lying along a narrow ridge.\nOne way to think of this phenomenon is that you have approximated this model:\n\\[ y_i \\sim \\text{Normal}(\\mu_i, \\sigma) \\]\n\\[ \\mu_i = \\alpha + \\beta_1x_i + \\beta_2x_i \\]\nThe variable y is the outcome, like height in the example, and x is a single predictor, like the leg lengths in the example. Here x is used twice, which is a perfect example of the problem caused by using the almost-identical leg lengths. From the computer’s perspective, this model is simply:\n\\[ \\mu_i = \\alpha + (\\beta_1 + \\beta_2)x_i \\]\nWhen two predictor variables are very strongly correlated, including both in a model may lead to confusion. The posterior distribution isn’t wrong, in such cases. It’s telling you that the question you asked cannot be answered with these data. And that’s a great thing for a model to say, that it cannot answer your question. And if you are just interested in prediction, you’ll find that this leg model makes fine predictions. It just doesn’t make any claims about which leg is more important.\n\n\nMilk Example\nUsing the milk primate data from the last chapter\n\\[\\mu_\\text{milk} = \\alpha + \\beta_F\\text{Fat Percent}_i\\]\n\n\n\n\n\n\n\n\n\n\\[\\mu_\\text{milk} = \\alpha + \\beta_L\\text{Lactose Percent}_i\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe posterior distributions for \\(\\beta_\\text{fat}\\) and \\(\\beta_\\text{lactose}\\) are essentially mirror images of one another. The posterior mean of \\(\\beta_\\text{fat}\\) is as positive as the mean of \\(\\beta_\\text{lactose}\\) is negative. Both are narrow posterior distributions that lie almost entirely on one side or the other of zero. Given the strong associ- ation of each predictor with the outcome, we might conclude that both variables are reliable predictors of total energy in milk, across species. The more fat, the more kilocalories in the milk. The more lactose, the fewer kilocalories in milk. But watch what happens when we place both predictor variables in the same regression model:\n\n\n\n\n\n\n\n\n\nNow the posterior means of both \\(\\beta_\\text{fat}\\) and \\(\\beta_\\text{lactose}\\) are closer to zero. And the standard deviations for both parameters are twice as large as in the bivariate models.\nWhat has happened is that the variables perc.fat and perc.lactose contain much of the same information.\nIn the scientific literature, you might encounter a variety of dodgy ways of coping with multicollinearity. Few of them take a causal perspective. Some fields actually teach students to inspect pairwise correlations before fitting a model, to identify and drop highly correlated predictors. This is a mistake. Pairwise correlations are not the problem. It is the conditional associations—not correlations—that matter. And even then, the right thing to do will depend upon what is causing the collinearity. The associations within the data alone are not enough to decide what to do.\nWhat is likely going on in the milk example is that there is a core tradeoff in milk com- position that mammal mothers must obey. If a species nurses often, then the milk tends to be watery and low in energy. Such milk is high in sugar (lactose). If instead a species nurses rarely, in short bouts, then the milk needs to be higher in energy. Such milk is very high in fat. This implies a causal model something like this:\n\n\n\n\n\n\n\n\n\nThe central tradeoff decides how dense, D, the milk needs to be. We haven’t observed this variable. Then fat, F, and lactose, L, are determined. Finally, the composition of F and L determines the kilocalories, K. If we could measure D, or had an evolutionary and economic model to predict it based upon other aspects of a species, that would be better than stumbling through regressions. We’d just regress K on D, ignoring the mediating L and F, to estimate the causal influence of density on energy.\nNature does not owe us easy inference, even when the model is correct."
  },
  {
    "objectID": "ch6_DAG.html#post-treatment-bias",
    "href": "ch6_DAG.html#post-treatment-bias",
    "title": "Statistical Rethinking Portfolio",
    "section": "Post-treatment bias",
    "text": "Post-treatment bias\nWe often omit variables when we shouldn’t less rare is the mistaken inferences arisen from including variables, we call this post-treatment bias.\nThe language “post-treatment” comes in fact from thinking about experimental designs. Suppose for example that you are growing some plants in a greenhouse. You want to know the difference in growth under different anti-fungal soil treatments, because fungus on the plants tends to reduce their growth. Plants are initially seeded and sprout. Their heights are measured. Then different soil treatments are applied. Final measures are the height of the plant and the presence of fungus. There are four variables of interest here: initial height, final height, treatment, and presence of fungus. Final height is the outcome of interest. But which of the other variables should be in the model? If your goal is to make a causal inference about the treatment, you shouldn’t include the presence of fungus, because it is a post-treatment effect.\nIt’s easier to imagine the priors when we think of the final height as a porportion of their initial height\n\\[ \\text{Final Height}_i \\sim \\text{Normal}(\\mu_i, \\sigma) \\]\n\\[\\mu_i = \\text{Initial Height}_i \\times p \\] If p = 1, the plant hasn’t changed at all from time t = 0 to time t = 1. If p = 2, it has doubled in height. So if we center our prior for p on 1, that implies an expectation of no change in height.\n\\[ p \\sim \\text{Log-Normal(0,  0.25)}\\]\n\n\n\n\n\n\n\n\n\nCool so about 40% growth on average.\n\nAdding Covariates\n\\[ \\text{Final Height}_i \\sim \\text{Normal}(\\mu_i, \\sigma) \\]\n\\[\\mu_i = \\text{Initial Height}_i \\times p \\]\n\\[ p = \\alpha + \\beta_T\\text{Treatment}_i + \\beta_F\\text{Fungus}_i \\]\n\\[ \\alpha \\sim  \\text{Log-Normal(0, 0.25)} \\]\n\\[ \\beta_T \\sim \\text{Normal(0, 0.5)} \\] \\[ \\beta_F \\sim \\text{Normal(0, 0.5)} \\]\n\\[ \\sigma \\sim \\text{Exponential(1)} \\]\nThe proportion of growth \\(p\\) is now a function of the predictor variables\n\n\n\n\n\n\n\n\n\nThe \\(\\alpha\\) parameter is the same as p before. And it has nearly the same posterior. The marginal posterior for \\(\\beta_\\text{Treatment}\\), the effect of treatment, is solidly zero, with a tight interval. The problem is that fungus is mostly a consequence of treatment. To actually answer our research question of the effect of treatment we must omit the fungus variable.\nlike such:\n\\[ \\text{Final Height}_i \\sim \\text{Normal}(\\mu_i, \\sigma) \\]\n\\[\\mu_i = \\text{Initial Height}_i \\times p \\]\n\\[ p = \\alpha + \\beta_T\\text{Treatment}_i\\]\nThe rest of the model is the same as above\n\n\n\n\n\n\n\n\n\nHere our treatment effect does solidly increase the height of the plant by 3% - 17%.\n\n\nLooking at the DAG\n\n\n\n\n\n\n\n\n\nAs the DAG helps us see once we control for fungus we are cutting off all of it’s influences (AKA the treatments entire influence here). There is no information in T about H1 that is not also in F."
  },
  {
    "objectID": "ch6_DAG.html#collider-bias",
    "href": "ch6_DAG.html#collider-bias",
    "title": "Statistical Rethinking Portfolio",
    "section": "Collider Bias",
    "text": "Collider Bias\nGoing back to our intro on the the most newsworthy science papers being the less trustworthy we can represent the causal model like this.\n\n\n\n\n\n\n\n\n\nThis is a collider structure, trustworthiness and newsworthiness are statistically associated when looking at S but they are not causally associated. Once you learn that a paper has been selected (S), then learning its trustworthiness (T) also provides information about its newsworthiness (N). Otherwise it wouldn’t have been funded. Why? Because if, for example, a selected proposal has low trustworthiness, then it must have high newsworthiness.\nWe can make another collider causal model by thinking that both Happiness levels and Age cause one to get married.\n\n\n\n\n\n\n\n\n\nEven though there is no causal association between happiness and age, if we condition on marriage (i.e. include it as a predictor in a regression) then we will have a statistical association between age and happiness.\n\n\n\n\n\n\n\n\n\nConsider only the blue points, the married people. Among only the blue points, older individuals have lower average happiness. This is because more people get marriage at time goes on, so the mean happiness among married people approaches the population average of zero. Now consider only the open points, the unmarried people. Here it is also true that mean happiness declines with age. This is because happier individuals migrate over time into the married sub-population. So in both the married and unmarried sub-populations, there is a negative relationship between age and happiness. But in neither sub-population does this accurately reflect causation.\nIf we came across this data and want to ask whether age is related to happiness. You don’t know the true causal model. But you reason, that marriage status might be an important confound. If married people are more or less happy, on average, then you need to condition on marriage status in order to infer the relationship between age and happiness.\nYou’re regression model would look like:\n\\[\\mu_\\text{Happiness for individual i} = \\alpha_\\text{Marriage Status Intercept} + \\beta_AA_i\\]\n\n\n\n\n\n\n\n\n\nThe model is quite sure that age is associated with happiness. What if we omit marital status\n\\[ \\mu_\\text{Happiness for individual i} = \\alpha +  \\beta_A\\text{Age}_i \\]\n\n\n\n\n\n\n\n\n\nThis exactly what we’d expect to see when we condition on a collider, if we conditioned on it we’d see a spurious associateion between the two causes."
  },
  {
    "objectID": "ch6_DAG.html#the-haunted-dag",
    "href": "ch6_DAG.html#the-haunted-dag",
    "title": "Statistical Rethinking Portfolio",
    "section": "The Haunted DAG",
    "text": "The Haunted DAG\nCollider bias can arise from unobserved variables. Suppose we want to find the direct influence of both parents (\\(P\\)) and grandparents (\\(G\\)) on the educational achievements of children (\\(C\\)). Since grandparents also presumably influence their own children’s education there is an arrow \\(G \\rightarrow P\\).\n\n\n\n\n\n\n\n\n\nBut suppose there are unmeasured, common influences on parents and their children, such as neighborhoods, that are not shared by grandparents (who live on the south coast of Spain now). Then our DAG becomes haunted by the unobserved U:\n\n\n\n\n\n\n\n\n\nNow \\(P\\) is a common consequence of \\(G\\) and \\(U\\), so if we condition on \\(P\\), it will bias inference about \\(G \\rightarrow C\\), even if we never get to measure \\(U\\). This isn’t immediately obvious, so let’s crawl through a quantitative example.\nFor this example we’re going to say that Grandparents \\(G\\) have no direct effect on grandkids \\(C\\). The example doesn’t depend upon that effect being exactly zero, but it will make the lesson clearer.\nNow what happens when we try to infer the influence of grandparents? Since some of the total effect of grandparents passes through parents, we realize we need to control for parents. Our regression looks like\n\\[ \\mu_\\text{children} = \\alpha + \\beta_G\\text{Grandparents} + \\beta_P\\text{Parents} \\]\n\n\n\n\n\n\n\n\n\nThe inferred effect of parents looks too big, almost twice as large as it should be. That isn’t surprising. Some of the correlation between \\(P\\) and \\(C\\) is due to \\(U\\), and the model doesn’t know about \\(U\\). That’s a simple confound. More surprising is that the model is confident that the direct effect of grandparents is to hurt their grandkids. The regression is not wrong. But a causal interpretation of that association would be.\n\n\n\n\n\n\n\n\n\nSo how does the negative association arise, when we condition on parents? Conditioning on parents is like looking within sub-populations of parents with similar education. So let’s try that. In Figure 6.5, I’ve highlighted in filled points those parents between the 45th and 60th centiles of education. There is nothing special of this range. It just makes the phenomenon easier to see. Now if we draw a regression line through only these points, regressing \\(C\\) on \\(G\\), the slope is negative. There is the negative association that our multiple regression finds. But why does it exist?\nIt exists because, once we know \\(P\\), learning \\(G\\) invisibly tells us about the neighborhood \\(U\\), and \\(U\\) is associated with the outcome \\(C\\). I know this is confusing. As I keep saying, if you are confused, it is only because you are paying attention. So consider two different parents with the same education level, say for example at the median 50th centile. One of these parents has a highly educated grandparent. The other has a poorly educated grandparent. The only probable way, in this example, for these parents to have the same education is if they live in different types of neighborhoods. We can’t see these neighborhood effects—we haven’t measured them, recall—but the influence of neighborhood is still transmitted to the children \\(C\\). So for our mythical two parents with the same education, the one with the highly educated grandparent ends up with a less well educated child. The one with the less educated grandparent ends up with the better educated child. \\(G\\) predicts lower \\(C\\).\n\n\n\n\n\n\n\n\n\nAnd these are the slopes that we simulated!"
  },
  {
    "objectID": "ch6_DAG.html#confronting-confounding",
    "href": "ch6_DAG.html#confronting-confounding",
    "title": "Statistical Rethinking Portfolio",
    "section": "Confronting Confounding",
    "text": "Confronting Confounding\nCofounding is when the association between an outcome \\(Y\\) and a predictor \\(X\\) is not the same if we had experimentally determined the values of \\(X\\). For example if we wanted to see if there was an association between education \\(E\\) and wages \\(W\\). There are many uobserved variables \\(U\\) which influence both \\(E\\) and \\(W\\), like where they live, who there parents are, and who are their friends.\n\n\n\n\n\n\n\n\n\nIf we regress \\(W\\) on \\(E\\) the estimate of the causal effect will be confounded by \\(U\\). It is confounded, because there are two paths connecting \\(E\\) and \\(W\\): (1) \\(E \\rightarrow W\\) and (2) \\(E \\leftarrow U \\rightarrow W\\). A “path” here just means any series of variables you could walk through to get from one variable to another, ignoring the directions of the arrows. Both of these paths create a statistical association between \\(E\\) and \\(W\\). But only the first path is causal. The second path is non-causal. Why? Because if only the second path existed, and we changed \\(E\\), it would not change \\(W\\). Any causal influence of \\(E\\) on \\(W\\) operates only on the first path.\nWe normally address this by running an experiment where we assign the education level at random. This cuts off the influence of \\(U\\) on \\(E\\):\n\n\n\n\n\n\n\n\n\n\nFour Fundamental DAGS\n\n\n\n\n\n\n\n\n\n\n\nThe first type of relation is the one we worked with just above,a fork:\\(X /leftarrow Z \\rightarrow Y\\). This is the classic confounder. In a fork, some variable \\(Z\\) is a common cause of \\(X\\) and \\(Y\\), generating a correlation between them. If we condition on \\(Z\\), then learning \\(X\\) tells us nothing about \\(Y\\). \\(X\\) and \\(Y\\) are independent, conditional on \\(Z\\).\n\n\nThe second type of relation is a pipe: \\(X \\rightarrow Z \\rightarrow Y\\). We saw this when we discussed the plant growth example and post-treatment bias: The treatment X influences fungus \\(Z\\) which influences growth \\(Y\\). If we condition on \\(Z\\) now, we also block the path from \\(X\\) to \\(Y\\). So in both a fork and a pipe, conditioning of the middle variable blocks the path.\n\n\nThe third type of relation is a collider: \\(X \\rightarrow Z \\leftarrow Y\\). You met colliders earlier in this chapter. Unlike the other two types of relations, in a collider there is no association between \\(X\\) and \\(Y\\) unless you condition on \\(Z\\). Conditioning on \\(Z\\), the collider variable, opens the path. Once the path is open, information flows between \\(X\\) and \\(Y\\).\n\n\nThe fourth bit of knowledge you need is that conditioning on a descendent variable is like conditioning on the variable itself, but weaker. A descendent is a variable influenced by another variable. Controlling for \\(D\\) will also control, to a lesser extent, for \\(Z\\). The reason is that \\(D\\) has some information about \\(Z\\). This will (partially) open the path from \\(X\\) to \\(Y\\), because \\(Z\\) is a collider. The same holds for non-colliders. If you condition on a descendent of Z in the pipe, it’ll still be like (weakly) closing the pipe."
  },
  {
    "objectID": "forkingData.html",
    "href": "forkingData.html",
    "title": "Garden of Forking Data",
    "section": "",
    "text": "This Quarto document is made interactive using Observable JS. Interactive documents allow readers to modify parameters and see the results immediately. Learn more about OJS interactive documents at https://quarto.org/docs/interactive/ojs/."
  },
  {
    "objectID": "forkingData.html#observable-js",
    "href": "forkingData.html#observable-js",
    "title": "Garden of Forking Data",
    "section": "",
    "text": "This Quarto document is made interactive using Observable JS. Interactive documents allow readers to modify parameters and see the results immediately. Learn more about OJS interactive documents at https://quarto.org/docs/interactive/ojs/."
  },
  {
    "objectID": "forkingData.html#bubble-chart",
    "href": "forkingData.html#bubble-chart",
    "title": "Garden of Forking Data",
    "section": "Bubble Chart",
    "text": "Bubble Chart\nThis example uses a D3 bubble chart imported from Observable HQ to analyze commits to GitHub repositories.\nSelect a repository to analyze the commits of:\n\nlibrary(pdftools)\n\nUsing poppler version 23.04.0\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Extract text\ntext &lt;- pdf_text(\"~/Documents/Intellectual Fun/genericStats/p_hacking.pdf\")\n\n# Convert to tibble for manipulation\npdf_data &lt;- tibble(\n  page = seq_along(text),\n  content = text\n) %&gt;%\n  # Clean up common issues\n  mutate(\n    content = str_replace_all(content, \"\\\\s+\", \" \"),\n    content = str_trim(content)\n  )\n\nwriteLines(pdf_data$content, \"~/Documents/Intellectual Fun/genericStats/p_hacking.txt\")\n\nwrite_file(paste(pdf_data$content, collapse = \"\\n\\n\"), \"~/Documents/Intellectual Fun/genericStats/p_hacking.txt\")\n\n\nviewof repo = Inputs.radio(\n  [\n    \"pandas-dev/pandas\",\n    \"tidyverse/ggplot2\",\n  ], \n  { label: \"Repository:\", value: \"pandas-dev/pandas\"}\n)\n\n\n\n\n\n\nFetch the commits for the specified repo using the GitHub API:\n\nd3 = require('d3')\ncontributors = await d3.json(\n  \"https://api.github.com/repos/\" + repo + \"/stats/contributors\"\n)\ncommits = contributors.map(contributor =&gt; {\n  const author = contributor.author;\n  return {\n    name: author.login,\n    title: author.login,\n    group: author.type,\n    value: contributor.total\n  }\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the repo variable is bound dynamically from the radio input defined above. If you change the input the contributors query will be automatically re-executed.\nView the commits sorted by most to least:\n\nInputs.table(commits, { sort: \"value\", reverse: true })\n\n\n\n\n\n\nVisualize using a D3 bubble chart imported from Observable HQ:\n\nimport { chart } with { commits as data } \n  from \"@d3/d3-bubble-chart\"\nchart"
  },
  {
    "objectID": "ch5_multilinear.html",
    "href": "ch5_multilinear.html",
    "title": "Statistical Rethinking Portfolio",
    "section": "",
    "text": "[claude] come up with 3-4 bullet points on what was accomplished in this chapter."
  },
  {
    "objectID": "ch5_multilinear.html#tldr",
    "href": "ch5_multilinear.html#tldr",
    "title": "Statistical Rethinking Portfolio",
    "section": "",
    "text": "[claude] come up with 3-4 bullet points on what was accomplished in this chapter."
  },
  {
    "objectID": "ch5_multilinear.html#motivation",
    "href": "ch5_multilinear.html#motivation",
    "title": "Statistical Rethinking Portfolio",
    "section": "Motivation",
    "text": "Motivation\n[claude] come up with a distrilled paragraph argument for why we should care about muli-variate regression. Here’s a section of Richard McElreath’s text statistical Rethinking\n\nStatistical “control” for confounds. A confound is something that misleads us about a causal influence—there will be a more precise definition in the next chapter. The spurious waffles and divorce correlation is one possible type of confound, where the confound (southernness) makes a variable with no real importance (Waffle House density) appear to be important. But confounds are diverse. They can hide real important variables just as easily as they can produce false ones.\nMultiple causation. A phenomenon may arise from multiple causes. Measurement of each cause is useful, so when we can use the same data to estimate more than one type of influence, we should. Furthermore, when causation is multiple, one cause can hide another.\nInteractions. The importance of one variable may depend upon another. For ex- ample, plants benefit from both light and water. But in the absence of either, the other is no benefit at all. Such interactions occur very often. Effective inference about one variable will often depend upon consideration of others.\n\nClaude give a quick description of the waffle divorce data set : Data for the individual States of the United States, describing number of Waffle House diners and various marriage and demographic facts.\nFormat Location : State name Loc : State abbreviation Population : 2010 population in millions MedianAgeMarriage: 2005-2010 median age at marriage Marriage : 2009 marriage rate per 1000 adults Marriage.SE : Standard error of rate Divorce : 2009 divorce rate per 1000 adults Divorce.SE : Standard error of rate WaffleHouses : Number of diners South : 1 indicates Southern State Slaves1860 : Number of slaves in 1860 census Population1860 : Population from 1860 census PropSlaves1860 : Proportion of total population that were slaves in 1860\n\n\n\n\n\n\n\n\n\nMost likely a spurious correlation, waffles don’t cause divorces nor vice versa"
  },
  {
    "objectID": "ch5_multilinear.html#spurious-correlation",
    "href": "ch5_multilinear.html#spurious-correlation",
    "title": "Statistical Rethinking Portfolio",
    "section": "Spurious Correlation",
    "text": "Spurious Correlation\nExcerpt from Richard McElreath: “Let’s leave waffles behind, at least for the moment. An example that is easier to understand is the correlation between divorce rate and marriage rate (Figure 5.2). The rate at which adults marry is a great predictor of divorce rate, as seen in the left-hand plot in the figure. But does marriage cause divorce? In a trivial sense it obviously does: One cannot get a divorce without first getting married. But there’s no reason high marriage rate must be correlated with divorce.”\n\n\n\n\n\n\n\n\n\nWe see that Divorce rate is high in the south and low in the north midwest and mid atlantic. Marriage rate is high in\n\n\n\n\n\n\n\n\n\n\nMedian Marriage Age Model\n\\[ \\textbf{b5.1} \\]\n\\[ \\text{Divorce}_i \\sim \\text{Intercept} + \\beta_1 \\text{Median Age Marriage}_i \\]\n\n📈 μ vs full distribution🎛 Parameters⚙️ Code🌌 Prior Vs Posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb5.1 &lt;- \n  brm(data = WaffleDivorce, \n      family = gaussian,\n      Divorce ~ 1 + MedianAgeMarriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma, ub = 10)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/b05.01\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe regression of Divorce Rate on Age of Marriage, tells us only that the total influence of age at marriage is strongly negative with divorce rate.\n\n\nMarriage Rate\n\\[ \\textbf{b5.2} \\]\n\\[ \\text{Divorce}_i \\sim \\text{Intercept} + \\beta_1 \\text{Marriage % Rate}_i \\]\n\n📈 μ vs full distribution🎛 Parameters⚙️ Code🌌 Prior Vs Posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb5.2  &lt;- \n  brm(data = WaffleDivorce, \n      family = gaussian,\n      Divorce ~ 1 + Marriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma, ub = 10)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/b05.02\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCombined Multi-linear model\n\\[ \\textbf{b5.3} \\]\n\\[ \\text{Divorce}_i \\sim \\text{Intercept} + \\beta_1 \\text{Marriage % Rate}_i + \\beta_2 \\text{Median Marriage Age}_i \\]\n\n🎛️ Parameters⚙️ Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb5.3 &lt;- brm(data = WaffleDivorce, \n      family = gaussian,\n      Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma, ub = 10)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/b05.031\")\n\n\n\n\n\n\nDiagnosing Multi-linear models\n\nPredictor Residual PlotPosterior Prediction PlotsCausal Counterfactual Plots\n\n\n\nMarriage RateMedian Marriage Age\n\n\n\n\n\n\n\n\n\n\n\nLittle relationship between divorce and marriage rates, once we have accounted for the effect of Median Marriage Age\n\n\n\n\n\n\n\n\n\n\n\nNegative relationship between divorce and marriage age, even after we have accounted for the effect of the Marriage % Rate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“This procedure also brings home the message that regression models measure the remaining association of each predictor with the out- come, after already knowing the other predictors. In computing the predictor residual plots, you had to perform those calculations yourself. In the unified multivariate model, it all hap- pens automatically. Nevertheless, it is useful to keep this fact in mind, because regressions can behave in surprising ways as a result.”\n\n\nSolving the DAG\nFrom McElreath, “Age of marriage influences divorce in two ways. First it can have a direct effect, perhaps because younger people change faster than older people and are therefore more likely to grow incompatible with a partner. Second, it can have an indirect effect by influencing the marriage rate. If people get married earlier, then the marriage rate may rise, because there are more young people. Consider for example if an evil dictator forced everyone to marry at age 65. Since a smaller fraction of the population lives to 65 than to 25, forcing delayed marriage will also reduce the marriage rate. If marriage rate itself has any direct effect on divorce, maybe by making marriage more or less normative, then some of that direct effect could be the indirect effect of age at marriage.”\n\n\n\n\n\n\n\n\n\nOnce we know median age at marriage for a State, there is little or no additional predictive power in also knowing the rate of marriage in that State.\n\nUS Map ResidualStates by ResidualWaffle House Effect"
  },
  {
    "objectID": "ch5_multilinear.html#masked-relationships",
    "href": "ch5_multilinear.html#masked-relationships",
    "title": "Statistical Rethinking Portfolio",
    "section": "Masked Relationships",
    "text": "Masked Relationships\nNew data set Comparative primate milk composition data, from Table 2 of Hinde and Milligan. 2011. Evolutionary Anthropology 20:9-23.\nspecies: Species name\nkcal.per.g: Kilocalories per gram of milk\nmass: Body mass of mother, in kilograms\nneocortex.perc: Percent of brain mass that is neocortex\n\nNeocortex % Effect on Milk Calories\n\n📈 μ vs full distribution🎛️ Parameters⚙️ Code🌌 Prior Vs Posterior\n\n\n\n\n\n\n\n\n\n\n\nThe posterior mean line is weakly positive, but it is highly imprecise. A lot of mildly positive and negative slopes are plausible, given this model and these data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmilk.01 &lt;- \n  brm(data = milk2, \n      family = gaussian,\n      kcal.per.g_s ~ 1 + neocortex.perc_s,\n      prior = c(prior(normal(0, .2), class = Intercept),\n                prior(normal(0, .5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/milk.01.6\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMass Effect on Milk Calories\nNow consider another predictor variable, adult female body mass, mass in the data frame. Let’s use the logarithm of mass, log(mass), as a predictor as well. Why the logarithm of mass instead of the raw mass in kilograms? It is often true that scaling measurements like body mass are related by magnitudes to other variables. Taking the log of a measure trans- lates the measure into magnitudes. So by using the logarithm of body mass here, we’re saying that we suspect that the magnitude of a mother’s body mass is related to milk energy, in a linear fashion.\n\n📈 μ vs full distribution🎛️ Parameters⚙️ Code\n\n\n\n\n\n\n\n\n\n\n\nThe posterior mean line is moderately negative, but it is highly imprecise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmilk.02 &lt;- \n  brm(data = milk2, \n      family = gaussian,\n      kcal.per.g_s ~ 1 + logMass_s,\n      prior = c(prior(normal(0, .2), class = Intercept),\n                prior(normal(0, .5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/milk.2.0\")\n\n\n\n\n\n\nMultivariate Milk"
  },
  {
    "objectID": "ch5_multilinear.html#categorical-variable",
    "href": "ch5_multilinear.html#categorical-variable",
    "title": "Statistical Rethinking Portfolio",
    "section": "Categorical Variable",
    "text": "Categorical Variable\n\nBinary Categories\nTwo mutually exclusive categories make the interpretation of models quite simple because it’s hard to forget that the lack of Category A implies Category B. An example, lets use male vs females as our categorical variable, with Males being represented as 1 and females as 0 in our data. The models interpretations is that the intercept is the effect of being female and the intercept + \\(\\beta_\\text{effect of male}\\) is the males effect. The same math goes into the interpretations when our categorical variables have more than 2 options, but it’s quite easy to forget that your comparing to one of the categories when you have a list of 20 categories. Always remember that you are comparing to some other category with factor effects.\nLet’s go back to our Kung! height dataset we used in the last chapter. Let’s ignore the effect of weight and the other variables and focus only on sex. Our model is\n\\[ \\text{Height}_i \\sim \\text{Normal(}\\mu_i, \\ \\sigma_i\\text{)}\\] \\[ \\mu_i = \\alpha + \\beta_\\text{male} \\mathbf{I}\\text{(male = 1)}\\]\n\\[\\alpha \\sim \\text{Normal(178, 20)}\\]\n\\[ \\beta_\\text{male} \\sim \\text{Normal(0, 10)}\\]\n\\[ \\sigma \\sim \\text{Uniform(0, 50)}\\]\nThe parameter \\(\\beta_\\text{male}\\) only influences prediction for the cases where the individual is a male. Using this approach means that \\(\\beta_\\text{male}\\) represents the expected difference between males and females in height. This makes assigning our priors a bit harder. Our approach assumes there is more uncertainty about males than females, because a male includes two parameters and therefore has two parameters.\n\n\n\n\n\n\n\n\n\nOur priors are a bit wide, and the extra variance on males doesn’t make to much sense.\nA better way is to use an index variable.\n\\[ \\text{Height}_i \\sim \\text{Normal(}\\mu_i, \\ \\sigma_i\\text{)}\\] \\[ \\mu_i = \\alpha_\\text{Sex[i]}\\]\n\\[\\alpha_j \\sim \\text{Normal(178, 20),  for j  = 1..2}\\]\n\\[ \\sigma \\sim \\text{Uniform(0, 50)}\\]\n\n\n\n\n\n\n\n\n\nWe actually care about the differences in the genders in height in this case. So for this model we simply subtract the posterior intercepts from each other to show that difference.\n\n\n\n\n\n\n\n\n\nBinary categories are easy, whether you use an indicator variable or instead an index variable. But when there are more than two categories, the indicator variable approach explodes. While the index approach does not change at all when you add more categories. You do get more parameters, of course, just as many as in the indicator variable approach. But the model specification looks just like it does in the binary case. And the priors continue to be easier, unless you really do have prior information about contrasts.\n\n\nMilk example"
  }
]