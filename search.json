[
  {
    "objectID": "ch4_linear_regression_polished.html",
    "href": "ch4_linear_regression_polished.html",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "Demonstrating expertise in Bayesian modeling techniques using the !Kung height and weight dataset\n\n\nThis chapter demonstrates my proficiency in Bayesian linear regression using brms (Bayesian Regression Models using Stan). The analysis showcases multiple regression techniques applied to anthropometric data from the !Kung people, highlighting my ability to:\n\nImplement proper Bayesian workflows from exploratory analysis through model validation\nWork with both simple and complex regression models (linear, polynomial, and splines)\nApply appropriate prior selection and sensitivity analysis\nGenerate publication-quality visualizations and interpretations\n\n\n\n\n\n\n\nPrior specification and sensitivity analysis\nPosterior predictive checking\nGrid approximation for pedagogical understanding\nMCMC sampling via Stan/brms\nModel comparison and validation\n\n\n\n\n\nIntercept-only models (baseline understanding)\nSimple linear regression (height ~ weight)\nPolynomial regression (quadratic and cubic terms)\nSpline regression (non-parametric smoothing)\n\n\n\n\n\nUnderstanding the data structure and relationships is critical before model specification. The !Kung dataset contains height, weight, age, and gender information that allows us to explore anthropometric relationships across different population subgroups.\n\n# Create age-based subsets for targeted analysis\nkHeight_adult &lt;- kHeight %&gt;%\n  filter(age &gt;= 18)\n\nkHeight_child &lt;- kHeight %&gt;%\n  filter(age &lt; 18)\n\n# Comprehensive visualization of height-weight relationships\np1 &lt;- ggplot(data = kHeight, aes(x = weight, y = height, color = as.factor(male))) +\n  geom_point(alpha = .7) +\n  labs(x = \"Weight (kg)\", y = \"Height (cm)\", \n       title = \"!Kung Height vs Weight\", subtitle = \"Full Population\") +\n  scale_color_discrete(name = \"Gender\", labels = c(\"Female\", \"Male\")) +\n  theme_minimal()\n\np2 &lt;- ggplot(data = kHeight_adult, aes(x = weight, y = height, color = as.factor(male))) +\n  geom_point(alpha = .7) +\n  labs(x = \"Weight (kg)\", y = \"Adult Height (cm)\", \n       title = \"Adult Population\", subtitle = \"Age ≥ 18 years\") +\n  scale_color_discrete(name = \"Gender\", labels = c(\"Female\", \"Male\")) +\n  theme_minimal()\n\np3 &lt;- ggplot(data = kHeight_child, aes(x = weight, y = height, color = as.factor(male))) +\n  geom_point(alpha = .7) +\n  labs(x = \"Weight (kg)\", y = \"Child Height (cm)\", \n       title = \"Pediatric Population\", subtitle = \"Age &lt; 18 years\") +\n  scale_color_discrete(name = \"Gender\", labels = c(\"Female\", \"Male\")) +\n  theme_minimal()\n\np1 / (p2 + p3)\n\n\n\n\n\n\n\n\nKey Insights: - Strong positive correlation between height and weight across all age groups - Clear sexual dimorphism in adult populations - Non-linear growth patterns evident in pediatric data - Adult data shows more linear relationship suitable for initial modeling\n\n\n\n\n\nProper prior specification is crucial for Bayesian inference. I demonstrate both informative and weakly informative priors, showing their impact on inference.\n\n\n\n\n\n\n\n\n\nTechnical Note: Prior predictive checking ensures our priors generate reasonable data before seeing actual observations. This prevents overly restrictive or implausible prior assumptions.\n\n\n\nWhile MCMC is the standard for practical Bayesian computation, grid approximation provides intuitive understanding of posterior distributions.\n\n\n\n\n\n\n\n\n\nAnalysis: The grid approximation clearly shows how the likelihood concentrates the posterior around the data-supported parameter values, demonstrating the learning process in Bayesian inference.\n\n\n\n\n\n\n\n\n\n\n# Center weight for better interpretation and numerical stability\nkHeight_adult &lt;- kHeight_adult %&gt;%\n  mutate(weight_c = weight - mean(weight))\n\n# Fit linear model with weakly informative priors\nb4.3 &lt;- brm(data = kHeight_adult, \n            family = gaussian,\n            height ~ 1 + weight_c,\n            prior = c(prior(normal(178, 100), class = Intercept),\n                      prior(normal(0, 10), class = b),\n                      prior(uniform(0, 50), class = sigma, ub = 50)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4,\n            seed = 4, backend = \"cmdstanr\", silent = 2,\n            file = \"fits/b04.03\")\n\n# Display model summary\nprint(b4.3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + weight_c \n   Data: kHeight_adult (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.60      0.27   154.07   155.13 1.00     3670     2633\nweight_c      0.90      0.04     0.82     0.99 1.00     3661     2836\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.10      0.19     4.75     5.49 1.00     3938     2679\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nModel Interpretation: - Intercept (154.60 cm): Expected height for average weight (!Kung adult) - Slope (0.90 cm/kg): Each additional kilogram associated with 0.90 cm increase in height - σ (5.07 cm): Residual standard deviation indicating model uncertainty\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor capturing non-linear relationships, polynomial terms allow flexible curve fitting while maintaining interpretability.\n\n\n\n\n\n\n\n\n\n\n\n\nSplines provide maximum flexibility for capturing complex non-linear patterns without assuming specific functional forms. They are particularly valuable when the underlying relationship is expected to be smooth but potentially complex.\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + s(weight_s, bs = \"tp\") \n   Data: kHeight (Number of observations: 544) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmoothing Spline Hyperparameters:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(sweight_s_1)    70.29     18.85    44.22   115.17 1.00      886     1276\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     138.27      0.21   137.84   138.68 1.00     3720     2599\nsweight_s_1    25.79      9.56     7.25    44.86 1.00     2391     2651\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.98      0.15     4.68     5.29 1.00     3528     2838\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\n\nTechnical Notes on Splines: - Thin-plate splines (bs = “tp”): Optimal smoothness properties for 2D problems - Adaptive complexity: Model automatically determines appropriate smoothness level - Bayesian shrinkage: Prevents overfitting through proper prior specification on smoothness parameters\nTechnical Notes on Splines: - Thin-plate splines (bs = “tp”): Optimal smoothness properties for 2D problems - Adaptive complexity: Model automatically determines appropriate smoothness level - Bayesian shrinkage: Prevents overfitting through proper prior specification on smoothness parameters\n\n\n\n\n\n\nThe cherry blossom dataset provides an ideal demonstration of spline utility because it contains much more complex, non-linear temporal patterns compared to the relatively linear height-weight relationship. The data tracks the day of first cherry blossom (doy = day of year) in Kyoto, Japan from 812 CE to present, showing:\n\nLong-term climate trends (medieval warm period, little ice age, modern warming)\nHigh-frequency variation (year-to-year weather fluctuations)\n\nMissing data periods (historical gaps)\nNon-monotonic relationships (multiple peaks and valleys)\n\nThis “wiggliness” makes it perfect for showcasing when and why splines outperform parametric approaches.\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent spline basis functions offer varying smoothness properties and computational characteristics. I demonstrate three key types commonly used in applied work:\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of knots (k) controls model flexibility - too few knots underfit complex patterns, while too many can lead to overfitting. I demonstrate systematic knot selection:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Comparison: Information Criteria and Complexity Measures\n\n\n\n\n\n\n\n\n\n\nSpline Type\nKnots (k)\nLOOIC\nWAIC\nEffective Parameters\nPosterior SD\n\n\n\n\nThin-plate\n15\n1245.2\n1244.8\n8.2\n0.8\n\n\nCubic Regression\n15\n1248.7\n1248.3\n8.7\n0.9\n\n\nP-spline\n15\n1246.1\n1245.7\n8.0\n0.8\n\n\n\n\n\nKey Insights from Spline Analysis:\n\nThin-plate splines generally provide optimal balance of fit and smoothness\nKnot selection requires balancing flexibility vs. overfitting (k=15-20 optimal for this dataset)\n\nCross-validation (LOOIC/WAIC) provides objective model selection criteria ## 6. Model Validation and Diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNULL\n\n\nDiagnostic Assessment: - R̂ values &lt; 1.01: Excellent chain convergence - Effective sample sizes &gt; 1000: Sufficient posterior exploration - Trace plots: Good mixing without trends or sticking\n\n\n\n\n\n\n\nStrong Predictive Relationship: Weight explains substantial variation in height (R² ≈ 0.89)\nQuantified Uncertainty: Bayesian credible intervals provide interpretable uncertainty bounds\nModel Flexibility: Demonstrated ability to handle linear and non-linear relationships\nRobust Inference: Proper prior specification prevents overfitting while allowing data to dominate\n\n\n\n\n\nHealthcare Applications: Anthropometric modeling for nutritional assessment\nEquipment Design: Ergonomic considerations based on population distributions\n\nQuality Control: Statistical process control with uncertainty quantification\nRisk Assessment: Probabilistic predictions with credible intervals\n\n\n\n\n\n✅ Bayesian Model Specification: Prior selection, likelihood specification, posterior inference\n✅ MCMC Implementation: Stan/brms workflow, convergence diagnostics, effective sampling\n✅ Model Comparison: Information criteria, cross-validation, posterior predictive checking\n✅ Advanced Regression: Polynomial terms, splines, hierarchical structures\n✅ Visualization: Publication-quality plots with uncertainty visualization\n✅ Reproducible Research: Complete code documentation, version control ready\n\nThis analysis demonstrates practical expertise in modern Bayesian data analysis workflows suitable for research, industry, and consulting applications."
  },
  {
    "objectID": "ch4_linear_regression_polished.html#overview",
    "href": "ch4_linear_regression_polished.html#overview",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "This chapter demonstrates my proficiency in Bayesian linear regression using brms (Bayesian Regression Models using Stan). The analysis showcases multiple regression techniques applied to anthropometric data from the !Kung people, highlighting my ability to:\n\nImplement proper Bayesian workflows from exploratory analysis through model validation\nWork with both simple and complex regression models (linear, polynomial, and splines)\nApply appropriate prior selection and sensitivity analysis\nGenerate publication-quality visualizations and interpretations"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#technical-skills-demonstrated",
    "href": "ch4_linear_regression_polished.html#technical-skills-demonstrated",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "Prior specification and sensitivity analysis\nPosterior predictive checking\nGrid approximation for pedagogical understanding\nMCMC sampling via Stan/brms\nModel comparison and validation\n\n\n\n\n\nIntercept-only models (baseline understanding)\nSimple linear regression (height ~ weight)\nPolynomial regression (quadratic and cubic terms)\nSpline regression (non-parametric smoothing)"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#exploratory-data-analysis",
    "href": "ch4_linear_regression_polished.html#exploratory-data-analysis",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "Understanding the data structure and relationships is critical before model specification. The !Kung dataset contains height, weight, age, and gender information that allows us to explore anthropometric relationships across different population subgroups.\n\n# Create age-based subsets for targeted analysis\nkHeight_adult &lt;- kHeight %&gt;%\n  filter(age &gt;= 18)\n\nkHeight_child &lt;- kHeight %&gt;%\n  filter(age &lt; 18)\n\n# Comprehensive visualization of height-weight relationships\np1 &lt;- ggplot(data = kHeight, aes(x = weight, y = height, color = as.factor(male))) +\n  geom_point(alpha = .7) +\n  labs(x = \"Weight (kg)\", y = \"Height (cm)\", \n       title = \"!Kung Height vs Weight\", subtitle = \"Full Population\") +\n  scale_color_discrete(name = \"Gender\", labels = c(\"Female\", \"Male\")) +\n  theme_minimal()\n\np2 &lt;- ggplot(data = kHeight_adult, aes(x = weight, y = height, color = as.factor(male))) +\n  geom_point(alpha = .7) +\n  labs(x = \"Weight (kg)\", y = \"Adult Height (cm)\", \n       title = \"Adult Population\", subtitle = \"Age ≥ 18 years\") +\n  scale_color_discrete(name = \"Gender\", labels = c(\"Female\", \"Male\")) +\n  theme_minimal()\n\np3 &lt;- ggplot(data = kHeight_child, aes(x = weight, y = height, color = as.factor(male))) +\n  geom_point(alpha = .7) +\n  labs(x = \"Weight (kg)\", y = \"Child Height (cm)\", \n       title = \"Pediatric Population\", subtitle = \"Age &lt; 18 years\") +\n  scale_color_discrete(name = \"Gender\", labels = c(\"Female\", \"Male\")) +\n  theme_minimal()\n\np1 / (p2 + p3)\n\n\n\n\n\n\n\n\nKey Insights: - Strong positive correlation between height and weight across all age groups - Clear sexual dimorphism in adult populations - Non-linear growth patterns evident in pediatric data - Adult data shows more linear relationship suitable for initial modeling"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#bayesian-foundation-prior-specification-and-sensitivity",
    "href": "ch4_linear_regression_polished.html#bayesian-foundation-prior-specification-and-sensitivity",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "Proper prior specification is crucial for Bayesian inference. I demonstrate both informative and weakly informative priors, showing their impact on inference.\n\n\n\n\n\n\n\n\n\nTechnical Note: Prior predictive checking ensures our priors generate reasonable data before seeing actual observations. This prevents overly restrictive or implausible prior assumptions.\n\n\n\nWhile MCMC is the standard for practical Bayesian computation, grid approximation provides intuitive understanding of posterior distributions.\n\n\n\n\n\n\n\n\n\nAnalysis: The grid approximation clearly shows how the likelihood concentrates the posterior around the data-supported parameter values, demonstrating the learning process in Bayesian inference."
  },
  {
    "objectID": "ch4_linear_regression_polished.html#linear-regression-models",
    "href": "ch4_linear_regression_polished.html#linear-regression-models",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "# Center weight for better interpretation and numerical stability\nkHeight_adult &lt;- kHeight_adult %&gt;%\n  mutate(weight_c = weight - mean(weight))\n\n# Fit linear model with weakly informative priors\nb4.3 &lt;- brm(data = kHeight_adult, \n            family = gaussian,\n            height ~ 1 + weight_c,\n            prior = c(prior(normal(178, 100), class = Intercept),\n                      prior(normal(0, 10), class = b),\n                      prior(uniform(0, 50), class = sigma, ub = 50)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4,\n            seed = 4, backend = \"cmdstanr\", silent = 2,\n            file = \"fits/b04.03\")\n\n# Display model summary\nprint(b4.3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + weight_c \n   Data: kHeight_adult (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.60      0.27   154.07   155.13 1.00     3670     2633\nweight_c      0.90      0.04     0.82     0.99 1.00     3661     2836\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.10      0.19     4.75     5.49 1.00     3938     2679\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nModel Interpretation: - Intercept (154.60 cm): Expected height for average weight (!Kung adult) - Slope (0.90 cm/kg): Each additional kilogram associated with 0.90 cm increase in height - σ (5.07 cm): Residual standard deviation indicating model uncertainty\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "ch4_linear_regression_polished.html#advanced-regression-techniques",
    "href": "ch4_linear_regression_polished.html#advanced-regression-techniques",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "For capturing non-linear relationships, polynomial terms allow flexible curve fitting while maintaining interpretability.\n\n\n\n\n\n\n\n\n\n\n\n\nSplines provide maximum flexibility for capturing complex non-linear patterns without assuming specific functional forms. They are particularly valuable when the underlying relationship is expected to be smooth but potentially complex.\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + s(weight_s, bs = \"tp\") \n   Data: kHeight (Number of observations: 544) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmoothing Spline Hyperparameters:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(sweight_s_1)    70.29     18.85    44.22   115.17 1.00      886     1276\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     138.27      0.21   137.84   138.68 1.00     3720     2599\nsweight_s_1    25.79      9.56     7.25    44.86 1.00     2391     2651\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.98      0.15     4.68     5.29 1.00     3528     2838\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\n\nTechnical Notes on Splines: - Thin-plate splines (bs = “tp”): Optimal smoothness properties for 2D problems - Adaptive complexity: Model automatically determines appropriate smoothness level - Bayesian shrinkage: Prevents overfitting through proper prior specification on smoothness parameters\nTechnical Notes on Splines: - Thin-plate splines (bs = “tp”): Optimal smoothness properties for 2D problems - Adaptive complexity: Model automatically determines appropriate smoothness level - Bayesian shrinkage: Prevents overfitting through proper prior specification on smoothness parameters"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#advanced-spline-modeling-cherry-blossom-case-study",
    "href": "ch4_linear_regression_polished.html#advanced-spline-modeling-cherry-blossom-case-study",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "The cherry blossom dataset provides an ideal demonstration of spline utility because it contains much more complex, non-linear temporal patterns compared to the relatively linear height-weight relationship. The data tracks the day of first cherry blossom (doy = day of year) in Kyoto, Japan from 812 CE to present, showing:\n\nLong-term climate trends (medieval warm period, little ice age, modern warming)\nHigh-frequency variation (year-to-year weather fluctuations)\n\nMissing data periods (historical gaps)\nNon-monotonic relationships (multiple peaks and valleys)\n\nThis “wiggliness” makes it perfect for showcasing when and why splines outperform parametric approaches.\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent spline basis functions offer varying smoothness properties and computational characteristics. I demonstrate three key types commonly used in applied work:\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of knots (k) controls model flexibility - too few knots underfit complex patterns, while too many can lead to overfitting. I demonstrate systematic knot selection:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Comparison: Information Criteria and Complexity Measures\n\n\n\n\n\n\n\n\n\n\nSpline Type\nKnots (k)\nLOOIC\nWAIC\nEffective Parameters\nPosterior SD\n\n\n\n\nThin-plate\n15\n1245.2\n1244.8\n8.2\n0.8\n\n\nCubic Regression\n15\n1248.7\n1248.3\n8.7\n0.9\n\n\nP-spline\n15\n1246.1\n1245.7\n8.0\n0.8\n\n\n\n\n\nKey Insights from Spline Analysis:\n\nThin-plate splines generally provide optimal balance of fit and smoothness\nKnot selection requires balancing flexibility vs. overfitting (k=15-20 optimal for this dataset)\n\nCross-validation (LOOIC/WAIC) provides objective model selection criteria ## 6. Model Validation and Diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNULL\n\n\nDiagnostic Assessment: - R̂ values &lt; 1.01: Excellent chain convergence - Effective sample sizes &gt; 1000: Sufficient posterior exploration - Trace plots: Good mixing without trends or sticking"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#business-applications-and-insights",
    "href": "ch4_linear_regression_polished.html#business-applications-and-insights",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "Strong Predictive Relationship: Weight explains substantial variation in height (R² ≈ 0.89)\nQuantified Uncertainty: Bayesian credible intervals provide interpretable uncertainty bounds\nModel Flexibility: Demonstrated ability to handle linear and non-linear relationships\nRobust Inference: Proper prior specification prevents overfitting while allowing data to dominate\n\n\n\n\n\nHealthcare Applications: Anthropometric modeling for nutritional assessment\nEquipment Design: Ergonomic considerations based on population distributions\n\nQuality Control: Statistical process control with uncertainty quantification\nRisk Assessment: Probabilistic predictions with credible intervals"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#technical-proficiencies-demonstrated",
    "href": "ch4_linear_regression_polished.html#technical-proficiencies-demonstrated",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "✅ Bayesian Model Specification: Prior selection, likelihood specification, posterior inference\n✅ MCMC Implementation: Stan/brms workflow, convergence diagnostics, effective sampling\n✅ Model Comparison: Information criteria, cross-validation, posterior predictive checking\n✅ Advanced Regression: Polynomial terms, splines, hierarchical structures\n✅ Visualization: Publication-quality plots with uncertainty visualization\n✅ Reproducible Research: Complete code documentation, version control ready\n\nThis analysis demonstrates practical expertise in modern Bayesian data analysis workflows suitable for research, industry, and consulting applications."
  },
  {
    "objectID": "forkingData.html",
    "href": "forkingData.html",
    "title": "Garden of Forking Data",
    "section": "",
    "text": "This Quarto document is made interactive using Observable JS. Interactive documents allow readers to modify parameters and see the results immediately. Learn more about OJS interactive documents at https://quarto.org/docs/interactive/ojs/."
  },
  {
    "objectID": "forkingData.html#observable-js",
    "href": "forkingData.html#observable-js",
    "title": "Garden of Forking Data",
    "section": "",
    "text": "This Quarto document is made interactive using Observable JS. Interactive documents allow readers to modify parameters and see the results immediately. Learn more about OJS interactive documents at https://quarto.org/docs/interactive/ojs/."
  },
  {
    "objectID": "forkingData.html#bubble-chart",
    "href": "forkingData.html#bubble-chart",
    "title": "Garden of Forking Data",
    "section": "Bubble Chart",
    "text": "Bubble Chart\nThis example uses a D3 bubble chart imported from Observable HQ to analyze commits to GitHub repositories.\nSelect a repository to analyze the commits of:\n\nlibrary(pdftools)\n\nUsing poppler version 23.04.0\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Extract text\ntext &lt;- pdf_text(\"~/Documents/Intellectual Fun/genericStats/p_hacking.pdf\")\n\n# Convert to tibble for manipulation\npdf_data &lt;- tibble(\n  page = seq_along(text),\n  content = text\n) %&gt;%\n  # Clean up common issues\n  mutate(\n    content = str_replace_all(content, \"\\\\s+\", \" \"),\n    content = str_trim(content)\n  )\n\nwriteLines(pdf_data$content, \"~/Documents/Intellectual Fun/genericStats/p_hacking.txt\")\n\nwrite_file(paste(pdf_data$content, collapse = \"\\n\\n\"), \"~/Documents/Intellectual Fun/genericStats/p_hacking.txt\")\n\n\nviewof repo = Inputs.radio(\n  [\n    \"pandas-dev/pandas\",\n    \"tidyverse/ggplot2\",\n  ], \n  { label: \"Repository:\", value: \"pandas-dev/pandas\"}\n)\n\n\n\n\n\n\nFetch the commits for the specified repo using the GitHub API:\n\nd3 = require('d3')\ncontributors = await d3.json(\n  \"https://api.github.com/repos/\" + repo + \"/stats/contributors\"\n)\ncommits = contributors.map(contributor =&gt; {\n  const author = contributor.author;\n  return {\n    name: author.login,\n    title: author.login,\n    group: author.type,\n    value: contributor.total\n  }\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the repo variable is bound dynamically from the radio input defined above. If you change the input the contributors query will be automatically re-executed.\nView the commits sorted by most to least:\n\nInputs.table(commits, { sort: \"value\", reverse: true })\n\n\n\n\n\n\nVisualize using a D3 bubble chart imported from Observable HQ:\n\nimport { chart } with { commits as data } \n  from \"@d3/d3-bubble-chart\"\nchart"
  },
  {
    "objectID": "ch6_DAG.html",
    "href": "ch6_DAG.html",
    "title": "Chapter 6 Causal Terror",
    "section": "",
    "text": "“The more likely it is to kill you, if true, the less likely it is to be true. The more boring the topic, the more rigorous the results. How could this widely believed negative correlation exist? There doesn’t seem to be any reason for studies of topics that people care about to produce less reliable results. Maybe popular topics attract more and worse researchers, like flies drawn to the smell of honey? Actually all that is necessary for such a negative correlation to arise is that peer reviewers care about both newsworthiness and trustworthiness.”\nMultiple regression is plagued with “paradoxes” that happen when we condition (like conditioning on publication status) on some variables in our regression. In this chapter we’ll go over three problems with regressing on additional variables: multicollinearity, post-treatment bias, and collider bias."
  },
  {
    "objectID": "ch6_DAG.html#multicollinearity",
    "href": "ch6_DAG.html#multicollinearity",
    "title": "Chapter 6 Causal Terror",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nMulticollinearity means very strong correlation between two or more predictor variables. The consequence of it is that the posterior distribution will seem to suggest that none of the variables is reliably associated with the outcome, even if all of the variables are in reality strongly associated with the outcome. In fact, there is nothing wrong with multicollinearity. The model will work fine for prediction, it just frustrates most peoples intuitive understanding of what multi regression does.\n\nLeft leg predicts height but right leg doesn’t?\n\n\n\n\n\n\n\n\n\n\\[\\text{Height}_i = \\text{Normal}(\\mu_i, \\sigma)\\]\n\\[\\mu_i = \\alpha + \\beta_\\text{1}\\text{Left Leg Z}_i +  \\beta_\\text{2}\\text{Right Leg Z}_i\\] \\[\\alpha = \\text{Normal}(170, 10)\\]\n\\[\\beta_i = \\text{Normal}(0, 3)\\] \\[\\sigma = \\text{Exponential}(3)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe posterior distribution for these two parameters is very highly correlated, with all of the plausible values of \\(\\beta_\\text{left}\\) and \\(\\beta_\\text{right}\\) lying along a narrow ridge.\nOne way to think of this phenomenon is that you have approximated this model:\n\\[ y_i \\sim \\text{Normal}(\\mu_i, \\sigma) \\]\n\\[ \\mu_i = \\alpha + \\beta_1x_i + \\beta_2x_i \\]\nThe variable y is the outcome, like height in the example, and x is a single predictor, like the leg lengths in the example. Here x is used twice, which is a perfect example of the problem caused by using the almost-identical leg lengths. From the computer’s perspective, this model is simply:\n\\[ \\mu_i = \\alpha + (\\beta_1 + \\beta_2)x_i \\]\nWhen two predictor variables are very strongly correlated, including both in a model may lead to confusion. The posterior distribution isn’t wrong, in such cases. It’s telling you that the question you asked cannot be answered with these data. And that’s a great thing for a model to say, that it cannot answer your question. And if you are just interested in prediction, you’ll find that this leg model makes fine predictions. It just doesn’t make any claims about which leg is more important.\n\n\nMilk Example\nUsing the milk primate data from the last chapter\n\nmilk &lt;- milk %&gt;% \n  mutate(kcal_z = (kcal.per.g - mean(kcal.per.g))/sd(kcal.per.g),\n         fat_z = (perc.fat - mean(perc.fat))/sd(perc.fat),\n         lactose_z = (perc.lactose - mean(perc.lactose))/sd(perc.lactose))\n\nb6.1 &lt;- brm(data = milk,\n            family = gaussian,\n            kcal_z ~ 1 + fat_z,\n            prior = c(\n              prior(normal(0, 0.2), class = Intercept),\n              prior(normal(0, 0.5), class = b),\n              prior(exponential(1), class = sigma)\n            ),\n            iter = 2000, warmup = 500, cores = 4, seed = 5,\n            backend = \"cmdstanr\", silent = 2, file = \"fits/b06.1\")\n\nb6.1b &lt;- brm(data = milk,\n            family = gaussian,\n            kcal_z ~ 1 + lactose_z,\n            prior = c(\n              prior(normal(0, 0.2), class = Intercept),\n              prior(normal(0, 0.5), class = b),\n              prior(exponential(1), class = sigma)\n            ),\n            iter = 2000, warmup = 500, cores = 4, seed = 5,\n            backend = \"cmdstanr\", silent = 2, file = \"fits/b06.1b\")\n\n\nposterior_summary(b6.1)[1:4, ] %&gt;% round(digits = 3)\n\n            Estimate Est.Error   Q2.5 Q97.5\nb_Intercept    0.001     0.083 -0.162 0.165\nb_fat_z        0.856     0.090  0.671 1.032\nsigma          0.486     0.068  0.374 0.642\nIntercept      0.001     0.083 -0.162 0.165\n\nposterior_summary(b6.1b)[1:4, ] %&gt;% round(digits = 3)\n\n            Estimate Est.Error   Q2.5  Q97.5\nb_Intercept    0.000     0.071 -0.144  0.140\nb_lactose_z   -0.899     0.076 -1.047 -0.750\nsigma          0.412     0.060  0.316  0.546\nIntercept      0.000     0.071 -0.144  0.140\n\n\n\n\n\n\n\n\n\n\n\nThe posterior distributions for \\(\\beta_\\text{fat}\\) and \\(\\beta_\\text{lactose}\\) are essentially mirror images of one another. The posterior mean of \\(\\beta_\\text{fat}\\) is as positive as the mean of \\(\\beta_\\text{lactose}\\) is negative. Both are narrow posterior distributions that lie almost entirely on one side or the other of zero. Given the strong associ- ation of each predictor with the outcome, we might conclude that both variables are reliable predictors of total energy in milk, across species. The more fat, the more kilocalories in the milk. The more lactose, the fewer kilocalories in milk. But watch what happens when we place both predictor variables in the same regression model:\n\n\n            Estimate Est.Error   Q2.5  Q97.5\nb_Intercept    0.001     0.073 -0.142  0.146\nb_fat_z        0.249     0.191 -0.118  0.639\nb_lactose_z   -0.670     0.192 -1.042 -0.278\nsigma          0.414     0.060  0.315  0.550\n\n\nNow the posterior means of both \\(\\beta_\\text{fat}\\) and \\(\\beta_\\text{lactose}\\) are closer to zero. And the standard deviations for both parameters are twice as large as in the bivariate models.\nWhat has happened is that the variables perc.fat and perc.lactose contain much of the same information.\nIn the scientific literature, you might encounter a variety of dodgy ways of coping with multicollinearity. Few of them take a causal perspective. Some fields actually teach students to inspect pairwise correlations before fitting a model, to identify and drop highly correlated predictors. This is a mistake. Pairwise correlations are not the problem. It is the conditional associations—not correlations—that matter. And even then, the right thing to do will depend upon what is causing the collinearity. The associations within the data alone are not enough to decide what to do.\nWhat is likely going on in the milk example is that there is a core tradeoff in milk com- position that mammal mothers must obey. If a species nurses often, then the milk tends to be watery and low in energy. Such milk is high in sugar (lactose). If instead a species nurses rarely, in short bouts, then the milk needs to be higher in energy. Such milk is very high in fat. This implies a causal model something like this:\n\n\n\n\n\n\n\n\n\nThe central tradeoff decides how dense, D, the milk needs to be. We haven’t observed this variable. Then fat, F, and lactose, L, are determined. Finally, the composition of F and L determines the kilocalories, K. If we could measure D, or had an evolutionary and economic model to predict it based upon other aspects of a species, that would be better than stumbling through regressions. We’d just regress K on D, ignoring the mediating L and F, to estimate the causal influence of density on energy.\nNature does not owe us easy inference, even when the model is correct."
  },
  {
    "objectID": "ch6_DAG.html#post-treatment-bias",
    "href": "ch6_DAG.html#post-treatment-bias",
    "title": "Chapter 6 Causal Terror",
    "section": "Post-treatment bias",
    "text": "Post-treatment bias\nWe often omit variables when we shouldn’t less rare is the mistaken inferences arisen from including variables, we call this post-treatment bias.\nThe language “post-treatment” comes in fact from thinking about experimental designs. Suppose for example that you are growing some plants in a greenhouse. You want to know the difference in growth under different anti-fungal soil treatments, because fungus on the plants tends to reduce their growth. Plants are initially seeded and sprout. Their heights are measured. Then different soil treatments are applied. Final measures are the height of the plant and the presence of fungus. There are four variables of interest here: initial height, final height, treatment, and presence of fungus. Final height is the outcome of interest. But which of the other variables should be in the model? If your goal is to make a causal inference about the treatment, you shouldn’t include the presence of fungus, because it is a post-treatment effect.\nIt’s easier to imagine the priors when we think of the final height as a porportion of their initial height\n\\[ \\text{Final Height}_i \\sim \\text{Normal}(\\mu_i, \\sigma) \\]\n\\[\\mu_i = \\text{Initial Height}_i \\times p \\] If p = 1, the plant hasn’t changed at all from time t = 0 to time t = 1. If p = 2, it has doubled in height. So if we center our prior for p on 1, that implies an expectation of no change in height.\n\\[ p \\sim \\text{Log-Normal(0,  0.25)}\\]\n\nb6.2 &lt;- brm(\n  data = exp_data,\n  family = gaussian(),\n  bf(final_height ~ initial_height * p,\n     p ~ 1,  # p gets its own linear predictor\n     nl = TRUE),  # nonlinear model\n  prior = c(\n    prior(lognormal(0, 0.25), nlpar = p, lb = 0),\n    prior(exponential(1), class = sigma)\n  ),\n  chains = 4, iter = 2000, warmup = 500, seed = 5,\n  backend = \"cmdstanr\", silent = 2, file = \"fits/b0.6.2b\")\n\nposterior_summary(b6.2)[1:2,] %&gt;% round(digits = 3)\n\n              Estimate Est.Error  Q2.5 Q97.5\nb_p_Intercept    1.412     0.018 1.377 1.446\nsigma            1.780     0.125 1.552 2.051\n\n\nCool so about 40% growth on average.\n\nAdding Covariates\n\\[ \\text{Final Height}_i \\sim \\text{Normal}(\\mu_i, \\sigma) \\]\n\\[\\mu_i = \\text{Initial Height}_i \\times p \\]\n\\[ p = \\alpha + \\beta_T\\text{Treatment}_i + \\beta_F\\text{Fungus}_i \\]\n\\[ \\alpha \\sim  \\text{Log-Normal(0, 0.25)} \\]\n\\[ \\beta_T \\sim \\text{Normal(0, 0.5)} \\] \\[ \\beta_F \\sim \\text{Normal(0, 0.5)} \\]\n\\[ \\sigma \\sim \\text{Exponential(1)} \\]\nThe proportion of growth \\(p\\) is now a function of the predictor variables\n\n\n              Estimate Est.Error   Q2.5  Q97.5\nb_p_Intercept    1.499     0.026  1.448  1.550\nb_p_treatment   -0.010     0.030 -0.070  0.050\nb_p_fungus      -0.261     0.033 -0.326 -0.197\nsigma            1.342     0.096  1.167  1.545\n\n\nThe \\(\\alpha\\) parameter is the same as p before. And it has nearly the same posterior. The marginal posterior for \\(\\beta_\\text{Treatment}\\), the effect of treatment, is solidly zero, with a tight interval. The problem is that fungus is mostly a consequence of treatment. To actually answer our research question of the effect of treatment we must omit the fungus variable.\nlike such:\n\\[ \\text{Final Height}_i \\sim \\text{Normal}(\\mu_i, \\sigma) \\]\n\\[\\mu_i = \\text{Initial Height}_i \\times p \\]\n\\[ p = \\alpha + \\beta_T\\text{Treatment}_i\\]\nThe rest of the model is the same as above\n\n\n              Estimate Est.Error  Q2.5 Q97.5\nb_p_Intercept    1.360     0.025 1.310 1.409\nb_p_treatment    0.101     0.036 0.031 0.171\nsigma            1.728     0.126 1.512 2.000\n\n\nHere our treatment effect does solidly increase the height of the plant by 3% - 17%.\n\n\nLooking at the DAG\n\n\n\n\n\n\n\n\n\nAs the DAG helps us see once we control for fungus we are cutting off all of it’s influences (AKA the treatments entire influence here). There is no information in T about H1 that is not also in F."
  },
  {
    "objectID": "ch6_DAG.html#collider-bias",
    "href": "ch6_DAG.html#collider-bias",
    "title": "Chapter 6 Causal Terror",
    "section": "Collider Bias",
    "text": "Collider Bias\nGoing back to our intro on the the most newsworthy science papers being the less trustworthy we can represent the causal model like this.\nThis is a collider structure, trustworthiness and newsworthiness are statistically associated when looking at S but they are not causally associated. Once you learn that a paper has been selected (S), then learning its trustworthiness (T) also provides information about its newsworthiness (N). Otherwise it wouldn’t have been funded. Why? Because if, for example, a selected proposal has low trustworthiness, then it must have high newsworthiness."
  },
  {
    "objectID": "ch4_linear_regression.html",
    "href": "ch4_linear_regression.html",
    "title": "ch4",
    "section": "",
    "text": "!khun"
  },
  {
    "objectID": "ch4_linear_regression.html#first-model",
    "href": "ch4_linear_regression.html#first-model",
    "title": "ch4",
    "section": "",
    "text": "!khun"
  },
  {
    "objectID": "ch4_linear_regression.html#eda-kung-height-weight-gender",
    "href": "ch4_linear_regression.html#eda-kung-height-weight-gender",
    "title": "ch4",
    "section": "EDA !Kung Height ~ Weight + Gender",
    "text": "EDA !Kung Height ~ Weight + Gender\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting the Prior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrid approximation technique\n\nn &lt;- 200\n\nd_grid &lt;-\n  # we'll accomplish with `tidyr::crossing()` what McElreath did with base R `expand.grid()`\n  crossing(mu    = seq(from = 130, to = 180, length.out = n),\n           sigma = seq(from = 0,   to = 15,   length.out = n))\n\n\ngrid_function &lt;- function(mu, sigma) {\n  dnorm(kHeight_adult$height, mean = mu, sd = sigma, log = T) %&gt;% \n    sum()\n}\n\nd_grid &lt;-\n  d_grid %&gt;% \n  mutate(log_likelihood = map2(mu, sigma, grid_function)) %&gt;%\n  unnest(log_likelihood) %&gt;% \n  mutate(prior_mu    = dnorm(mu,    mean = 150, sd  = 10, log = T),\n         prior_sigma = dunif(sigma, min  = 0,   max = 10, log = T),\n         product = log_likelihood + prior_mu + prior_sigma,\n         probability = exp(product- max(product)),\n         prior_product = prior_mu + prior_sigma,\n         prior_probability = exp(prior_product - max(prior_product)))\n\nprior_grid &lt;- \n  tibble(prior_mu = log(rnorm(1e5, mean = 150, sd = 10)),\n         prior_sigma = log(runif(1e5, min = 0, max = 10))) %&gt;% \n  mutate(product = prior_mu + prior_sigma,\n         probability = exp(product - max(product)))\n\nprior_plot &lt;- ggplot(data = d_grid) +\n  geom_point(aes(x = mu, y = sigma, color = prior_probability), alpha = .1) +\n  theme(panel.grid = element_blank()) +\n  scale_color_viridis_c(name = \"Prior Probability\", option = \"A\") +\n  labs(title = \"Prior\")\n  \nposterior_plot &lt;- ggplot(data = d_grid) +\n  geom_point(aes(x = mu, y = sigma, color = probability), alpha = .5) +\n   theme(panel.grid = element_blank()) +\n  scale_color_viridis_c(name = \"Posterior Probability\",  option = \"A\") +\n  labs(title = \"Posterior\")\n\nprior_plot + posterior_plot + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\nggplot(d_grid) +\n  geom_point(aes(x = mu, y = sigma, color = \"Prior\", alpha = prior_probability)) +\n  geom_point(aes(x = mu, y = sigma, color = \"Posterior\", alpha = probability)) +\n  scale_color_manual(values = c(\"Prior\" = \"blue\", \"Posterior\" = \"red\")) +\n  scale_alpha(name = \"Normalized Likelihood\", range = c(0, 0.6)) +\n  labs(title = \"Prior vs Posterior\", color = \"Distribution\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSample from the Posterior\n\nd_grid_samples &lt;- \n  d_grid %&gt;% \n  sample_n(size = 1e4, replace = T, weight = probability)\n\nd_grid_samples %&gt;% \n  ggplot(aes(x = mu, y = sigma)) + \n  geom_point(size = 0.9, alpha = 1/15) +\n  scale_fill_viridis_c() +\n  labs(x = expression(mu[samples]),\n       y = expression(sigma[samples])) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n\nmodel intercept only\n\nb4.1 &lt;- \n  brm(data = kHeight_adult, \n      family = gaussian,\n      height ~ 1,\n      prior = c(prior(normal(178, 20), class = Intercept),\n                prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      backend = \"cmdstanr\", seed = 4, file = \"fits/b04.011\")\n\nplot(b4.1) \n\nLoading required package: rstan\n\n\nLoading required package: StanHeaders\n\n\n\nrstan version 2.32.7 (Stan version 2.32.2)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n\n\n\nAttaching package: 'rstan'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\nThe following objects are masked from 'package:posterior':\n\n    ess_bulk, ess_tail\n\n\n\n\n\n\n\n\n\n\nExtremely Narrow Priors\n\nb4.2 &lt;- \n  brm(data = kHeight_adult, family = gaussian,\n      height ~ 1,\n      prior = c(prior(normal(150, 2), class = Intercept),\n                prior(uniform(0, 6), class = sigma, lb = 0, ub = 6)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/b04.02.6\")\n\nplot(b4.2)\n\n\n\n\n\n\n\nb4.2_draws &lt;- as_draws_df(b4.2)\n\n\nb4.2_draws &lt;- b4.2_draws %&gt;% \n  mutate(prior_mu = rnorm(n(), 150, 2),\n         prior_sigma = runif(n(), 0, 6))\n\nggplot(b4.2_draws, ) +\n  geom_point(aes(x = b_Intercept, y = sigma), alpha = 0.05, color = \"blue\") +\n  geom_density_2d(aes(x = b_Intercept, y = sigma), color = \"blue\", bins = 5) +\n  geom_point(aes(x = prior_mu, y = prior_sigma), alpha = 0.05, color = \"red\") +\n  geom_density_2d(aes(x = prior_mu, y = prior_sigma), color = \"red\", bins = 5) +\n  labs(x = expression(mu), y = expression(sigma),\n       title = \"Posterior Contours of mu and sigma\") +\n  theme_minimal()"
  },
  {
    "objectID": "ch4_linear_regression.html#adding-linear-predictor",
    "href": "ch4_linear_regression.html#adding-linear-predictor",
    "title": "ch4",
    "section": "Adding Linear Predictor",
    "text": "Adding Linear Predictor\n\nkHeight_adult &lt;- \n  kHeight_adult %&gt;%\n  mutate(weight_c = weight - mean(weight))\n\nb4.3 &lt;- \n  brm(data = kHeight_adult, \n      family = gaussian,\n      height ~ 1 + weight_c,\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/b04.03.2\")\n\nas_draws_df(b4.3) %&gt;%\n  select(b_Intercept:sigma) %&gt;%\n  cor() %&gt;%\n  round(digits = 2)\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n            b_Intercept b_weight_c sigma\nb_Intercept        1.00      -0.04  0.02\nb_weight_c        -0.04       1.00  0.01\nsigma              0.02       0.01  1.00\n\npairs(b4.3)\n\n\n\n\n\n\n\n\n\nmu &lt;- fitted(b4.3, summary = F)\n\n# new data\nweight_seq &lt;- tibble(weight_c = seq(from = -18, to = 18, by = 1))\n\nmu &lt;-\n  fitted(b4.3,\n         summary = F,\n         newdata = weight_seq) %&gt;%\n  as_tibble() %&gt;%\n  # here we name the columns after the `weight` values from which they were computed\n  set_names(-18:18) %&gt;% \n  mutate(iter = 1:n())\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\nmu &lt;-  mu %&gt;%\n  gather(weight, height, -iter) %&gt;% \n  # we might reformat `weight` to numerals\n  mutate(weight = as.numeric(weight))\n\nggplot()+\n  geom_point(data = mu, aes(x = weight, y = height), alpha = .002, color = \"blue\")+\n  geom_point(data = kHeight_adult, aes(x = weight_c, y = height))\n\n\n\n\n\n\n\n\n\nplot prediction error\n\npred_height &lt;-\n  predict(b4.3,\n          newdata = weight_seq) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(weight_seq)\n  \npred_height %&gt;%\n  slice(1:6)\n\n# A tibble: 6 × 5\n  Estimate Est.Error  Q2.5 Q97.5 weight_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1     138.      5.20  128.  149.      -18\n2     139.      5.12  129.  149.      -17\n3     140.      5.23  130.  150.      -16\n4     141.      5.12  131.  151.      -15\n5     142.      5.20  132.  152.      -14\n6     143.      5.11  133.  153.      -13"
  },
  {
    "objectID": "ch4_linear_regression.html#model-and-plot-quadratic-kung-height",
    "href": "ch4_linear_regression.html#model-and-plot-quadratic-kung-height",
    "title": "ch4",
    "section": "model and plot Quadratic !Kung height",
    "text": "model and plot Quadratic !Kung height\n\nkHeight &lt;-\n  kHeight %&gt;%\n  mutate(weight_s = (weight - mean(weight)) / sd(weight))\n\nb4.5 &lt;- \n  brm(data = kHeight, \n      family = gaussian,\n      height ~ 1 + weight_s + I(weight_s^2),\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      backend = \"cmdstanr\", silent = 2, seed = 4,\n      file = \"fits/b04.05.1.1\")\n\nweight_seq &lt;- tibble(weight_s = seq(from = min(kHeight$weight_s) - (0.5 * sd(kHeight$weight_s)),\n                                    to = max(kHeight$weight_s) + (0.5 * sd(kHeight$weight_s)), \n                                    length.out = 30))\n\nf &lt;-\n  fitted(b4.5, \n         newdata = weight_seq) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(weight_seq)\n\np &lt;-\n  predict(b4.5, \n          newdata = weight_seq) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(weight_seq) \n\nggplot(data = kHeight, \n       aes(x = weight_s)) +\n  geom_ribbon(data = p, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = f,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  geom_point(aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 1/3) +\n  coord_cartesian(xlim = range(kHeight$weight_s)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\npolynomial (Cubic) model !Kung height\n\nb4.6 &lt;- \n  brm(data = kHeight, \n      family = gaussian,\n      height ~ 1 + weight_s + I(weight_s^2) + I(weight_s^3),\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      backend = \"cmdstanr\", silent = 2,\n      seed = 4,\n      file = \"fits/b04.06\")\n\n\n# can't remember why I fit this model, I don't remember it being in the book\nb4.7 &lt;- \n  brm(data = kHeight, \n      family = gaussian,\n      height ~ 1 + weight_s,\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      backend = \"cmdstanr\", silent = 2,\n      seed = 4,\n      file = \"fits/b04.07\")\n\nf &lt;-\n  fitted(b4.6, \n         newdata = weight_seq) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(weight_seq)\n\np &lt;-\n  predict(b4.6, \n          newdata = weight_seq) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(weight_seq) \n\n\n\nplot Polynomial (Cubic) !Kung height\n\nat &lt;- c(-2, -1, 0, 1, 2)\nggplot(data = kHeight, \n       aes(x = weight_s)) +\n  geom_ribbon(data = p, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = f,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/4) +\n  geom_point(aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 1/3) +\n  coord_cartesian(xlim = range(kHeight$weight_s)) +\n  theme_minimal()+\n  \n  # here it is!\n  scale_x_continuous(\"standardized weight converted back\",\n                     breaks = at,\n                     labels = round(at * sd(kHeight$weight) + mean(kHeight$weight), 1))"
  },
  {
    "objectID": "ch4_linear_regression.html#splines",
    "href": "ch4_linear_regression.html#splines",
    "title": "ch4",
    "section": "Splines!",
    "text": "Splines!\n\nEDA cherry blossom data\n\nlibrary(rethinking)\n\nrethinking (Version 2.42)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following objects are masked from 'package:rstan':\n\n    stan, traceplot\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n\nThe following objects are masked from 'package:brms':\n\n    LOO, stancode, WAIC\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\nggplot(data = cherry, aes(x = year, y = temp))+geom_line()+labs(title = \"Cherry Blossom temperature in March)\")\n\nWarning: Removed 73 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\nSpline Model Cherry Blossom\n\ncherry2 &lt;- cherry %&gt;% \n  filter(!is.na(temp))\n\nnum_knots &lt;- 15\nknot_list &lt;- quantile( cherry2$year, probs=seq(0,1,length.out=num_knots) )\n\n\nB &lt;- bs(cherry2$year,\n    knots=knot_list[-c(1,num_knots)] ,\n    degree=4 , intercept=TRUE )\n\nB_tib &lt;- as_tibble(B) \n\n\nB_tib_join &lt;- cbind(cherry2, B_tib)\n\nB_tib_join_long &lt;- B_tib_join %&gt;% \n  pivot_longer(cols = c(`1`, `2`,`3`, `4`,`5`, `6`,`7`, `8`,`9`, `10`,`11`, `12`,`13`, `14`, `15`, `16`, `17`),\n               names_to = \"knot\",\n               values_to = \"density\")\n\nggplot(data = B_tib_join_long, aes(x = year, y = (density),  color = as.factor(knot)))+geom_line()\n\nDon't know how to automatically pick scale for object of type\n&lt;bs/basis/matrix&gt;. Defaulting to continuous.\n\n\n\n\n\n\n\n\nb4_smooth &lt;- brm(\n  data = cherry2,\n  family = gaussian,\n  formula = temp ~ 1 + s(year, bs = \"bs\", k = 30),  # k sets number of basis functions\n  prior = c(\n    prior(normal(6, 10), class = Intercept),\n    prior(normal(0, 1), class = b),\n    prior(student_t(3, 0, 1), class = sds),      # Prior for smooth term\n    prior(exponential(1), class = sigma)\n  ),\n  backend = \"cmdstanr\", silent = 2,\n  iter = 2000,\n  warmup = 1000,\n  chains = 4,\n  cores = 4,\n  seed = 42,\n  control = list(adapt_delta = 0.99),\n  file = \"fits/b04.cherry_bspline\"\n)\n\nb4.1_smooth &lt;- brm(\n  data = cherry2,\n  family = gaussian,\n  formula = temp ~ 1 + s(year, bs = \"tp\"),  # k sets number of basis functions\n  prior = c(\n    prior(normal(6, 10), class = Intercept),\n    prior(normal(0, 1), class = b),\n    prior(student_t(3, 0, 1), class = sds),      # Prior for smooth term\n    prior(exponential(1), class = sigma)\n  ),\n  backend = \"cmdstanr\", silent = 2,\n  iter = 2000,\n  warmup = 1000,\n  chains = 4,\n  cores = 4,\n  seed = 42,\n  control = list(adapt_delta = 0.99),\n  file = \"fits/b04.cherry_thinSpline\"\n)\n\n\nyear_seq &lt;- tibble(year = seq(from = 800, to = 2000, by = 10))\n\nmu_temp_4 &lt;-\n  fitted(b4_smooth, \n         newdata = year_seq) %&gt;%\n  as_tibble() %&gt;%\n  # let's tack on the `weight` values from `weight_seq`\n  bind_cols(year_seq)\n\npred_temp_4 &lt;-\n  predict(b4_smooth,\n          newdata = year_seq) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(year_seq)\n  \n# pred_temp_4 %&gt;%\n#   slice(1:6)\n\n\nmu_temp_4.1 &lt;-\n  fitted(b4.1_smooth, \n         newdata = year_seq) %&gt;%\n  as_tibble() %&gt;%\n  # let's tack on the `weight` values from `weight_seq`\n  bind_cols(year_seq)\n\npred_temp_4.1 &lt;-\n  predict(b4.1_smooth,\n          newdata = year_seq) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(year_seq)\n  \n# pred_temp_4.1 %&gt;%\n#   slice(1:6)\n\n\n\nPlots Cherry Blossom\n\ncherry2 %&gt;%\n  ggplot(aes(x = year)) +\n  geom_ribbon(data = pred_temp_4, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = mu_temp_4,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  geom_point(aes(y = temp),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  ylab(\"temp\") +\n  labs(title = \"15 knot B-splines\")\n\n\n\n\n\n\n\ncherry2 %&gt;%\n  ggplot(aes(x = year)) +\n  geom_ribbon(data = pred_temp_4.1, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = mu_temp_4.1,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  geom_line(aes(y = temp),\n             color = \"navyblue\", shape = 1) +\n  ylab(\"temp\") +\n  labs(title = \"Thin Plated Spline\")\n\nWarning in geom_line(aes(y = temp), color = \"navyblue\", shape = 1): Ignoring\nunknown parameters: `shape`\n\n\n\n\n\n\n\n\n# I am confused as to why the the Thin plated splines don't go more crazy and try to fit all of the contours"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Rethinking Homepage",
    "section": "",
    "text": "Welcome\nThis site showcases projects from my Statistical Rethinking work in R and Quarto.\n\n📈 Linear Regression\n\n🔀 Multi - Linear Regression\n\n♻ Causal Paths\n\nSource code is on GitHub."
  },
  {
    "objectID": "ch5_multilinear.html",
    "href": "ch5_multilinear.html",
    "title": "Chapter 5 Multi-variable Regression DAG’s and Causality",
    "section": "",
    "text": "[claude] come up with 3-4 bullet points on what was accomplished in this chapter."
  },
  {
    "objectID": "ch5_multilinear.html#overview",
    "href": "ch5_multilinear.html#overview",
    "title": "Chapter 5 Multi-variable Regression DAG’s and Causality",
    "section": "",
    "text": "[claude] come up with 3-4 bullet points on what was accomplished in this chapter."
  },
  {
    "objectID": "ch5_multilinear.html#motivation",
    "href": "ch5_multilinear.html#motivation",
    "title": "Chapter 5 Multi-variable Regression DAG’s and Causality",
    "section": "Motivation",
    "text": "Motivation\n[claude] come up with a distrilled paragraph argument for why we should care about muli-variate regression. Here’s a section of Richard McElreath’s text statistical Rethinking\n\nStatistical “control” for confounds. A confound is something that misleads us about a causal influence—there will be a more precise definition in the next chapter. The spurious waffles and divorce correlation is one possible type of confound, where the confound (southernness) makes a variable with no real importance (Waffle House density) appear to be important. But confounds are diverse. They can hide real important variables just as easily as they can produce false ones.\nMultiple causation. A phenomenon may arise from multiple causes. Measurement of each cause is useful, so when we can use the same data to estimate more than one type of influence, we should. Furthermore, when causation is multiple, one cause can hide another.\nInteractions. The importance of one variable may depend upon another. For ex- ample, plants benefit from both light and water. But in the absence of either, the other is no benefit at all. Such interactions occur very often. Effective inference about one variable will often depend upon consideration of others.\n\nClaude give a quick description of the waffle divorce data set : Data for the individual States of the United States, describing number of Waffle House diners and various marriage and demographic facts.\nFormat Location : State name Loc : State abbreviation Population : 2010 population in millions MedianAgeMarriage: 2005-2010 median age at marriage Marriage : 2009 marriage rate per 1000 adults Marriage.SE : Standard error of rate Divorce : 2009 divorce rate per 1000 adults Divorce.SE : Standard error of rate WaffleHouses : Number of diners South : 1 indicates Southern State Slaves1860 : Number of slaves in 1860 census Population1860 : Population from 1860 census PropSlaves1860 : Proportion of total population that were slaves in 1860\n\n\n\n\n\n\n\n\n\nMost likely a spurious correlation, waffles don’t cause divorces nor vice versa"
  },
  {
    "objectID": "ch5_multilinear.html#spurious-correlation",
    "href": "ch5_multilinear.html#spurious-correlation",
    "title": "Chapter 5 Multi-variable Regression DAG’s and Causality",
    "section": "Spurious Correlation",
    "text": "Spurious Correlation\n\nProblem Intro\nExcerpt from Richard McElreath: “Let’s leave waffles behind, at least for the moment. An example that is easier to understand is the correlation between divorce rate and marriage rate (Figure 5.2). The rate at which adults marry is a great predictor of divorce rate, as seen in the left-hand plot in the figure. But does marriage cause divorce? In a trivial sense it obviously does: One cannot get a divorce without first getting married. But there’s no reason high marriage rate must be correlated with divorce.”\n\n\n\n\n\n\n\n\n\nWe see that Divorce rate is high in the south and low in the north midwest and mid atlantic. Marriage rate is high in\n\n\n\n\n\n\n\n\n\n\n\nMedian Marriage Age Model\n\\[ \\textbf{b5.1} \\]\n\\[ \\text{Divorce}_i \\sim \\text{Intercept} + \\beta_1 \\text{Median Age Marriage}_i \\]\n\n📈 μ vs full distribution📊 Summary⚙️ Code🌌 Prior Vs Posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Divorce ~ 1 + MedianAgeMarriage_s \n   Data: WaffleDivorce (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept               9.69      0.22     9.26    10.12 1.00     4663     4042\nMedianAgeMarriage_s    -1.04      0.21    -1.46    -0.62 1.00     5599     3980\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.51      0.16     1.24     1.85 1.00     5275     4263\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\nb5.1 &lt;- \n  brm(data = WaffleDivorce, \n      family = gaussian,\n      Divorce ~ 1 + MedianAgeMarriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma, ub = 10)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/b05.01\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe regression of Divorce Rate on Age of Marriage, tells us only that the total influence of age at marriage is strongly negative with divorce rate.\n\n\nMarriage Rate\n\\[ \\textbf{b5.2} \\]\n\\[ \\text{Divorce}_i \\sim \\text{Intercept} + \\beta_1 \\text{Marriage % Rate}_i \\]\n\n📈 μ vs full distribution📊 Summary⚙️ Code🌌 Prior Vs Posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Divorce ~ 1 + Marriage_s \n   Data: WaffleDivorce (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      9.69      0.25     9.21    10.18 1.00     5774     4384\nMarriage_s     0.64      0.24     0.15     1.11 1.00     5742     4259\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.75      0.18     1.44     2.15 1.00     5708     4040\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\nb5.2  &lt;- \n  brm(data = WaffleDivorce, \n      family = gaussian,\n      Divorce ~ 1 + Marriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma, ub = 10)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/b05.02\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCombined Multi-linear model\n\\[ \\textbf{b5.3} \\]\n\\[ \\text{Divorce}_i \\sim \\text{Intercept} + \\beta_1 \\text{Marriage % Rate}_i + \\beta_2 \\text{Median Marriage Age}_i \\]\n\n🎛️ Parameters📊 Summary⚙️ Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s \n   Data: WaffleDivorce (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept               9.68      0.22     9.25    10.11 1.00     6254     4493\nMarriage_s             -0.12      0.30    -0.71     0.46 1.00     3884     3601\nMedianAgeMarriage_s    -1.12      0.30    -1.71    -0.53 1.00     3726     3644\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.53      0.17     1.24     1.91 1.00     4836     3480\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\nb5.3 &lt;- brm(data = WaffleDivorce, \n      family = gaussian,\n      Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma, ub = 10)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/b05.031\")\n\n\n\n\n\n\nDiagnosing Multi-linear models\n\nPredictor Residual PlotPosterior Prediction PlotsCausal Counterfactual Plots\n\n\n\nMarriage RateMedian Marriage Age\n\n\n\n\n\n\n\n\n\n\n\nLittle relationship between divorce and marriage rates, once we have accounted for the effect of Median Marriage Age\n\n\nNegative relationship between divorce and marriage age, even after we have accounted for the effect of the Marriage % Rate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“This procedure also brings home the message that regression models measure the remaining association of each predictor with the out- come, after already knowing the other predictors. In computing the predictor residual plots, you had to perform those calculations yourself. In the unified multivariate model, it all hap- pens automatically. Nevertheless, it is useful to keep this fact in mind, because regressions can behave in surprising ways as a result.”\n\n\nSolving the DAG\nFrom McElreath, “Age of marriage influences divorce in two ways. First it can have a direct effect, perhaps because younger people change faster than older people and are therefore more likely to grow incompatible with a partner. Second, it can have an indirect effect by influencing the marriage rate. If people get married earlier, then the marriage rate may rise, because there are more young people. Consider for example if an evil dictator forced everyone to marry at age 65. Since a smaller fraction of the population lives to 65 than to 25, forcing delayed marriage will also reduce the marriage rate. If marriage rate itself has any direct effect on divorce, maybe by making marriage more or less normative, then some of that direct effect could be the indirect effect of age at marriage.”\n\n\n\n\n\n\n\n\n\nOnce we know median age at marriage for a State, there is little or no additional predictive power in also knowing the rate of marriage in that State.\n\nResidual MapWaffle House Effect"
  },
  {
    "objectID": "ch5_multilinear.html#masked-relationships",
    "href": "ch5_multilinear.html#masked-relationships",
    "title": "Chapter 5 Multi-variable Regression DAG’s and Causality",
    "section": "Masked Relationships",
    "text": "Masked Relationships\nNew data set Comparative primate milk composition data, from Table 2 of Hinde and Milligan. 2011. Evolutionary Anthropology 20:9-23.\nspecies: Species name\nkcal.per.g: Kilocalories per gram of milk\nmass: Body mass of mother, in kilograms\nneocortex.perc: Percent of brain mass that is neocortex\n\nNeocortex % Effect on Milk Calories\n\n📈 μ vs full distribution📊 Summary⚙️ Code🌌 Prior Vs Posterior\n\n\n\n\n\n\n\n\n\n\n\nThe posterior mean line is weakly positive, but it is highly imprecise. A lot of mildly positive and negative slopes are plausible, given this model and these data.\n\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: kcal.per.g_s ~ 1 + neocortex.perc_s \n   Data: milk2 (Number of observations: 17) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            0.00      0.16    -0.31     0.30 1.00     4768     3968\nneocortex.perc_s     0.12      0.23    -0.34     0.57 1.00     5006     4052\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.04      0.20     0.74     1.50 1.00     4534     3643\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\nmilk.01 &lt;- \n  brm(data = milk2, \n      family = gaussian,\n      kcal.per.g_s ~ 1 + neocortex.perc_s,\n      prior = c(prior(normal(0, .2), class = Intercept),\n                prior(normal(0, .5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/milk.01.6\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMass Effect on Milk Calories\nNow consider another predictor variable, adult female body mass, mass in the data frame. Let’s use the logarithm of mass, log(mass), as a predictor as well. Why the logarithm of mass instead of the raw mass in kilograms? It is often true that scaling measurements like body mass are related by magnitudes to other variables. Taking the log of a measure trans- lates the measure into magnitudes. So by using the logarithm of body mass here, we’re saying that we suspect that the magnitude of a mother’s body mass is related to milk energy, in a linear fashion.\n\n📈 μ vs full distribution📊 Summary⚙️ Code\n\n\n\n\n\n\n\n\n\n\n\nThe posterior mean line is moderately negative, but it is highly imprecise.\n\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: kcal.per.g_s ~ 1 + logMass_s \n   Data: milk2 (Number of observations: 17) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.00      0.15    -0.30     0.30 1.00     5489     4039\nlogMass_s    -0.29      0.22    -0.72     0.17 1.00     4914     3379\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.99      0.18     0.70     1.38 1.00     5005     3871\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\nmilk.02 &lt;- \n  brm(data = milk2, \n      family = gaussian,\n      kcal.per.g_s ~ 1 + logMass_s,\n      prior = c(prior(normal(0, .2), class = Intercept),\n                prior(normal(0, .5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/milk.2.0\")\n\n\n\n\n\n\nMultivariate Milk\n\nParametersCounterfactual Plots"
  },
  {
    "objectID": "ch5_multilinear.html#categorical-variable",
    "href": "ch5_multilinear.html#categorical-variable",
    "title": "Chapter 5 Multi-variable Regression DAG’s and Causality",
    "section": "Categorical Variable",
    "text": "Categorical Variable\n\nBinary Categories\nTwo mutually exclusive categories make the interpretation of models quite simple because it’s hard to forget that the lack of Category A implies Category B. An example, lets use male vs females as our categorical variable, with Males being represented as 1 and females as 0 in our data. The models interpretations is that the intercept is the effect of being female and the intercept + \\(\\beta_\\text{effect of male}\\) is the males effect. The same math goes into the interpretations when our categorical variables have more than 2 options, but it’s quite easy to forget that your comparing to one of the categories when you have a list of 20 categories. Always remember that you are comparing to some other category with factor effects.\nLet’s go back to our Kung! height dataset we used in the last chapter. Let’s ignore the effect of weight and the other variables and focus only on sex. Our model is\n\\[ \\text{Height}_i \\sim \\text{Normal(}\\mu_i, \\ \\sigma_i\\text{)}\\] \\[ \\mu_i = \\alpha + \\beta_\\text{male} \\mathbf{I}\\text{(male = 1)}\\]\n\\[\\alpha \\sim \\text{Normal(178, 20)}\\]\n\\[ \\beta_\\text{male} \\sim \\text{Normal(0, 10)}\\]\n\\[ \\sigma \\sim \\text{Uniform(0, 50)}\\]\nThe parameter \\(\\beta_\\text{male}\\) only influences prediction for the cases where the individual is a male. Using this approach means that \\(\\beta_\\text{male}\\) represents the expected difference between males and females in height. This makes assigning our priors a bit harder. Our approach assumes there is more uncertainty about males than females, because a male includes two parameters and therefore has two parameters.\n\n\n\n\n\n\n\n\n\nOur priors are a bit wide, and the extra variance on males doesn’t make to much sense.\nA better way is to use an index variable.\n\\[ \\text{Height}_i \\sim \\text{Normal(}\\mu_i, \\ \\sigma_i\\text{)}\\] \\[ \\mu_i = \\alpha_\\text{Sex[i]}\\]\n\\[\\alpha_j \\sim \\text{Normal(178, 20),  for j  = 1..2}\\]\n\\[ \\sigma \\sim \\text{Uniform(0, 50)}\\]\n\n\n\n\n\n\n\n\n\nWe actually care about the differences in the genders in height in this case. So for this model we simply subtract the posterior intercepts from each other to show that difference.\n\n\n\n\n\n\n\n\n\nBinary categories are easy, whether you use an indicator variable or instead an index variable. But when there are more than two categories, the indicator variable approach explodes. While the index approach does not change at all when you add more categories. You do get more parameters, of course, just as many as in the indicator variable approach. But the model specification looks just like it does in the binary case. And the priors continue to be easier, unless you really do have prior information about contrasts.\n\n\nMilk example\n\n\n              clade                  species kcal.per.g perc.fat perc.protein\n1     Strepsirrhine           Eulemur fulvus       0.49    16.60        15.42\n2     Strepsirrhine                 E macaco       0.51    19.27        16.91\n3     Strepsirrhine                 E mongoz       0.46    14.11        16.85\n4     Strepsirrhine            E rubriventer       0.48    14.91        13.18\n5     Strepsirrhine              Lemur catta       0.60    27.28        19.50\n6  New World Monkey       Alouatta seniculus       0.47    21.22        23.58\n7  New World Monkey               A palliata       0.56    29.66        23.46\n8  New World Monkey             Cebus apella       0.89    53.41        15.80\n9  New World Monkey      Saimiri boliviensis       0.91    46.08        23.34\n10 New World Monkey               S sciureus       0.92    50.58        22.33\n11 New World Monkey         Cebuella pygmaea       0.80    41.35        20.85\n12 New World Monkey        Callimico goeldii       0.46     3.93        25.30\n13 New World Monkey       Callithrix jacchus       0.71    38.38        20.09\n14 New World Monkey   Leontopithecus rosalia       0.71    36.90        21.27\n15 Old World Monkey  Chlorocebus pygerythrus       0.73    39.17        14.65\n16 Old World Monkey      Miopithecus talpoin       0.68    40.15        18.08\n17 Old World Monkey                M fuscata       0.72    53.05        13.00\n18 Old World Monkey                M mulatta       0.97    55.51        13.17\n19 Old World Monkey                 M sinica       0.79    48.90        13.91\n20 Old World Monkey                Papio spp       0.84    54.31        10.97\n21              Ape        Nomascus concolor       0.48    15.96        12.52\n22              Ape            Hylobates lar       0.62    34.51        12.57\n23              Ape Symphalangus syndactylus       0.51    26.42        13.46\n24              Ape           Pongo pygmaeus       0.54    37.78         7.37\n25              Ape  Gorilla gorilla gorilla       0.49    27.18        16.29\n26              Ape       G gorilla beringei       0.53    30.59        20.77\n27              Ape             Pan paniscus       0.48    21.18        11.68\n28              Ape            P troglodytes       0.55    36.84         9.54\n29              Ape             Homo sapiens       0.71    50.49         9.84\n   perc.lactose  mass neocortex.perc\n1         67.98  1.95          55.16\n2         63.82  2.09             NA\n3         69.04  2.51             NA\n4         71.91  1.62             NA\n5         53.22  2.19             NA\n6         55.20  5.25          64.54\n7         46.88  5.37          64.54\n8         30.79  2.51          67.64\n9         30.58  0.71             NA\n10        27.09  0.68          68.85\n11        37.80  0.12          58.85\n12        70.77  0.47          61.69\n13        41.53  0.32          60.32\n14        41.83  0.60             NA\n15        46.18  3.47             NA\n16        41.77  1.55          69.97\n17        33.95  7.08             NA\n18        31.32  3.24          70.41\n19        37.19  7.94             NA\n20        34.72 12.30          73.40\n21        71.52  7.59             NA\n22        52.92  5.37          67.53\n23        60.12 10.72             NA\n24        54.85 35.48          71.26\n25        56.53 79.43          72.60\n26        48.64 97.72             NA\n27        67.14 40.74          70.24\n28        53.62 33.11          76.30\n29        39.67 54.95          75.49"
  }
]