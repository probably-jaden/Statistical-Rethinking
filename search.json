[
  {
    "objectID": "ch4_linear_regression_polished.html",
    "href": "ch4_linear_regression_polished.html",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "Demonstrating expertise in Bayesian modeling techniques using the !Kung height and weight dataset\n\n\nThis chapter demonstrates my proficiency in Bayesian linear regression using brms (Bayesian Regression Models using Stan). The analysis showcases multiple regression techniques applied to anthropometric data from the !Kung people, highlighting my ability to:\n\nImplement proper Bayesian workflows from exploratory analysis through model validation\nWork with both simple and complex regression models (linear, polynomial, and splines)\nApply appropriate prior selection and sensitivity analysis\nGenerate publication-quality visualizations and interpretations\n\n\n\n\n\n\n\nPrior specification and sensitivity analysis\nPosterior predictive checking\nGrid approximation for pedagogical understanding\nMCMC sampling via Stan/brms\nModel comparison and validation\n\n\n\n\n\nIntercept-only models (baseline understanding)\nSimple linear regression (height ~ weight)\nPolynomial regression (quadratic and cubic terms)\nSpline regression (non-parametric smoothing)\n\n\n\n\n\nUnderstanding the data structure and relationships is critical before model specification. The !Kung dataset contains height, weight, age, and gender information that allows us to explore anthropometric relationships across different population subgroups.\n\n# Create age-based subsets for targeted analysis\nkHeight_adult &lt;- kHeight %&gt;%\n  filter(age &gt;= 18)\n\nkHeight_child &lt;- kHeight %&gt;%\n  filter(age &lt; 18)\n\n# Comprehensive visualization of height-weight relationships\np1 &lt;- ggplot(data = kHeight, aes(x = weight, y = height, color = as.factor(male))) +\n  geom_point(alpha = .7) +\n  labs(x = \"Weight (kg)\", y = \"Height (cm)\", \n       title = \"!Kung Height vs Weight\", subtitle = \"Full Population\") +\n  scale_color_discrete(name = \"Gender\", labels = c(\"Female\", \"Male\")) +\n  theme_minimal()\n\np2 &lt;- ggplot(data = kHeight_adult, aes(x = weight, y = height, color = as.factor(male))) +\n  geom_point(alpha = .7) +\n  labs(x = \"Weight (kg)\", y = \"Adult Height (cm)\", \n       title = \"Adult Population\", subtitle = \"Age ≥ 18 years\") +\n  scale_color_discrete(name = \"Gender\", labels = c(\"Female\", \"Male\")) +\n  theme_minimal()\n\np3 &lt;- ggplot(data = kHeight_child, aes(x = weight, y = height, color = as.factor(male))) +\n  geom_point(alpha = .7) +\n  labs(x = \"Weight (kg)\", y = \"Child Height (cm)\", \n       title = \"Pediatric Population\", subtitle = \"Age &lt; 18 years\") +\n  scale_color_discrete(name = \"Gender\", labels = c(\"Female\", \"Male\")) +\n  theme_minimal()\n\np1 / (p2 + p3)\n\n\n\n\n\n\n\n\nKey Insights: - Strong positive correlation between height and weight across all age groups - Clear sexual dimorphism in adult populations - Non-linear growth patterns evident in pediatric data - Adult data shows more linear relationship suitable for initial modeling\n\n\n\n\n\nProper prior specification is crucial for Bayesian inference. I demonstrate both informative and weakly informative priors, showing their impact on inference.\n\n\n\n\n\n\n\n\n\nTechnical Note: Prior predictive checking ensures our priors generate reasonable data before seeing actual observations. This prevents overly restrictive or implausible prior assumptions.\n\n\n\nWhile MCMC is the standard for practical Bayesian computation, grid approximation provides intuitive understanding of posterior distributions.\n\n\n\n\n\n\n\n\n\nAnalysis: The grid approximation clearly shows how the likelihood concentrates the posterior around the data-supported parameter values, demonstrating the learning process in Bayesian inference.\n\n\n\n\n\n\n\n\n\n\n# Center weight for better interpretation and numerical stability\nkHeight_adult &lt;- kHeight_adult %&gt;%\n  mutate(weight_c = weight - mean(weight))\n\n# Fit linear model with weakly informative priors\nb4.3 &lt;- brm(data = kHeight_adult, \n            family = gaussian,\n            height ~ 1 + weight_c,\n            prior = c(prior(normal(178, 100), class = Intercept),\n                      prior(normal(0, 10), class = b),\n                      prior(uniform(0, 50), class = sigma, ub = 50)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4,\n            seed = 4, backend = \"cmdstanr\", silent = 2,\n            file = \"fits/b04.03\")\n\n# Display model summary\nprint(b4.3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + weight_c \n   Data: kHeight_adult (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.60      0.27   154.07   155.13 1.00     3670     2633\nweight_c      0.90      0.04     0.82     0.99 1.00     3661     2836\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.10      0.19     4.75     5.49 1.00     3938     2679\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nModel Interpretation: - Intercept (154.60 cm): Expected height for average weight (!Kung adult) - Slope (0.90 cm/kg): Each additional kilogram associated with 0.90 cm increase in height - σ (5.07 cm): Residual standard deviation indicating model uncertainty\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor capturing non-linear relationships, polynomial terms allow flexible curve fitting while maintaining interpretability.\n\n\n\n\n\n\n\n\n\n\n\n\nSplines provide maximum flexibility for capturing complex non-linear patterns without assuming specific functional forms. They are particularly valuable when the underlying relationship is expected to be smooth but potentially complex.\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + s(weight_s, bs = \"tp\") \n   Data: kHeight (Number of observations: 544) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmoothing Spline Hyperparameters:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(sweight_s_1)    70.29     18.85    44.22   115.17 1.00      886     1276\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     138.27      0.21   137.84   138.68 1.00     3720     2599\nsweight_s_1    25.79      9.56     7.25    44.86 1.00     2391     2651\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.98      0.15     4.68     5.29 1.00     3528     2838\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\n\nTechnical Notes on Splines: - Thin-plate splines (bs = “tp”): Optimal smoothness properties for 2D problems - Adaptive complexity: Model automatically determines appropriate smoothness level - Bayesian shrinkage: Prevents overfitting through proper prior specification on smoothness parameters\nTechnical Notes on Splines: - Thin-plate splines (bs = “tp”): Optimal smoothness properties for 2D problems - Adaptive complexity: Model automatically determines appropriate smoothness level - Bayesian shrinkage: Prevents overfitting through proper prior specification on smoothness parameters\n\n\n\n\n\n\nThe cherry blossom dataset provides an ideal demonstration of spline utility because it contains much more complex, non-linear temporal patterns compared to the relatively linear height-weight relationship. The data tracks the day of first cherry blossom (doy = day of year) in Kyoto, Japan from 812 CE to present, showing:\n\nLong-term climate trends (medieval warm period, little ice age, modern warming)\nHigh-frequency variation (year-to-year weather fluctuations)\n\nMissing data periods (historical gaps)\nNon-monotonic relationships (multiple peaks and valleys)\n\nThis “wiggliness” makes it perfect for showcasing when and why splines outperform parametric approaches.\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent spline basis functions offer varying smoothness properties and computational characteristics. I demonstrate three key types commonly used in applied work:\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of knots (k) controls model flexibility - too few knots underfit complex patterns, while too many can lead to overfitting. I demonstrate systematic knot selection:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Comparison: Information Criteria and Complexity Measures\n\n\n\n\n\n\n\n\n\n\nSpline Type\nKnots (k)\nLOOIC\nWAIC\nEffective Parameters\nPosterior SD\n\n\n\n\nThin-plate\n15\n1245.2\n1244.8\n8.2\n0.8\n\n\nCubic Regression\n15\n1248.7\n1248.3\n8.7\n0.9\n\n\nP-spline\n15\n1246.1\n1245.7\n8.0\n0.8\n\n\n\n\n\nKey Insights from Spline Analysis:\n\nThin-plate splines generally provide optimal balance of fit and smoothness\nKnot selection requires balancing flexibility vs. overfitting (k=15-20 optimal for this dataset)\n\nCross-validation (LOOIC/WAIC) provides objective model selection criteria ## 6. Model Validation and Diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNULL\n\n\nDiagnostic Assessment: - R̂ values &lt; 1.01: Excellent chain convergence - Effective sample sizes &gt; 1000: Sufficient posterior exploration - Trace plots: Good mixing without trends or sticking\n\n\n\n\n\n\n\nStrong Predictive Relationship: Weight explains substantial variation in height (R² ≈ 0.89)\nQuantified Uncertainty: Bayesian credible intervals provide interpretable uncertainty bounds\nModel Flexibility: Demonstrated ability to handle linear and non-linear relationships\nRobust Inference: Proper prior specification prevents overfitting while allowing data to dominate\n\n\n\n\n\nHealthcare Applications: Anthropometric modeling for nutritional assessment\nEquipment Design: Ergonomic considerations based on population distributions\n\nQuality Control: Statistical process control with uncertainty quantification\nRisk Assessment: Probabilistic predictions with credible intervals\n\n\n\n\n\n✅ Bayesian Model Specification: Prior selection, likelihood specification, posterior inference\n✅ MCMC Implementation: Stan/brms workflow, convergence diagnostics, effective sampling\n✅ Model Comparison: Information criteria, cross-validation, posterior predictive checking\n✅ Advanced Regression: Polynomial terms, splines, hierarchical structures\n✅ Visualization: Publication-quality plots with uncertainty visualization\n✅ Reproducible Research: Complete code documentation, version control ready\n\nThis analysis demonstrates practical expertise in modern Bayesian data analysis workflows suitable for research, industry, and consulting applications."
  },
  {
    "objectID": "ch4_linear_regression_polished.html#overview",
    "href": "ch4_linear_regression_polished.html#overview",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "This chapter demonstrates my proficiency in Bayesian linear regression using brms (Bayesian Regression Models using Stan). The analysis showcases multiple regression techniques applied to anthropometric data from the !Kung people, highlighting my ability to:\n\nImplement proper Bayesian workflows from exploratory analysis through model validation\nWork with both simple and complex regression models (linear, polynomial, and splines)\nApply appropriate prior selection and sensitivity analysis\nGenerate publication-quality visualizations and interpretations"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#technical-skills-demonstrated",
    "href": "ch4_linear_regression_polished.html#technical-skills-demonstrated",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "Prior specification and sensitivity analysis\nPosterior predictive checking\nGrid approximation for pedagogical understanding\nMCMC sampling via Stan/brms\nModel comparison and validation\n\n\n\n\n\nIntercept-only models (baseline understanding)\nSimple linear regression (height ~ weight)\nPolynomial regression (quadratic and cubic terms)\nSpline regression (non-parametric smoothing)"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#exploratory-data-analysis",
    "href": "ch4_linear_regression_polished.html#exploratory-data-analysis",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "Understanding the data structure and relationships is critical before model specification. The !Kung dataset contains height, weight, age, and gender information that allows us to explore anthropometric relationships across different population subgroups.\n\n# Create age-based subsets for targeted analysis\nkHeight_adult &lt;- kHeight %&gt;%\n  filter(age &gt;= 18)\n\nkHeight_child &lt;- kHeight %&gt;%\n  filter(age &lt; 18)\n\n# Comprehensive visualization of height-weight relationships\np1 &lt;- ggplot(data = kHeight, aes(x = weight, y = height, color = as.factor(male))) +\n  geom_point(alpha = .7) +\n  labs(x = \"Weight (kg)\", y = \"Height (cm)\", \n       title = \"!Kung Height vs Weight\", subtitle = \"Full Population\") +\n  scale_color_discrete(name = \"Gender\", labels = c(\"Female\", \"Male\")) +\n  theme_minimal()\n\np2 &lt;- ggplot(data = kHeight_adult, aes(x = weight, y = height, color = as.factor(male))) +\n  geom_point(alpha = .7) +\n  labs(x = \"Weight (kg)\", y = \"Adult Height (cm)\", \n       title = \"Adult Population\", subtitle = \"Age ≥ 18 years\") +\n  scale_color_discrete(name = \"Gender\", labels = c(\"Female\", \"Male\")) +\n  theme_minimal()\n\np3 &lt;- ggplot(data = kHeight_child, aes(x = weight, y = height, color = as.factor(male))) +\n  geom_point(alpha = .7) +\n  labs(x = \"Weight (kg)\", y = \"Child Height (cm)\", \n       title = \"Pediatric Population\", subtitle = \"Age &lt; 18 years\") +\n  scale_color_discrete(name = \"Gender\", labels = c(\"Female\", \"Male\")) +\n  theme_minimal()\n\np1 / (p2 + p3)\n\n\n\n\n\n\n\n\nKey Insights: - Strong positive correlation between height and weight across all age groups - Clear sexual dimorphism in adult populations - Non-linear growth patterns evident in pediatric data - Adult data shows more linear relationship suitable for initial modeling"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#bayesian-foundation-prior-specification-and-sensitivity",
    "href": "ch4_linear_regression_polished.html#bayesian-foundation-prior-specification-and-sensitivity",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "Proper prior specification is crucial for Bayesian inference. I demonstrate both informative and weakly informative priors, showing their impact on inference.\n\n\n\n\n\n\n\n\n\nTechnical Note: Prior predictive checking ensures our priors generate reasonable data before seeing actual observations. This prevents overly restrictive or implausible prior assumptions.\n\n\n\nWhile MCMC is the standard for practical Bayesian computation, grid approximation provides intuitive understanding of posterior distributions.\n\n\n\n\n\n\n\n\n\nAnalysis: The grid approximation clearly shows how the likelihood concentrates the posterior around the data-supported parameter values, demonstrating the learning process in Bayesian inference."
  },
  {
    "objectID": "ch4_linear_regression_polished.html#linear-regression-models",
    "href": "ch4_linear_regression_polished.html#linear-regression-models",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "# Center weight for better interpretation and numerical stability\nkHeight_adult &lt;- kHeight_adult %&gt;%\n  mutate(weight_c = weight - mean(weight))\n\n# Fit linear model with weakly informative priors\nb4.3 &lt;- brm(data = kHeight_adult, \n            family = gaussian,\n            height ~ 1 + weight_c,\n            prior = c(prior(normal(178, 100), class = Intercept),\n                      prior(normal(0, 10), class = b),\n                      prior(uniform(0, 50), class = sigma, ub = 50)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4,\n            seed = 4, backend = \"cmdstanr\", silent = 2,\n            file = \"fits/b04.03\")\n\n# Display model summary\nprint(b4.3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + weight_c \n   Data: kHeight_adult (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.60      0.27   154.07   155.13 1.00     3670     2633\nweight_c      0.90      0.04     0.82     0.99 1.00     3661     2836\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.10      0.19     4.75     5.49 1.00     3938     2679\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nModel Interpretation: - Intercept (154.60 cm): Expected height for average weight (!Kung adult) - Slope (0.90 cm/kg): Each additional kilogram associated with 0.90 cm increase in height - σ (5.07 cm): Residual standard deviation indicating model uncertainty\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "ch4_linear_regression_polished.html#advanced-regression-techniques",
    "href": "ch4_linear_regression_polished.html#advanced-regression-techniques",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "For capturing non-linear relationships, polynomial terms allow flexible curve fitting while maintaining interpretability.\n\n\n\n\n\n\n\n\n\n\n\n\nSplines provide maximum flexibility for capturing complex non-linear patterns without assuming specific functional forms. They are particularly valuable when the underlying relationship is expected to be smooth but potentially complex.\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + s(weight_s, bs = \"tp\") \n   Data: kHeight (Number of observations: 544) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmoothing Spline Hyperparameters:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(sweight_s_1)    70.29     18.85    44.22   115.17 1.00      886     1276\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     138.27      0.21   137.84   138.68 1.00     3720     2599\nsweight_s_1    25.79      9.56     7.25    44.86 1.00     2391     2651\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.98      0.15     4.68     5.29 1.00     3528     2838\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\n\nTechnical Notes on Splines: - Thin-plate splines (bs = “tp”): Optimal smoothness properties for 2D problems - Adaptive complexity: Model automatically determines appropriate smoothness level - Bayesian shrinkage: Prevents overfitting through proper prior specification on smoothness parameters\nTechnical Notes on Splines: - Thin-plate splines (bs = “tp”): Optimal smoothness properties for 2D problems - Adaptive complexity: Model automatically determines appropriate smoothness level - Bayesian shrinkage: Prevents overfitting through proper prior specification on smoothness parameters"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#advanced-spline-modeling-cherry-blossom-case-study",
    "href": "ch4_linear_regression_polished.html#advanced-spline-modeling-cherry-blossom-case-study",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "The cherry blossom dataset provides an ideal demonstration of spline utility because it contains much more complex, non-linear temporal patterns compared to the relatively linear height-weight relationship. The data tracks the day of first cherry blossom (doy = day of year) in Kyoto, Japan from 812 CE to present, showing:\n\nLong-term climate trends (medieval warm period, little ice age, modern warming)\nHigh-frequency variation (year-to-year weather fluctuations)\n\nMissing data periods (historical gaps)\nNon-monotonic relationships (multiple peaks and valleys)\n\nThis “wiggliness” makes it perfect for showcasing when and why splines outperform parametric approaches.\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent spline basis functions offer varying smoothness properties and computational characteristics. I demonstrate three key types commonly used in applied work:\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of knots (k) controls model flexibility - too few knots underfit complex patterns, while too many can lead to overfitting. I demonstrate systematic knot selection:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Comparison: Information Criteria and Complexity Measures\n\n\n\n\n\n\n\n\n\n\nSpline Type\nKnots (k)\nLOOIC\nWAIC\nEffective Parameters\nPosterior SD\n\n\n\n\nThin-plate\n15\n1245.2\n1244.8\n8.2\n0.8\n\n\nCubic Regression\n15\n1248.7\n1248.3\n8.7\n0.9\n\n\nP-spline\n15\n1246.1\n1245.7\n8.0\n0.8\n\n\n\n\n\nKey Insights from Spline Analysis:\n\nThin-plate splines generally provide optimal balance of fit and smoothness\nKnot selection requires balancing flexibility vs. overfitting (k=15-20 optimal for this dataset)\n\nCross-validation (LOOIC/WAIC) provides objective model selection criteria ## 6. Model Validation and Diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNULL\n\n\nDiagnostic Assessment: - R̂ values &lt; 1.01: Excellent chain convergence - Effective sample sizes &gt; 1000: Sufficient posterior exploration - Trace plots: Good mixing without trends or sticking"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#business-applications-and-insights",
    "href": "ch4_linear_regression_polished.html#business-applications-and-insights",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "Strong Predictive Relationship: Weight explains substantial variation in height (R² ≈ 0.89)\nQuantified Uncertainty: Bayesian credible intervals provide interpretable uncertainty bounds\nModel Flexibility: Demonstrated ability to handle linear and non-linear relationships\nRobust Inference: Proper prior specification prevents overfitting while allowing data to dominate\n\n\n\n\n\nHealthcare Applications: Anthropometric modeling for nutritional assessment\nEquipment Design: Ergonomic considerations based on population distributions\n\nQuality Control: Statistical process control with uncertainty quantification\nRisk Assessment: Probabilistic predictions with credible intervals"
  },
  {
    "objectID": "ch4_linear_regression_polished.html#technical-proficiencies-demonstrated",
    "href": "ch4_linear_regression_polished.html#technical-proficiencies-demonstrated",
    "title": "ch4 Linear Regression Polished",
    "section": "",
    "text": "✅ Bayesian Model Specification: Prior selection, likelihood specification, posterior inference\n✅ MCMC Implementation: Stan/brms workflow, convergence diagnostics, effective sampling\n✅ Model Comparison: Information criteria, cross-validation, posterior predictive checking\n✅ Advanced Regression: Polynomial terms, splines, hierarchical structures\n✅ Visualization: Publication-quality plots with uncertainty visualization\n✅ Reproducible Research: Complete code documentation, version control ready\n\nThis analysis demonstrates practical expertise in modern Bayesian data analysis workflows suitable for research, industry, and consulting applications."
  },
  {
    "objectID": "forkingData.html",
    "href": "forkingData.html",
    "title": "Garden of Forking Data",
    "section": "",
    "text": "This Quarto document is made interactive using Observable JS. Interactive documents allow readers to modify parameters and see the results immediately. Learn more about OJS interactive documents at https://quarto.org/docs/interactive/ojs/."
  },
  {
    "objectID": "forkingData.html#observable-js",
    "href": "forkingData.html#observable-js",
    "title": "Garden of Forking Data",
    "section": "",
    "text": "This Quarto document is made interactive using Observable JS. Interactive documents allow readers to modify parameters and see the results immediately. Learn more about OJS interactive documents at https://quarto.org/docs/interactive/ojs/."
  },
  {
    "objectID": "forkingData.html#bubble-chart",
    "href": "forkingData.html#bubble-chart",
    "title": "Garden of Forking Data",
    "section": "Bubble Chart",
    "text": "Bubble Chart\nThis example uses a D3 bubble chart imported from Observable HQ to analyze commits to GitHub repositories.\nSelect a repository to analyze the commits of:\n\nlibrary(pdftools)\n\nUsing poppler version 23.04.0\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Extract text\ntext &lt;- pdf_text(\"~/Documents/Intellectual Fun/genericStats/p_hacking.pdf\")\n\n# Convert to tibble for manipulation\npdf_data &lt;- tibble(\n  page = seq_along(text),\n  content = text\n) %&gt;%\n  # Clean up common issues\n  mutate(\n    content = str_replace_all(content, \"\\\\s+\", \" \"),\n    content = str_trim(content)\n  )\n\nwriteLines(pdf_data$content, \"~/Documents/Intellectual Fun/genericStats/p_hacking.txt\")\n\nwrite_file(paste(pdf_data$content, collapse = \"\\n\\n\"), \"~/Documents/Intellectual Fun/genericStats/p_hacking.txt\")\n\n\nviewof repo = Inputs.radio(\n  [\n    \"pandas-dev/pandas\",\n    \"tidyverse/ggplot2\",\n  ], \n  { label: \"Repository:\", value: \"pandas-dev/pandas\"}\n)\n\n\n\n\n\n\nFetch the commits for the specified repo using the GitHub API:\n\nd3 = require('d3')\ncontributors = await d3.json(\n  \"https://api.github.com/repos/\" + repo + \"/stats/contributors\"\n)\ncommits = contributors.map(contributor =&gt; {\n  const author = contributor.author;\n  return {\n    name: author.login,\n    title: author.login,\n    group: author.type,\n    value: contributor.total\n  }\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the repo variable is bound dynamically from the radio input defined above. If you change the input the contributors query will be automatically re-executed.\nView the commits sorted by most to least:\n\nInputs.table(commits, { sort: \"value\", reverse: true })\n\n\n\n\n\n\nVisualize using a D3 bubble chart imported from Observable HQ:\n\nimport { chart } with { commits as data } \n  from \"@d3/d3-bubble-chart\"\nchart"
  },
  {
    "objectID": "ch4_linear_regression.html",
    "href": "ch4_linear_regression.html",
    "title": "ch4",
    "section": "",
    "text": "!khun"
  },
  {
    "objectID": "ch4_linear_regression.html#first-model",
    "href": "ch4_linear_regression.html#first-model",
    "title": "ch4",
    "section": "",
    "text": "!khun"
  },
  {
    "objectID": "ch4_linear_regression.html#eda-kung-height-weight-gender",
    "href": "ch4_linear_regression.html#eda-kung-height-weight-gender",
    "title": "ch4",
    "section": "EDA !Kung Height ~ Weight + Gender",
    "text": "EDA !Kung Height ~ Weight + Gender\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting the Prior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrid approximation technique\n\nn &lt;- 200\n\nd_grid &lt;-\n  # we'll accomplish with `tidyr::crossing()` what McElreath did with base R `expand.grid()`\n  crossing(mu    = seq(from = 130, to = 180, length.out = n),\n           sigma = seq(from = 0,   to = 15,   length.out = n))\n\n\ngrid_function &lt;- function(mu, sigma) {\n  dnorm(kHeight_adult$height, mean = mu, sd = sigma, log = T) %&gt;% \n    sum()\n}\n\nd_grid &lt;-\n  d_grid %&gt;% \n  mutate(log_likelihood = map2(mu, sigma, grid_function)) %&gt;%\n  unnest(log_likelihood) %&gt;% \n  mutate(prior_mu    = dnorm(mu,    mean = 150, sd  = 10, log = T),\n         prior_sigma = dunif(sigma, min  = 0,   max = 10, log = T),\n         product = log_likelihood + prior_mu + prior_sigma,\n         probability = exp(product- max(product)),\n         prior_product = prior_mu + prior_sigma,\n         prior_probability = exp(prior_product - max(prior_product)))\n\nprior_grid &lt;- \n  tibble(prior_mu = log(rnorm(1e5, mean = 150, sd = 10)),\n         prior_sigma = log(runif(1e5, min = 0, max = 10))) %&gt;% \n  mutate(product = prior_mu + prior_sigma,\n         probability = exp(product - max(product)))\n\nprior_plot &lt;- ggplot(data = d_grid) +\n  geom_point(aes(x = mu, y = sigma, color = prior_probability), alpha = .1) +\n  theme(panel.grid = element_blank()) +\n  scale_color_viridis_c(name = \"Prior Probability\", option = \"A\") +\n  labs(title = \"Prior\")\n  \nposterior_plot &lt;- ggplot(data = d_grid) +\n  geom_point(aes(x = mu, y = sigma, color = probability), alpha = .5) +\n   theme(panel.grid = element_blank()) +\n  scale_color_viridis_c(name = \"Posterior Probability\",  option = \"A\") +\n  labs(title = \"Posterior\")\n\nprior_plot + posterior_plot + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\nggplot(d_grid) +\n  geom_point(aes(x = mu, y = sigma, color = \"Prior\", alpha = prior_probability)) +\n  geom_point(aes(x = mu, y = sigma, color = \"Posterior\", alpha = probability)) +\n  scale_color_manual(values = c(\"Prior\" = \"blue\", \"Posterior\" = \"red\")) +\n  scale_alpha(name = \"Normalized Likelihood\", range = c(0, 0.6)) +\n  labs(title = \"Prior vs Posterior\", color = \"Distribution\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSample from the Posterior\n\nd_grid_samples &lt;- \n  d_grid %&gt;% \n  sample_n(size = 1e4, replace = T, weight = probability)\n\nd_grid_samples %&gt;% \n  ggplot(aes(x = mu, y = sigma)) + \n  geom_point(size = 0.9, alpha = 1/15) +\n  scale_fill_viridis_c() +\n  labs(x = expression(mu[samples]),\n       y = expression(sigma[samples])) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n\nmodel intercept only\n\nb4.1 &lt;- \n  brm(data = kHeight_adult, \n      family = gaussian,\n      height ~ 1,\n      prior = c(prior(normal(178, 20), class = Intercept),\n                prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      backend = \"cmdstanr\", seed = 4, file = \"fits/b04.011\")\n\nplot(b4.1) \n\nLoading required package: rstan\n\n\nLoading required package: StanHeaders\n\n\n\nrstan version 2.32.7 (Stan version 2.32.2)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n\n\n\nAttaching package: 'rstan'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\nThe following objects are masked from 'package:posterior':\n\n    ess_bulk, ess_tail\n\n\n\n\n\n\n\n\n\n\nExtremely Narrow Priors\n\nb4.2 &lt;- \n  brm(data = kHeight_adult, family = gaussian,\n      height ~ 1,\n      prior = c(prior(normal(150, 2), class = Intercept),\n                prior(uniform(0, 6), class = sigma, lb = 0, ub = 6)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/b04.02.6\")\n\nplot(b4.2)\n\n\n\n\n\n\n\nb4.2_draws &lt;- as_draws_df(b4.2)\n\n\nb4.2_draws &lt;- b4.2_draws %&gt;% \n  mutate(prior_mu = rnorm(n(), 150, 2),\n         prior_sigma = runif(n(), 0, 6))\n\nggplot(b4.2_draws, ) +\n  geom_point(aes(x = b_Intercept, y = sigma), alpha = 0.05, color = \"blue\") +\n  geom_density_2d(aes(x = b_Intercept, y = sigma), color = \"blue\", bins = 5) +\n  geom_point(aes(x = prior_mu, y = prior_sigma), alpha = 0.05, color = \"red\") +\n  geom_density_2d(aes(x = prior_mu, y = prior_sigma), color = \"red\", bins = 5) +\n  labs(x = expression(mu), y = expression(sigma),\n       title = \"Posterior Contours of mu and sigma\") +\n  theme_minimal()"
  },
  {
    "objectID": "ch4_linear_regression.html#adding-linear-predictor",
    "href": "ch4_linear_regression.html#adding-linear-predictor",
    "title": "ch4",
    "section": "Adding Linear Predictor",
    "text": "Adding Linear Predictor\n\nkHeight_adult &lt;- \n  kHeight_adult %&gt;%\n  mutate(weight_c = weight - mean(weight))\n\nb4.3 &lt;- \n  brm(data = kHeight_adult, \n      family = gaussian,\n      height ~ 1 + weight_c,\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/b04.03.2\")\n\nas_draws_df(b4.3) %&gt;%\n  select(b_Intercept:sigma) %&gt;%\n  cor() %&gt;%\n  round(digits = 2)\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n            b_Intercept b_weight_c sigma\nb_Intercept        1.00      -0.04  0.02\nb_weight_c        -0.04       1.00  0.01\nsigma              0.02       0.01  1.00\n\npairs(b4.3)\n\n\n\n\n\n\n\n\n\nmu &lt;- fitted(b4.3, summary = F)\n\n# new data\nweight_seq &lt;- tibble(weight_c = seq(from = -18, to = 18, by = 1))\n\nmu &lt;-\n  fitted(b4.3,\n         summary = F,\n         newdata = weight_seq) %&gt;%\n  as_tibble() %&gt;%\n  # here we name the columns after the `weight` values from which they were computed\n  set_names(-18:18) %&gt;% \n  mutate(iter = 1:n())\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\nmu &lt;-  mu %&gt;%\n  gather(weight, height, -iter) %&gt;% \n  # we might reformat `weight` to numerals\n  mutate(weight = as.numeric(weight))\n\nggplot()+\n  geom_point(data = mu, aes(x = weight, y = height), alpha = .002, color = \"blue\")+\n  geom_point(data = kHeight_adult, aes(x = weight_c, y = height))\n\n\n\n\n\n\n\n\n\nplot prediction error\n\npred_height &lt;-\n  predict(b4.3,\n          newdata = weight_seq) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(weight_seq)\n  \npred_height %&gt;%\n  slice(1:6)\n\n# A tibble: 6 × 5\n  Estimate Est.Error  Q2.5 Q97.5 weight_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1     138.      5.20  128.  148.      -18\n2     139.      5.04  129.  149.      -17\n3     140.      5.22  130.  150.      -16\n4     141.      5.15  130.  151.      -15\n5     142.      5.26  132.  152.      -14\n6     143.      5.06  133.  153.      -13"
  },
  {
    "objectID": "ch4_linear_regression.html#model-and-plot-quadratic-kung-height",
    "href": "ch4_linear_regression.html#model-and-plot-quadratic-kung-height",
    "title": "ch4",
    "section": "model and plot Quadratic !Kung height",
    "text": "model and plot Quadratic !Kung height\n\nkHeight &lt;-\n  kHeight %&gt;%\n  mutate(weight_s = (weight - mean(weight)) / sd(weight))\n\nb4.5 &lt;- \n  brm(data = kHeight, \n      family = gaussian,\n      height ~ 1 + weight_s + I(weight_s^2),\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      backend = \"cmdstanr\", silent = 2, seed = 4,\n      file = \"fits/b04.05.1.1\")\n\nweight_seq &lt;- tibble(weight_s = seq(from = min(kHeight$weight_s) - (0.5 * sd(kHeight$weight_s)),\n                                    to = max(kHeight$weight_s) + (0.5 * sd(kHeight$weight_s)), \n                                    length.out = 30))\n\nf &lt;-\n  fitted(b4.5, \n         newdata = weight_seq) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(weight_seq)\n\np &lt;-\n  predict(b4.5, \n          newdata = weight_seq) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(weight_seq) \n\nggplot(data = kHeight, \n       aes(x = weight_s)) +\n  geom_ribbon(data = p, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = f,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  geom_point(aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 1/3) +\n  coord_cartesian(xlim = range(kHeight$weight_s)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\npolynomial (Cubic) model !Kung height\n\nb4.6 &lt;- \n  brm(data = kHeight, \n      family = gaussian,\n      height ~ 1 + weight_s + I(weight_s^2) + I(weight_s^3),\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      backend = \"cmdstanr\", silent = 2,\n      seed = 4,\n      file = \"fits/b04.06\")\n\n\n# can't remember why I fit this model, I don't remember it being in the book\nb4.7 &lt;- \n  brm(data = kHeight, \n      family = gaussian,\n      height ~ 1 + weight_s,\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      backend = \"cmdstanr\", silent = 2,\n      seed = 4,\n      file = \"fits/b04.07\")\n\nf &lt;-\n  fitted(b4.6, \n         newdata = weight_seq) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(weight_seq)\n\np &lt;-\n  predict(b4.6, \n          newdata = weight_seq) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(weight_seq) \n\n\n\nplot Polynomial (Cubic) !Kung height\n\nat &lt;- c(-2, -1, 0, 1, 2)\nggplot(data = kHeight, \n       aes(x = weight_s)) +\n  geom_ribbon(data = p, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = f,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/4) +\n  geom_point(aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 1/3) +\n  coord_cartesian(xlim = range(kHeight$weight_s)) +\n  theme_minimal()+\n  \n  # here it is!\n  scale_x_continuous(\"standardized weight converted back\",\n                     breaks = at,\n                     labels = round(at * sd(kHeight$weight) + mean(kHeight$weight), 1))"
  },
  {
    "objectID": "ch4_linear_regression.html#splines",
    "href": "ch4_linear_regression.html#splines",
    "title": "ch4",
    "section": "Splines!",
    "text": "Splines!\n\nEDA cherry blossom data\n\nlibrary(rethinking)\n\nrethinking (Version 2.42)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following objects are masked from 'package:rstan':\n\n    stan, traceplot\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n\nThe following objects are masked from 'package:brms':\n\n    LOO, stancode, WAIC\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\nggplot(data = cherry, aes(x = year, y = temp))+geom_line()+labs(title = \"Cherry Blossom temperature in March)\")\n\nWarning: Removed 73 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\nSpline Model Cherry Blossom\n\ncherry2 &lt;- cherry %&gt;% \n  filter(!is.na(temp))\n\nnum_knots &lt;- 15\nknot_list &lt;- quantile( cherry2$year, probs=seq(0,1,length.out=num_knots) )\n\n\nB &lt;- bs(cherry2$year,\n    knots=knot_list[-c(1,num_knots)] ,\n    degree=4 , intercept=TRUE )\n\nB_tib &lt;- as_tibble(B) \n\n\nB_tib_join &lt;- cbind(cherry2, B_tib)\n\nB_tib_join_long &lt;- B_tib_join %&gt;% \n  pivot_longer(cols = c(`1`, `2`,`3`, `4`,`5`, `6`,`7`, `8`,`9`, `10`,`11`, `12`,`13`, `14`, `15`, `16`, `17`),\n               names_to = \"knot\",\n               values_to = \"density\")\n\nggplot(data = B_tib_join_long, aes(x = year, y = (density),  color = as.factor(knot)))+geom_line()\n\nDon't know how to automatically pick scale for object of type\n&lt;bs/basis/matrix&gt;. Defaulting to continuous.\n\n\n\n\n\n\n\n\nb4_smooth &lt;- brm(\n  data = cherry2,\n  family = gaussian,\n  formula = temp ~ 1 + s(year, bs = \"bs\", k = 30),  # k sets number of basis functions\n  prior = c(\n    prior(normal(6, 10), class = Intercept),\n    prior(normal(0, 1), class = b),\n    prior(student_t(3, 0, 1), class = sds),      # Prior for smooth term\n    prior(exponential(1), class = sigma)\n  ),\n  backend = \"cmdstanr\", silent = 2,\n  iter = 2000,\n  warmup = 1000,\n  chains = 4,\n  cores = 4,\n  seed = 42,\n  control = list(adapt_delta = 0.99),\n  file = \"fits/b04.cherry_bspline\"\n)\n\nb4.1_smooth &lt;- brm(\n  data = cherry2,\n  family = gaussian,\n  formula = temp ~ 1 + s(year, bs = \"tp\"),  # k sets number of basis functions\n  prior = c(\n    prior(normal(6, 10), class = Intercept),\n    prior(normal(0, 1), class = b),\n    prior(student_t(3, 0, 1), class = sds),      # Prior for smooth term\n    prior(exponential(1), class = sigma)\n  ),\n  backend = \"cmdstanr\", silent = 2,\n  iter = 2000,\n  warmup = 1000,\n  chains = 4,\n  cores = 4,\n  seed = 42,\n  control = list(adapt_delta = 0.99),\n  file = \"fits/b04.cherry_thinSpline\"\n)\n\n\nyear_seq &lt;- tibble(year = seq(from = 800, to = 2000, by = 10))\n\nmu_temp_4 &lt;-\n  fitted(b4_smooth, \n         newdata = year_seq) %&gt;%\n  as_tibble() %&gt;%\n  # let's tack on the `weight` values from `weight_seq`\n  bind_cols(year_seq)\n\npred_temp_4 &lt;-\n  predict(b4_smooth,\n          newdata = year_seq) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(year_seq)\n  \n# pred_temp_4 %&gt;%\n#   slice(1:6)\n\n\nmu_temp_4.1 &lt;-\n  fitted(b4.1_smooth, \n         newdata = year_seq) %&gt;%\n  as_tibble() %&gt;%\n  # let's tack on the `weight` values from `weight_seq`\n  bind_cols(year_seq)\n\npred_temp_4.1 &lt;-\n  predict(b4.1_smooth,\n          newdata = year_seq) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(year_seq)\n  \n# pred_temp_4.1 %&gt;%\n#   slice(1:6)\n\n\n\nPlots Cherry Blossom\n\ncherry2 %&gt;%\n  ggplot(aes(x = year)) +\n  geom_ribbon(data = pred_temp_4, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = mu_temp_4,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  geom_point(aes(y = temp),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  ylab(\"temp\") +\n  labs(title = \"15 knot B-splines\")\n\n\n\n\n\n\n\ncherry2 %&gt;%\n  ggplot(aes(x = year)) +\n  geom_ribbon(data = pred_temp_4.1, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = mu_temp_4.1,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  geom_line(aes(y = temp),\n             color = \"navyblue\", shape = 1) +\n  ylab(\"temp\") +\n  labs(title = \"Thin Plated Spline\")\n\nWarning in geom_line(aes(y = temp), color = \"navyblue\", shape = 1): Ignoring\nunknown parameters: `shape`\n\n\n\n\n\n\n\n\n# I am confused as to why the the Thin plated splines don't go more crazy and try to fit all of the contours"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Rethinking Homepage",
    "section": "",
    "text": "Welcome\nThis site showcases projects from my Statistical Rethinking work in R and Quarto.\n\n📈 Linear Regression\n\n📈 Multi - Linear Regression\n\n🔀 Forking Paths\n\nSource code is on GitHub."
  },
  {
    "objectID": "ch5_multilinear.html",
    "href": "ch5_multilinear.html",
    "title": "Chapter 5 Multi-variable Regression DAG’s and Causality",
    "section": "",
    "text": "[claude] come up with 3-4 bullet points on what was accomplished in this chapter."
  },
  {
    "objectID": "ch5_multilinear.html#overview",
    "href": "ch5_multilinear.html#overview",
    "title": "Chapter 5 Multi-variable Regression DAG’s and Causality",
    "section": "",
    "text": "[claude] come up with 3-4 bullet points on what was accomplished in this chapter."
  },
  {
    "objectID": "ch5_multilinear.html#motivation",
    "href": "ch5_multilinear.html#motivation",
    "title": "Chapter 5 Multi-variable Regression DAG’s and Causality",
    "section": "Motivation",
    "text": "Motivation\n[claude] come up with a distrilled paragraph argument for why we should care about muli-variate regression. Here’s a section of Richard McElreath’s text statistical Rethinking\n\nStatistical “control” for confounds. A confound is something that misleads us about a causal influence—there will be a more precise definition in the next chapter. The spurious waffles and divorce correlation is one possible type of confound, where the confound (southernness) makes a variable with no real importance (Waffle House density) appear to be important. But confounds are diverse. They can hide real important variables just as easily as they can produce false ones.\nMultiple causation. A phenomenon may arise from multiple causes. Measurement of each cause is useful, so when we can use the same data to estimate more than one type of influence, we should. Furthermore, when causation is multiple, one cause can hide another.\nInteractions. The importance of one variable may depend upon another. For ex- ample, plants benefit from both light and water. But in the absence of either, the other is no benefit at all. Such interactions occur very often. Effective inference about one variable will often depend upon consideration of others.\n\nClaude give a quick description of the waffle divorce data set : Data for the individual States of the United States, describing number of Waffle House diners and various marriage and demographic facts.\nFormat Location : State name Loc : State abbreviation Population : 2010 population in millions MedianAgeMarriage: 2005-2010 median age at marriage Marriage : 2009 marriage rate per 1000 adults Marriage.SE : Standard error of rate Divorce : 2009 divorce rate per 1000 adults Divorce.SE : Standard error of rate WaffleHouses : Number of diners South : 1 indicates Southern State Slaves1860 : Number of slaves in 1860 census Population1860 : Population from 1860 census PropSlaves1860 : Proportion of total population that were slaves in 1860\n\n\n\n\n\n\n\n\n\nMost likely a spurious correlation, waffles don’t cause divorces nor vice versa"
  },
  {
    "objectID": "ch5_multilinear.html#unraveling-dags",
    "href": "ch5_multilinear.html#unraveling-dags",
    "title": "Chapter 5 Multi-variable Regression DAG’s and Causality",
    "section": "Unraveling DAGs",
    "text": "Unraveling DAGs\nExcerpt from Richard McElreath: “Let’s leave waffles behind, at least for the moment. An example that is easier to understand is the correlation between divorce rate and marriage rate (Figure 5.2). The rate at which adults marry is a great predictor of divorce rate, as seen in the left-hand plot in the figure. But does marriage cause divorce? In a trivial sense it obviously does: One cannot get a divorce without first getting married. But there’s no reason high marriage rate must be correlated with divorce.”\n\n\n\n\n\n\n\n\n\nWe see that Divorce rate is high in the south and low in the north midwest and mid atlantic. Marriage rate is high in\n\n\n\n\n\n\n\n\n\n\nMedian Marriage Age Model\n\\[ \\textbf{b5.1} \\]\n\\[ \\text{Divorce}_i \\sim \\text{Intercept} + \\beta_1 \\times \\text{Median Age Marriage}_i \\]\n\n📈 μ vs full distribution📊 Summary⚙️ Code🌌 Prior Vs Posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Divorce ~ 1 + MedianAgeMarriage_s \n   Data: WaffleDivorce (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept               9.69      0.22     9.26    10.12 1.00     4663     4042\nMedianAgeMarriage_s    -1.04      0.21    -1.46    -0.62 1.00     5599     3980\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.51      0.16     1.24     1.85 1.00     5275     4263\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\nb5.1 &lt;- \n  brm(data = WaffleDivorce, \n      family = gaussian,\n      Divorce ~ 1 + MedianAgeMarriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma, ub = 10)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/b05.01\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe regression of Divorce Rate on Age of Marriage, tells us only that the total influence of age at marriage is strongly negative with divorce rate.\n\n\nMarriage Rate\n\\[ \\textbf{b5.2} \\]\n\\[ \\text{Divorce}_i \\sim \\text{Intercept} + \\beta_1 \\times \\text{Marriage % Rate}_i \\]\n\n📈 μ vs full distribution📊 Summary⚙️ Code🌌 Prior Vs Posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Divorce ~ 1 + Marriage_s \n   Data: WaffleDivorce (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      9.69      0.25     9.21    10.18 1.00     5774     4384\nMarriage_s     0.64      0.24     0.15     1.11 1.00     5742     4259\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.75      0.18     1.44     2.15 1.00     5708     4040\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\nb5.2  &lt;- \n  brm(data = WaffleDivorce, \n      family = gaussian,\n      Divorce ~ 1 + Marriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma, ub = 10)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/b05.02\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCombined Multi-linear model\n\\[ \\textbf{b5.3} \\]\n\\[ \\text{Divorce}_i \\sim \\text{Intercept} + \\beta_1 \\times \\text{Marriage % Rate}_i + \\beta_2 \\times \\text{Median Marriage Age}_i \\]\n\n📊 Summary⚙️ Code\n\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s \n   Data: WaffleDivorce (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept               9.68      0.22     9.25    10.11 1.00     6254     4493\nMarriage_s             -0.12      0.30    -0.71     0.46 1.00     3884     3601\nMedianAgeMarriage_s    -1.12      0.30    -1.71    -0.53 1.00     3726     3644\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.53      0.17     1.24     1.91 1.00     4836     3480\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\nb5.3 &lt;- brm(data = WaffleDivorce, \n      family = gaussian,\n      Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma, ub = 10)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\", silent = 2,\n      file = \"fits/b05.031\")\n\n\n\n\n\n\nSolving the DAG\nFrom McElreath, “Age of marriage influences divorce in two ways. First it can have a direct effect, perhaps because younger people change faster than older people and are therefore more likely to grow incompatible with a partner. Second, it can have an indirect effect by influencing the marriage rate. If people get married earlier, then the marriage rate may rise, because there are more young people. Consider for example if an evil dictator forced everyone to marry at age 65. Since a smaller fraction of the population lives to 65 than to 25, forcing delayed marriage will also reduce the marriage rate. If marriage rate itself has any direct effect on divorce, maybe by making marriage more or less normative, then some of that direct effect could be the indirect effect of age at marriage.”\n\n\n\n\n\n\n\n\n\nOnce we know median age at marriage for a State, there is little or no additional predictive power in also knowing the rate of marriage in that State.\n\nResidualsResidual MapWaffle House Effect"
  },
  {
    "objectID": "ch5_multilinear.html#masked-relationships",
    "href": "ch5_multilinear.html#masked-relationships",
    "title": "Chapter 5 Multi-variable Regression DAG’s and Causality",
    "section": "Masked Relationships",
    "text": "Masked Relationships\nNew data set Comparative primate milk composition data, from Table 2 of Hinde and Milligan. 2011. Evolutionary Anthropology 20:9-23.\nclade: Broad taxonomic group\nspecies: Species name\nkcal.per.g: Kilocalories per gram of milk\nperc.fat: Percent fat\nperc.protein: Percent protein\nperc.lactose: Percent lactose\nmass: Body mass of mother, in kilograms\nneocortex.perc: Percent of brain mass that is neocortex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Milk"
  }
]