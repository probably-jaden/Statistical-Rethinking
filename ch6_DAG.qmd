---
editor: visual
execute:
  echo: false
  warning: false
  message: false
  cache: true
  cache.lazy: false
---

## Causal Terror Intro
```{r setup}
library(tidyverse)
library(brms)
library(bayesplot)
library(tidybayes)
library(patchwork)
library(GGally)
library(dagitty)
library(ggdag)

library(rethinking)
data(milk)
```


"The more likely it is to kill you, if true, the less likely it is to be true. The more boring the topic, the more rigorous the results. How could this widely believed negative correlation exist? There doesn’t seem to be any reason for studies of topics that people care about to produce less reliable results. Maybe popular topics attract more and worse researchers, like flies drawn to the smell of honey? Actually all that is necessary for such a negative correlation to arise is that peer reviewers care about both newsworthiness and trustworthiness."


```{r}
n <- 300
data <- tibble(
  x = rnorm(n, mean = 0, sd = 1),
  y = rnorm(n, mean = 0, sd = 1)
)

# Define diagonal line: y = -x + c
# For a line cutting through top right of the circle, we'll use c = 1.5
line_intercept <- 1.5

# Classify points: above and to the right means y > -x + c
data <- data %>%
  mutate(above_line = y > (-x + line_intercept))

# Create the plot
ggplot(data, aes(x = x, y = y)) +
  geom_point(aes(color = above_line), alpha = 0.7, size = 2) +
  geom_abline(slope = -1, intercept = line_intercept, 
              color = "blue4", size = 1, linetype = "solid") +
  scale_color_manual(values = c("FALSE" = "gray50", "TRUE" = "blue"),
                     name = "Publication Status",
                     labels = c("Unpublished", "Published")) +
  labs(title = "Selection Distortion Effects of Publishing Papers",
       subtitle = "Peer reviewers create a negative correlation ",
       x = "Trustworthiness",
       y = "Newsworthiness") +
  theme_minimal() +
  coord_fixed() # Keeps aspect ratio 1:1 to show true circle shape
```

Multiple regression is plagued with "paradoxes" that happen when we condition (like conditioning on publication status) on some variables in our regression. In this chapter we'll go over three problems with regressing on additional variables: multicollinearity, post-treatment bias, and collider bias.

## Multicollinearity

Multicollinearity means very strong correlation between two or more predictor variables. The consequence of it is that the posterior distribution will seem to suggest that none of the variables is reliably associated with the outcome, even if all of the variables are in reality strongly associated with the outcome. In fact, there is nothing wrong with multicollinearity. The model will work fine for prediction, it just frustrates most peoples intuitive understanding of what multi regression does.

### Left leg predicts height but right leg doesn't?

```{r}
#| fig-width: 8
#| fig-height: 4

n = 300
height_sim = tibble(height = rnorm(n, 170, 6.5)) %>% 
  mutate(below_hip_height = height * rnorm(n, .45, .02),
         left_leg = below_hip_height * rnorm(n, 1, .001),
         right_leg = below_hip_height * rnorm(n, 1, .001),
         left_leg_z = (left_leg - mean(left_leg))/sd(left_leg),
         right_leg_z = (right_leg - mean(right_leg))/sd(right_leg),)

leftLeg_plot <- ggplot(data = height_sim)+
  geom_point(aes(x= left_leg, y = height), alpha = 1, size = 1)+
  theme_minimal()+
  labs(x = "Left Leg Height (cm)", y = "Height (cm)", title = "Left Leg vs Height")

rightLeg_plot <- ggplot(data = height_sim)+
  geom_point(aes(x= right_leg, y = height), alpha = 1, size = 1) +
  theme_minimal()+
  labs(x = "Right Leg Height (cm)", y = "Height (cm)", title = "Right Leg vs Height")

RightLeft_plot <- ggplot(data = height_sim)+
  geom_point(aes(x= right_leg, y = left_leg), alpha = 1, size = 1)+
  theme_minimal()+
  labs(x = "Right Leg Height (cm)", y = "Left Leg Height (cm)", title = "Right Leg vs Left Leg")

leftLeg_plot + rightLeg_plot + RightLeft_plot
```


$$\text{Height}_i = \text{Normal}(\mu_i, \sigma)$$

$$\mu_i = \alpha + \beta_\text{1}\text{Left Leg Z}_i +  \beta_\text{2}\text{Right Leg Z}_i$$
$$\alpha = \text{Normal}(170, 10)$$

$$\beta_i = \text{Normal}(0, 3)$$
$$\sigma = \text{Exponential}(3)$$



```{r}
#| fig-width: 8
#| fig-height: 2


b6.0 <- brm(data = height_sim,
    family = gaussian,
    height ~ 1 + left_leg_z + right_leg_z,
    prior = c(
      prior(normal(170, 10), class = Intercept),
      prior(normal(0, 10), class = b),
      prior(exponential(5), class = sigma)
    ),
    seed = 5, iter = 2000, warmup = 500, cores = 4,
    backend = "cmdstanr", silent = 2, file = "fits/b06D")


as_tibble(b6.0) %>% 
  rename("Left Leg" = b_left_leg_z,
         "Right Leg" = b_right_leg_z) %>% 
  dplyr::select(c(`Left Leg`, `Right Leg`)) %>% 
  pivot_longer(cols = everything(),
               names_to = "Leg",
               values_to = "Height Effect") %>% 
  ggplot(aes(x = `Height Effect`, y = reorder(Leg, `Height Effect`))) +
  stat_halfeye(point_interval = median_qi, .width = .95,
               fill = "firebrick4") +
  labs(x = "Height Effect",
       y = NULL) +
  theme_bw() +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        panel.grid = element_blank())


bLeft_vs_bRight_plot <- as_tibble(b6.0) %>% 
  rename("Left Leg" = b_left_leg_z,
         "Right Leg" = b_right_leg_z) %>% 
  dplyr::select(c(`Left Leg`, `Right Leg`)) %>% 
  ggplot()+
  geom_point(aes(x = `Right Leg`, y = `Left Leg`), size = .1, alpha = .1)+
  theme_minimal()+
  labs(title = "Coefficients Correlation")

bLeft_plus_bRight_plot <- as_tibble(b6.0) %>% 
  mutate(legSum = b_left_leg_z + b_right_leg_z) %>% 
  dplyr::select(c(legSum)) %>% 
  ggplot()+
  geom_density(aes(x = legSum), lwd = 1, adjust = 2, color = "purple3")+
  theme_minimal()+
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank())+
  labs(x = "Left Leg Z + Right Leg Z Effect", title = "Coefficients Combined Effect")

bLeft_vs_bRight_plot + bLeft_plus_bRight_plot
```
The posterior distribution for these two parameters is very highly correlated, with all of the plausible values of $\beta_\text{left}$ and $\beta_\text{right}$ lying along a narrow ridge.

One way to think of this phenomenon is that you have approximated this model:

$$ y_i \sim \text{Normal}(\mu_i, \sigma) $$

$$ \mu_i = \alpha + \beta_1x_i + \beta_2x_i $$ 

The variable y is the outcome, like height in the example, and x is a single predictor, like the leg lengths in the example. Here x is used twice, which is a perfect example of the problem caused by using the almost-identical leg lengths. From the computer’s perspective, this model is simply:

$$ \mu_i = \alpha + (\beta_1 + \beta_2)x_i $$ 


When two predictor variables are very strongly correlated, including both in a model may lead to confusion. The posterior distribution isn’t wrong, in such cases. It’s telling you that the question you asked cannot be answered with these data. And that’s a great thing for a model to say, that it cannot answer your question. And if you are just interested in prediction, you’ll find that this leg model makes fine predictions. It just doesn’t make any claims about which leg is more important.


### Milk Example

Using the milk primate data from the last chapter

```{r}
milk <- milk %>% 
  mutate(kcal_z = (kcal.per.g - mean(kcal.per.g))/sd(kcal.per.g),
         fat_z = (perc.fat - mean(perc.fat))/sd(perc.fat),
         lactose_z = (perc.lactose - mean(perc.lactose))/sd(perc.lactose))

b6.1 <- brm(data = milk,
            family = gaussian,
            kcal_z ~ 1 + fat_z,
            prior = c(
              prior(normal(0, 0.2), class = Intercept),
              prior(normal(0, 0.5), class = b),
              prior(exponential(1), class = sigma)
            ),
            iter = 2000, warmup = 500, cores = 4, seed = 5,
            backend = "cmdstanr", silent = 2, file = "fits/b06.1")

b6.1b <- brm(data = milk,
            family = gaussian,
            kcal_z ~ 1 + lactose_z,
            prior = c(
              prior(normal(0, 0.2), class = Intercept),
              prior(normal(0, 0.5), class = b),
              prior(exponential(1), class = sigma)
            ),
            iter = 2000, warmup = 500, cores = 4, seed = 5,
            backend = "cmdstanr", silent = 2, file = "fits/b06.1b")


posterior_summary(b6.1)[1:4, ] %>% round(digits = 3)
posterior_summary(b6.1b)[1:4, ] %>% round(digits = 3)
```


```{r}
#| fig-width: 8
#| fig-height: 4

ggpairs(data = milk, columns = c("kcal_z", "fat_z", "lactose_z"))+
  theme(panel.grid = element_blank())
```


The posterior distributions for $\beta_\text{fat}$ and $\beta_\text{lactose}$ are essentially mirror images of one another. The
posterior mean of $\beta_\text{fat}$ is as positive as the mean of $\beta_\text{lactose}$ is negative. Both are narrow posterior distributions that lie almost entirely on one side or the other of zero. Given the strong associ- ation of each predictor with the outcome, we might conclude that both variables are reliable predictors of total energy in milk, across species. The more fat, the more kilocalories in the milk. The more lactose, the fewer kilocalories in milk. But watch what happens when we place both predictor variables in the same regression model:

```{r}
b6.1c <- brm(data = milk,
             family = gaussian,
             kcal_z ~ 1 + fat_z + lactose_z,
             prior = c(
               prior(normal(0, 0.2), class = Intercept),
               prior(normal(0, 0.5), class = b),
               prior(exponential(1), class = sigma)
             ),
             iter = 2000, warmup = 500, cores = 4, seed = 5,
             backend = "cmdstanr", silent = 2, file = "fits/b06.1c")

posterior_summary(b6.1c)[1:4,] %>% round(digits = 3)
```
Now the posterior means of both $\beta_\text{fat}$ and $\beta_\text{lactose}$ are closer to zero. And the standard deviations for both parameters are twice as large as in the bivariate models.

What has happened is that the variables perc.fat and perc.lactose contain much of the same information.

In the scientific literature, you might encounter a variety of dodgy ways of coping with multicollinearity. Few of them take a causal perspective. Some fields actually teach students to inspect pairwise correlations before fitting a model, to identify and drop highly correlated predictors. This is a mistake. Pairwise correlations are not the problem. It is the conditional associations—not correlations—that matter. And even then, the right thing to do will depend upon what is causing the collinearity. The associations within the data alone are not enough to decide what to do.

What is likely going on in the milk example is that there is a core tradeoff in milk com- position that mammal mothers must obey. If a species nurses often, then the milk tends to be watery and low in energy. Such milk is high in sugar (lactose). If instead a species nurses rarely, in short bouts, then the milk needs to be higher in energy. Such milk is very high in fat. This implies a causal model something like this:

```{r}
#| fig-width: 8
#| fig-height: 2

dag_coords <-
  tibble(name = c("L", "D", "F", "K"),
         x    = c(1, 2, 3, 2),
         y    = c(2, 2, 2, 1))

m1 <- dagify("L" ~ "D",
             "F" ~ "D",
             "K" ~ "L" + "F",
       coords = dag_coords) %>%
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(color = "firebrick", alpha = 1/4, size = 10) +
  geom_dag_text(color = "firebrick") +
  geom_dag_edges(edge_color = "firebrick")+
  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = c(0.2, 0.2)) +
  theme_bw() +
  theme(panel.grid = element_blank())+
  labs(caption = "L = lactose, D = Milk Density, F = Fat, K = kiloCalories")

m1

```

The central tradeoff decides how dense, D, the milk needs to be. We haven’t observed this variable. Then fat, F, and lactose, L, are determined. Finally, the composition of F and L determines the kilocalories, K. If we could measure D, or had an evolutionary and economic model to predict it based upon other aspects of a species, that would be better than stumbling through regressions. We’d just regress K on D, ignoring the mediating L and F, to estimate the causal influence of density on energy.

Nature does not owe us easy inference, even when the model is correct.


## Post-treatment bias

We often omit variables when we shouldn't less rare is the mistaken inferences arisen from including variables, we call this post-treatment bias. 

The language “post-treatment” comes in fact from thinking about experimental designs. Suppose for example that you are growing some plants in a greenhouse. You want to know the difference in growth under different anti-fungal soil treatments, because fungus on the plants tends to reduce their growth. Plants are initially seeded and sprout. Their heights are measured. Then different soil treatments are applied. Final measures are the height of the plant and the presence of fungus. There are four variables of interest here: initial height, final height, treatment, and presence of fungus. Final height is the outcome of interest. But which of the other variables should be in the model? If your goal is to make a causal inference about the treatment, you shouldn’t include the presence of fungus, because it is a post-treatment effect.

```{r}
n <- 100 # number of plants
initial_height <- rnorm(n, 10, 2)
treatment <- rep(0:1, each = 100/2)
fungus <- rbinom(n, size =1, prob = 0.5 - (treatment * 0.4))

final_height = initial_height + rnorm(n, 5 - (3*fungus))

exp_data <- tibble(initial_height = initial_height,
       treatment = treatment,
       fungus = fungus,
       final_height = final_height)
```

It's easier to imagine the priors when we think of the final height as a porportion of their initial height



$$ \text{Final Height}_i \sim \text{Normal}(\mu_i, \sigma) $$

$$\mu_i = \text{Initial Height}_i \times p $$
If p = 1, the plant hasn’t changed at all from time t = 0 to time t = 1. If p = 2, it has doubled in height. So if we center our prior for p on 1, that implies an expectation of no change in height.

$$ p \sim \text{Log-Normal(0,  0.25)}$$

```{r, echo = TRUE}
b6.2 <- brm(
  data = exp_data,
  family = gaussian(),
  bf(final_height ~ initial_height * p,
     p ~ 1,  # p gets its own linear predictor
     nl = TRUE),  # nonlinear model
  prior = c(
    prior(lognormal(0, 0.25), nlpar = p, lb = 0),
    prior(exponential(1), class = sigma)
  ),
  chains = 4, iter = 2000, warmup = 500, seed = 5,
  backend = "cmdstanr", silent = 2, file = "fits/b0.6.2b")

posterior_summary(b6.2)[1:2,] %>% round(digits = 3)

```


Cool so about 40% growth on average.

### Adding Covariates

$$ \text{Final Height}_i \sim \text{Normal}(\mu_i, \sigma) $$

$$\mu_i = \text{Initial Height}_i \times p $$

$$ p = \alpha + \beta_T\text{Treatment}_i + \beta_F\text{Fungus}_i $$

$$ \alpha \sim  \text{Log-Normal(0, 0.25)} $$

$$ \beta_T \sim \text{Normal(0, 0.5)} $$
$$ \beta_F \sim \text{Normal(0, 0.5)} $$

$$ \sigma \sim \text{Exponential(1)} $$

The proportion of growth $p$ is now a function of the predictor variables

```{r}
b6.3 <- brm(data = exp_data,
            family = gaussian,
            bf(
              final_height ~ initial_height * p,
              p ~ 1 + treatment + fungus,
              nl = TRUE
            ),
            prior = c(
              prior(lognormal(0, 0.25), nlpar = p, class = b, coef = Intercept),
              prior(normal(0, 0.5), nlpar = p, class = b, coef = fungus),
              prior(normal(0, 0.5), nlpar = p, class = b, coef = treatment),
              prior(exponential(1), class = sigma)
            ),
            iter = 2000, warmup = 500, cores = 4, seed = 5,
            backend = "cmdstanr", silent = 2, file = "fits/b06.3")

# get_prior(bf(final_height ~ initial_height * p, p ~ treatment + fungus, nl = TRUE),
#           data = exp_data, family = gaussian())

posterior_summary(b6.3)[1:4,] %>% round(digits = 3)
```
The $\alpha$ parameter is the same as p before. And it has nearly the same posterior. The marginal posterior for $\beta_\text{Treatment}$, the effect of treatment, is solidly zero, with a tight interval. The problem is that fungus is mostly a consequence of treatment. To actually answer our research question of the effect of treatment we must omit the fungus variable.

like such:

$$ \text{Final Height}_i \sim \text{Normal}(\mu_i, \sigma) $$

$$\mu_i = \text{Initial Height}_i \times p $$

$$ p = \alpha + \beta_T\text{Treatment}_i$$

The rest of the model is the same as above

```{r}
b6.4 <- brm(data = exp_data,
            family = gaussian,
            bf(
              final_height ~ initial_height * p,
              p ~ 1 + treatment,
              nl = TRUE
            ),
            prior = c(
              prior(lognormal(0, 0.25), nlpar = p, class = b, coef = Intercept),
              prior(normal(0, 0.5), nlpar = p, class = b, coef = treatment),
              prior(exponential(1), class = sigma)
            ),
            iter = 2000, warmup = 500, cores = 4, seed = 5,
            backend = "cmdstanr", silent = 2, file = "fits/b06.4")

# get_prior(bf(final_height ~ initial_height * p, p ~ treatment + fungus, nl = TRUE),
#           data = exp_data, family = gaussian())

posterior_summary(b6.4)[1:3,] %>% round(digits = 3)
```

Here our treatment effect does solidly increase the height of the plant by 3% - 17%.

### Looking at the DAG

```{r}
#| fig-width: 8
#| fig-height: 2

dag_coords <-
  tibble(name = c("H0", "H1", "F", "T"),
         x    = c(1, 2, 3, 4),
         y    = c(2, 1, 2, 2))

m2 <- dagify("H1" ~ "H0 + F",
             "F" ~ "T",
       coords = dag_coords) %>%
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(color = "firebrick", alpha = 1/4, size = 10) +
  geom_dag_text( color = "firebrick", parse = TRUE) +
  geom_dag_edges(edge_color = "firebrick") +
  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = c(0.2, 0.2)) +
  theme_bw() +
  theme(panel.grid = element_blank())+
  labs(caption = "H0 = initial Height, H1 = final height, F = fungus, T = treatment")

m2

```

As the DAG helps us see once we control for fungus we are cutting off all of it's influences (AKA the treatments entire influence here). There is no information in T about H1 that is not also in F. 


## Collider Bias 

Going back to our intro on the the most newsworthy science papers being the less trustworthy we can represent the causal model like this.

```{r}
#| fig-width: 8
#| fig-height: 1

dag_coords <- tibble(name = c("T", "S", "N"),
                     x = c(1, 2, 3),
                     y = c(1, 1, 1))

m3 <- dagify("S" ~ "T" + "N",
             coords = dag_coords) %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend))+
  geom_dag_point(color = "firebrick", alpha = 1/4, size = 10)+
  geom_dag_text( color = "firebrick", parse = TRUE) +
  geom_dag_edges(edge_color = "firebrick") +
  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = c(0.2, 0.2)) +
  theme_bw() +
  theme(panel.grid = element_blank())+
  labs(caption = "T = Trustworthiness, N = Newsworthiness, S = Selection")

m3
```

This is a collider structure, trustworthiness and newsworthiness are statistically associated when looking at S but they are not causally associated. Once you learn that a paper has been selected (S), then learning its trustworthiness (T) also provides information about its newsworthiness (N). Otherwise it wouldn’t have been funded. Why? Because if, for example, a selected proposal has low trustworthiness, then it must have high newsworthiness.

We can make another collider causal model by thinking that both Happiness levels and Age cause one to get married.

```{r}
#| fig-width: 8
#| fig-height: 1

dag_coords <- tibble(name = c("H", "M", "A"),
                     x = c(1, 2, 3),
                     y = c(1, 1, 1),
                     observed = c(TRUE, TRUE, FALSE))  # A is unobserved

m4 <- dagify("M" ~ "H" + "A",
             coords = dag_coords) %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend))+
  geom_dag_point(color = "firebrick", alpha = 1/4, size = 10)+
  geom_dag_text( color = "firebrick", parse = TRUE) +
  geom_dag_edges(edge_color = "firebrick") +
  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = c(0.2, 0.2)) +
  theme_bw() +
  theme(panel.grid = element_blank())+
  labs(caption = "H = Happiness, M = Marritial Status, A = Age")


m4
```

Even though there is no causal association between happiness and age, if we condition on marriage (i.e. include it as a predictor in a regression) then we will have a statistical association between age and happiness. 


```{r}
#| fig-width: 10
#| fig-height: 4

sim_Marriages <- as_tibble(sim_happiness( seed=1977 , N_years=1000))


ggplot(data = sim_Marriages, aes(x = age, y = happiness, color = as.factor(married), fill = as.factor(married)))+
  scale_color_manual(values = c("black", "blue"), labels = c("Not Married", "Married"), name = "Marital Status")+
  scale_fill_manual(values = c("white", "blue"), labels = c("Not Married", "Married"), name = "Marital Status")+
  geom_point(shape = 21)+
  theme_minimal()
```

Consider only the blue points, the married people. Among only the blue points, older individuals have lower average happiness. This is because more people get marriage at time goes on, so the mean happiness among married people approaches the population average of zero. Now consider only the open points, the unmarried people. Here it is also true that mean happiness declines with age. This is because happier individuals migrate over time into the married sub-population. So in both the married and unmarried sub-populations, there is a negative relationship between age and happiness. But in neither sub-population does this accurately reflect causation.

If we came across this data and want to ask whether age is related to happiness. You don't know the true causal model. But you reason, that marriage status might be an important confound. If married people are more or less happy, on average, then you need to condition on marriage status in order to infer the relationship between age and happiness.

You're regression model would look like:

$$\mu_\text{Happiness for individual i} = \alpha_\text{Marriage Status Intercept} + \beta_AA_i$$

```{r}
#| fig-width: 10
#| fig-height: 2

sim_Marriages_clean <- sim_Marriages %>%
  filter(age >= 18) %>% 
  mutate(adult_age = (age - 18) / (65-18),
         single = -1 * (married - 1),
         maritialStatus = ifelse(married == 1, "married", "single"))


# get_prior(happiness ~ 0 + single + married + adult_age,
#           data = sim_Marriages_clean,
#           family = gaussian())


b6.5 <- brm(
  data = sim_Marriages_clean,
  family = gaussian(),
  happiness ~ 0 + single + married + adult_age,
  prior = c(
    prior(normal(0, 1), class = b, coef = married),
    prior(normal(0, 1), class = b, coef = single),
    prior(normal(0, 2), class = b, coef = adult_age),
    prior(exponential(2), class = sigma)
  ),
  iter = 2000, warmup = 500, seed = 4, cores = 4,
  backend = "cmdstanr", silent = 2, file = "fits/b06.5.3"
)

#posterior_summary(b6.5)[1:4, ] %>% round(digits = 3)

as_tibble(b6.5) %>% 
  rename("Married" = b_married,
         "Single" = b_single,
         "Age" = b_adult_age) %>% 
  dplyr::select(c(`Married`, `Single`, `Age`)) %>% 
  pivot_longer(cols = everything(),
               names_to = "Covariate",
               values_to = "Effect on happiness") %>% 
  ggplot(aes(x = `Effect on happiness`, y = reorder(Covariate, `Effect on happiness`))) +
  stat_halfeye(point_interval = median_qi, .width = .95,
               fill = "firebrick4") +
  labs(x = "Effect on happiness",
       y = NULL) +
  theme_bw() +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        panel.grid = element_blank())
```

The model is quite sure that age is associated with happiness. What if we omit marital status

$$ \mu_\text{Happiness for individual i} = \alpha +  \beta_A\text{Age}_i $$

```{r}
#| fig-width: 10
#| fig-height: 2

b6.6 <- brm(
  data = sim_Marriages_clean,
  family = gaussian(),
  happiness ~ 1 + adult_age,
  prior = c(
    prior(normal(0, 2), class = b, coef = adult_age),
    prior(exponential(2), class = sigma)
  ),
  iter = 2000, warmup = 500, seed = 4, cores = 4,
  backend = "cmdstanr", silent = 2, file = "fits/b06.6.1"
)

#posterior_summary(b6.5)[1:4, ] %>% round(digits = 3)

as_tibble(b6.6) %>% 
  rename("Age" = b_adult_age) %>% 
  dplyr::select(c(`Age`, `Intercept`)) %>% 
  pivot_longer(cols = everything(),
               names_to = "Covariate",
               values_to = "Effect on happiness") %>% 
  ggplot(aes(x = `Effect on happiness`, y = reorder(Covariate, `Effect on happiness`))) +
  stat_halfeye(point_interval = median_qi, .width = .95,
               fill = "firebrick4") +
  labs(x = "Effect on happiness",
       y = NULL) +
  theme_bw() +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        panel.grid = element_blank())
```

This exactly what we'd expect to see when we condition on a collider, if we conditioned on it we'd see a spurious associateion between the two causes.


## The Haunted DAG

Collider bias can arise from unobserved variables. Suppose we want to find the direct influence of both parents ($P$) and grandparents ($G$) on the educational achievements of children ($C$). Since grandparents also presumably influence their own children's education there is an arrow $G \rightarrow P$.

```{r}
#| fig-width: 3
#| fig-height: 3
dag_coords <- tibble(name = c("G", "P", "C"),
                     x = c(1, 2, 2),
                     y = c(2, 2, 1))

m5 <- dagify("C" ~ "P" + "G",
             "P" ~ "G",
             coords = dag_coords) %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend))+
  geom_dag_point(color = "firebrick", alpha = 1/4, size = 10)+
  geom_dag_text( color = "firebrick", parse = TRUE) +
  geom_dag_edges(edge_color = "firebrick") +
  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = c(0.2, 0.2)) +
  theme_bw() +
  theme(panel.grid = element_blank())+
  labs(caption = "G = Grandparents, P = Parents, C = Children")

m5
```

But suppose there are unmeasured, common influences on parents and their children, such as neighborhoods, that are not shared by grandparents (who live on the south coast of Spain now). Then our DAG becomes haunted by the unobserved U:

```{r}
#| fig-width: 4
#| fig-height: 3
dag_coords <- tibble(name = c("G", "P", "C", "U"),
                     x = c(1, 2, 2, 2.5),
                     y = c(2, 2, 1, 1.5))

m6 <- dagify("C" ~ "P" + "G" + "U",
             "P" ~ "G" + "U",
             coords = dag_coords) %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend))+
  geom_dag_point(color = "firebrick", alpha = 1/4, size = 10)+
  geom_dag_point(data = . %>% filter(name == "U"), 
                 color = "firebrick4", fill = NA, size = 10, shape = 21, 
                 stroke = 2, linetype = "dashed")+
  geom_dag_text( color = "firebrick", parse = TRUE) +
  geom_dag_edges(edge_color = "firebrick") +
  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = c(0.2, 0.2)) +
  theme_bw() +
  theme(panel.grid = element_blank())+
  labs(caption = "G = Grandparents, P = Parents, C = Children, U = Unobserved Effect")

m6
```

Now $P$ is a common consequence of $G$ and $U$, so if we condition on $P$, it will bias inference about $G \rightarrow C$, even if we never get to measure $U$. This isn't immediately obvious, so let’s crawl through a quantitative example.

```{r family education simulation, include = TRUE}
b_GP <- 1 # direct effect of G on P
b_GC <- 0 # direct effecct of G on C
b_PC <- 1 # direct effect of C on P
b_U <- 2 # direct effect of U on P and C
```

For this example we're going to say that Grandparents $G$ have no direct effect on grandkids $C$. The example doesn’t depend upon that effect being exactly zero, but it will make the lesson clearer.

```{r}
N_sims <- 200 # number of grandparent-parent-child triads
set.seed(1)
U <- 2 * rbinom(N_sims, 1, 0.5) - 1
G <- rnorm(N_sims)
P <- rnorm(N_sims, (b_GP * G) + (b_U * U))
C <- rnorm(N_sims, (b_GC * G) + (b_PC * P) + (b_U * U))

familySim <- tibble(C=C, P=P, G=G, U=U)

```

Now what happens when we try to infer the influence of grandparents? Since some of the total effect of grandparents passes through parents, we realize we need to control for parents. Our regression looks like

$$ \mu_\text{children} = \alpha + \beta_G\text{Grandparents} + \beta_P\text{Parents} $$
```{r}
#| fig-width: 10
#| fig-height: 3

b6.7 <- brm(data = familySim,
            family = gaussian,
            C ~ 1 + G + P,
            prior = c(
              prior(normal(0, 5), class = Intercept),
              prior(normal(0, 2), class = b),
              prior(exponential(2), class = sigma)
            ),
            iter = 2000, warmup = 500, cores = 4, seed = 4,
            backend = "cmdstanr", silent = 2, file = "fits/b06.7.0"
            )

as_tibble(b6.7) %>% 
  rename("Grandparents" = b_G,
         "Parents" = b_P) %>% 
  dplyr::select(c(`Grandparents`, `Parents`, `Intercept`)) %>% 
  pivot_longer(cols = everything(),
               names_to = "Covariate",
               values_to = "Effect") %>% 
  ggplot(aes(x = `Effect`, y = reorder(Covariate, `Effect`))) +
  stat_halfeye(point_interval = median_qi, .width = .95,
               fill = "firebrick4") +
  labs(x = "Effect on Childs Educational Attainment",
       y = NULL) +
  theme_bw() +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        panel.grid = element_blank())
```

The inferred effect of parents looks too big, almost twice as large as it should be. That isn’t surprising. Some of the correlation between $P$ and $C$ is due to $U$, and the model doesn’t know about $U$. That’s a simple confound. More surprising is that the model is confident that the direct effect of grandparents is to hurt their grandkids. The regression is not wrong. But a causal interpretation of that association would be.

```{r}
familySim <- familySim %>% 
  mutate(cZ = (C - mean(C))/sd(C),
         gZ = (G - mean(G))/sd(G),
         pMid = ifelse(P > quantile(P, .45) & P < quantile(P, .65), TRUE, FALSE))

ggplot(data = familySim)+
  geom_point(aes(x = gZ, y = cZ, color = as.factor(U), fill = pMid), shape = 21)+
  scale_color_manual(values = c("black", "blue"), labels = c("Bad Neighborhoods", "Good Neighborhoods"), name = "Unobserved Neighborhood")+
  scale_fill_manual(values = c("white", "orange"), labels = c("Not Mid Parents", "Mid Parents"), name = "Parents Education Level")+
  geom_smooth(data = . %>% filter(pMid), method = "lm", aes(x= gZ, y = cZ), se = FALSE, color = "orange")+
  theme_minimal()+
  labs(x = "Grandparents Effect", y = "Childerns Education", title = "Regression with Parents in 45th to 65th centiles")
```

So how does the negative association arise, when we condition on parents? Conditioning on parents is like looking within sub-populations of parents with similar education. So let’s try that. In Figure 6.5, I’ve highlighted in filled points those parents between the 45th and 60th centiles of education. There is nothing special of this range. It just makes the phenomenon easier to see. Now if we draw a regression line through only these points, regressing C on G, the slope is negative. There is the negative association that our multiple regression finds. But why does it exist?
    
It exists because, once we know P, learning G invisibly tells us about the neighborhood U, and U is associated with the outcome C. I know this is confusing. As I keep saying, if you are confused, it is only because you are paying attention. So consider two different parents with the same education level, say for example at the median 50th centile. One of these parents has a highly educated grandparent. The other has a poorly educated grandparent. The only probable way, in this example, for these parents to have the same education is if they live in different types of neighborhoods. We can’t see these neighborhood effects—we haven’t measured them, recall—but the influence of neighborhood is still transmitted to the children C. So for our mythical two parents with the same education, the one with the highly educated grandparent ends up with a less well educated child. The one with the less educated grandparent ends up with the better educated child. G predicts lower C.
